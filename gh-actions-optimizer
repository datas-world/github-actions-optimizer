#!/usr/bin/env python3

# GitHub Actions Optimizer - GitHub CLI Extension
# Copyright (C) 2025 Torsten Marco Knodt
#
# This file is part of github-actions-optimizer.
# Licensed under the GNU General Public License v3.0

"""GitHub Actions Optimizer - GitHub CLI Extension.

A comprehensive GitHub CLI extension for optimizing GitHub Actions workflows
through cost analysis, performance optimization, and security auditing.

Features:
- Workflow analysis and optimization recommendations
- Cost calculation and optimization strategies
- Security auditing and compliance checks
- Runner optimization suggestions
- Integration with GitHub API via gh CLI
"""

import argparse
import json
import os
import subprocess  # nosec B404
import sys
import urllib.request
from typing import Any, Dict, List, Optional, cast

import yaml

# Extension metadata
EXTENSION_NAME = "actions-optimizer"
EXTENSION_VERSION = "v0.1.0-dev"
EXTENSION_DESCRIPTION = (
    "Optimize GitHub Actions workflows for cost, performance, and security"
)

# ANSI color codes


class Colors:
    """ANSI color codes for terminal output formatting."""

    RED = "\033[0;31m"
    GREEN = "\033[0;32m"
    YELLOW = "\033[1;33m"
    BLUE = "\033[0;34m"
    BOLD = "\033[1m"
    NC = "\033[0m"  # No Color


def log_info(message: str) -> None:
    """Log info message to stderr."""
    print(f"{Colors.BLUE}[INFO]{Colors.NC} {message}", file=sys.stderr)


def log_warn(message: str) -> None:
    """Log warning message to stderr."""
    print(f"{Colors.YELLOW}[WARN]{Colors.NC} {message}", file=sys.stderr)


def log_error(message: str) -> None:
    """Log error message to stderr."""
    print(f"{Colors.RED}[ERROR]{Colors.NC} {message}", file=sys.stderr)


def log_success(message: str) -> None:
    """Log success message to stderr."""
    print(f"{Colors.GREEN}[SUCCESS]{Colors.NC} {message}", file=sys.stderr)


def check_dependencies() -> None:
    """Check if required dependencies are available."""
    deps = ["gh", "jq"]
    missing = []

    for dep in deps:
        try:
            subprocess.run([dep, "--version"], capture_output=True, check=True)
        except (subprocess.CalledProcessError, FileNotFoundError):
            missing.append(dep)

    if missing:
        log_error(f"Missing required dependencies: {', '.join(missing)}")
        log_info("Please install the missing dependencies and try again.")
        sys.exit(1)


def get_current_repo() -> Optional[str]:
    """Get current repository from gh CLI, GitHub Actions env vars, or git remote."""
    # First try GitHub Actions environment variables
    github_repository = os.environ.get("GITHUB_REPOSITORY")
    if github_repository:
        log_info(f"Using GitHub Actions repository: {github_repository}")
        return github_repository

    try:
        # Try gh CLI second
        result = subprocess.run(
            ["gh", "repo", "view", "--json", "nameWithOwner"],
            capture_output=True,
            text=True,
            check=True,
        )
        repo_info = json.loads(result.stdout)
        repo_raw = repo_info.get("nameWithOwner")
        if repo_raw and isinstance(repo_raw, str):
            log_info(f"Using gh CLI detected repository: {repo_raw}")
            return cast(str, repo_raw)
    except (subprocess.CalledProcessError, json.JSONDecodeError, FileNotFoundError):
        pass

    try:
        # Fallback to git remote
        result = subprocess.run(
            ["git", "remote", "get-url", "origin"],
            capture_output=True,
            text=True,
            check=True,
        )
        remote_url = result.stdout.strip()
        # Parse GitHub URL to get owner/repo
        if "github.com" in remote_url:
            if remote_url.endswith(".git"):
                remote_url = remote_url[:-4]
            parts = remote_url.split("/")[-2:]
            if len(parts) == 2:
                repo = f"{parts[0]}/{parts[1]}"
                log_info(f"Using git remote detected repository: {repo}")
                return repo
    except subprocess.CalledProcessError:
        pass

    return None


def validate_repo(repo: Optional[str]) -> str:
    """Validate repository format or get current repo."""
    if not repo:
        repo = get_current_repo()
        if not repo:
            log_error("No repository specified and unable to detect current repository")
            log_info("Use --repo owner/repo or run from within a GitHub repository")
            log_info("Or use --sample-data to test with sample data")
            sys.exit(1)
        log_info(f"Using current repository: {repo}")

    if "/" not in repo:
        log_error("Invalid repository format. Expected: owner/repo")
        sys.exit(1)

    parts = repo.split("/")
    if len(parts) != 2 or not all(part.strip() for part in parts):
        log_error("Invalid repository format. Expected: owner/repo")
        sys.exit(1)

    return repo


def get_repo_for_command(args: argparse.Namespace) -> str:
    """Get repository for command, handling sample data case."""
    if hasattr(args, "sample_data") and args.sample_data:
        return "sample/repo"

    # Check if repo was provided via command line
    repo = getattr(args, "repo", None)

    # If no repo specified, try to auto-detect
    if not repo:
        repo = get_current_repo()
        if not repo:
            log_error("No repository specified and unable to detect current repository")
            log_info("Use --repo owner/repo, run from within a GitHub repository,")
            log_info("or use --sample-data to test with sample data")
            sys.exit(1)

    return validate_repo(repo)


def run_gh_command(
    args: List[str], check: bool = True
) -> subprocess.CompletedProcess[str]:
    """Run a gh CLI command."""
    try:
        result = subprocess.run(
            ["gh"] + args, capture_output=True, text=True, check=check
        )
        return result
    except subprocess.CalledProcessError as e:
        log_error(f"GitHub CLI command failed: {e}")
        if e.stderr:
            log_error(f"Error output: {e.stderr}")
        sys.exit(1)


def get_sample_workflows() -> List[Dict[str, Any]]:
    """Get sample workflow data for testing."""
    return [
        {
            "name": "ci.yml",
            "download_url": "sample",
            "content": """
name: CI
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - run: pip install -r requirements.txt
      - run: pytest
""",
        },
        {
            "name": "security.yml",
            "download_url": "sample",
            "content": """
name: Security
on: [push]
jobs:
  security:
    runs-on: macos-latest
    steps:
      - uses: actions/checkout@v4
      - run: bandit -r .
""",
        },
    ]


def get_sample_security_data() -> Dict[str, Any]:
    """Get sample security data for testing."""
    return {
        "has_vulnerability_alerts": True,
        "has_dependabot_alerts": True,
        "has_automated_security_fixes": False,
        "security_and_analysis": {
            "secret_scanning": {"status": "enabled"},
            "dependabot_security_updates": {"status": "disabled"},
        },
    }


def get_workflows(repo: str) -> List[Dict[str, Any]]:
    """Get workflow files from repository."""
    log_info(f"Fetching workflows from {repo}...")

    result = run_gh_command(
        [
            "api",
            f"repos/{repo}/contents/.github/workflows",
            "--jq",
            '.[] | select(.type == "file" and (.name | endswith(".yml") or endswith(".yaml")))',
        ],
        check=False,
    )

    if result.returncode != 0:
        log_error("Failed to fetch workflows. Check repository access and permissions.")
        return []

    workflows = []
    for line in result.stdout.strip().split("\n"):
        if line.strip():
            try:
                workflow = json.loads(line)
                workflows.append(workflow)
            except json.JSONDecodeError:
                continue

    return workflows


def get_workflows_with_repo(repo: str) -> List[Dict[str, Any]]:
    """Get workflow files from repository."""
    log_info(f"Fetching workflows from {repo}...")

    result = run_gh_command(
        [
            "api",
            f"repos/{repo}/contents/.github/workflows",
            "--jq",
            '.[] | select(.type == "file" and (.name | endswith(".yml") or endswith(".yaml")))',
        ],
        check=False,
    )

    if result.returncode != 0:
        log_error("Failed to fetch workflows. Check repository access and permissions.")
        return []

    workflows = []
    for line in result.stdout.strip().split("\n"):
        if line.strip():
            try:
                workflow = json.loads(line)
                workflows.append(workflow)
            except json.JSONDecodeError:
                continue

    return workflows


def download_workflow_content(download_url: str) -> str:
    """Download workflow content from URL."""
    try:
        with urllib.request.urlopen(download_url) as response:  # nosec B310
            content_bytes = cast(bytes, response.read())
            content = content_bytes.decode("utf-8")
            return content
    except Exception as e:
        log_error(f"Failed to download workflow content: {e}")
        return ""


def analyze_workflow(content: str, name: str) -> Dict[str, Any]:
    """Analyze workflow for optimization opportunities."""
    log_info(f"Analyzing workflow: {name}")

    issues = []
    recommendations = []

    # Check for expensive runners
    if "macos" in content.lower() or "windows" in content.lower():
        issues.append("Uses expensive runners (macOS/Windows)")
        recommendations.append(
            "Consider using ubuntu-latest for non-platform-specific jobs"
        )

    # Check for missing concurrency controls
    if "concurrency:" not in content:
        issues.append("Missing concurrency controls")
        recommendations.append("Add concurrency controls to prevent redundant runs")

    # Check for missing cache
    setup_actions = ["setup-node", "setup-python", "setup-java", "setup-go"]
    if any(action in content for action in setup_actions) and "cache:" not in content:
        issues.append("Missing dependency caching")
        recommendations.append("Enable caching for faster builds and reduced costs")

    # Check for security issues
    if "pull_request_target" in content:
        issues.append("Uses potentially unsafe pull_request_target trigger")
        recommendations.append(
            "Review pull_request_target usage for security implications"
        )

    # Check for hardcoded secrets
    if "${{" in content and any(
        secret in content.upper() for secret in ["PASSWORD", "TOKEN", "KEY", "SECRET"]
    ):
        issues.append("Potential hardcoded secrets detected")
        recommendations.append(
            "Ensure all secrets use GitHub secrets or environment variables"
        )

    return {
        "name": name,
        "issues": issues,
        "recommendations": recommendations,
        "has_issues": len(issues) > 0,
    }


def cmd_analyze(args: argparse.Namespace) -> None:
    """Analyze workflows for optimization opportunities."""
    repo = get_repo_for_command(args)

    if args.sample_data:
        if not args.quiet:
            log_info("Using sample data for analysis...")
        workflows = get_sample_workflows()
    else:
        workflows = get_workflows(repo)

    if not workflows:
        log_info(f"No workflows found in {repo}")
        return

    results = []

    for workflow in workflows:
        name = workflow.get("name", "unknown")

        if args.sample_data:
            content = workflow.get("content", "")
        else:
            download_url = workflow.get("download_url")
            if not download_url:
                log_warn(f"No download URL for workflow: {name}")
                continue

            if args.verbose:
                log_info(f"Downloading workflow: {name}")
            content = download_workflow_content(download_url)

        if content:
            analysis = analyze_workflow(content, name)
            results.append(analysis)

    # Output results
    if args.format == "json":
        output = {"repository": repo, "workflows": results}
        if args.output:
            with open(args.output, "w") as f:
                json.dump(output, f, indent=2)
            if not args.quiet:
                log_info(f"Analysis saved to {args.output}")
        else:
            print(json.dumps(output, indent=2))
    elif args.format == "yaml":
        try:
            output = {"repository": repo, "workflows": results}
            yaml_output = yaml.dump(output, default_flow_style=False)
            if args.output:
                with open(args.output, "w") as f:
                    f.write(yaml_output)
                log_success(f"Analysis saved to {args.output}")
            else:
                print(yaml_output)
        except ImportError:
            log_error("PyYAML not installed. Install with: pip install PyYAML")
            sys.exit(1)
    else:
        # Table format
        if not args.quiet:
            print(f"\n{Colors.BOLD}Analysis Results for {repo}{Colors.NC}")
            print("=" * 60)

        for result in results:
            print(f"\n{Colors.BOLD}Workflow: {result['name']}{Colors.NC}")

            if result["has_issues"]:
                print(f"{Colors.YELLOW}Issues found:{Colors.NC}")
                for issue in result["issues"]:
                    print(f"  - {issue}")

                print(f"{Colors.BLUE}Recommendations:{Colors.NC}")
                for rec in result["recommendations"]:
                    print(f"  - {rec}")
            else:
                print(f"{Colors.GREEN}No optimization issues found{Colors.NC}")

        if args.output:
            # Save table format to file as well
            with open(args.output, "w") as f:
                f.write(f"Analysis Results for {repo}\n")
                f.write("=" * 60 + "\n")
                for result in results:
                    f.write(f"\nWorkflow: {result['name']}\n")
                    if result["has_issues"]:
                        f.write("Issues found:\n")
                        for issue in result["issues"]:
                            f.write(f"  - {issue}\n")
                        f.write("Recommendations:\n")
                        for rec in result["recommendations"]:
                            f.write(f"  - {rec}\n")
                    else:
                        f.write("No optimization issues found\n")
            log_success(f"Analysis saved to {args.output}")

    if args.web:
        # Open repository actions page in browser
        if not args.sample_data:
            subprocess.run(["gh", "repo", "view", repo, "--web"], check=False)


def cmd_cost(args: argparse.Namespace) -> None:
    """Calculate and display workflow costs."""
    if not args.quiet:
        log_info("Calculating workflow costs...")

    costs = {
        "ubuntu-latest": 0.008,
        "ubuntu-22.04": 0.008,
        "ubuntu-20.04": 0.008,
        "windows-latest": 0.016,
        "windows-2022": 0.016,
        "windows-2019": 0.016,
        "macos-latest": 0.08,
        "macos-14": 0.08,
        "macos-13": 0.08,
        "macos-12": 0.08,
        "macos-latest-xlarge": 0.16,
        "macos-14-xlarge": 0.16,
    }

    if args.format == "json":
        output = {
            "runner_costs_per_minute": costs,
            "currency": "USD",
            "last_updated": "2025-09-03",
            "optimization_tips": [
                "Use ubuntu-latest for most jobs (cheapest)",
                "Only use macOS/Windows when platform-specific testing needed",
                "Enable concurrency controls to prevent redundant runs",
                "Use caching to reduce build times",
                "Consider self-hosted runners for high-volume usage",
            ],
        }

        if args.output:
            with open(args.output, "w") as f:
                json.dump(output, f, indent=2)
            log_success(f"Cost analysis saved to {args.output}")
        else:
            print(json.dumps(output, indent=2))
    elif args.format == "yaml":
        try:
            output = {
                "runner_costs_per_minute": costs,
                "currency": "USD",
                "last_updated": "2025-09-03",
                "optimization_tips": [
                    "Use ubuntu-latest for most jobs (cheapest)",
                    "Only use macOS/Windows when platform-specific testing needed",
                    "Enable concurrency controls to prevent redundant runs",
                    "Use caching to reduce build times",
                    "Consider self-hosted runners for high-volume usage",
                ],
            }
            yaml_output = yaml.dump(output, default_flow_style=False)
            if args.output:
                with open(args.output, "w") as f:
                    f.write(yaml_output)
                log_success(f"Cost analysis saved to {args.output}")
            else:
                print(yaml_output)
        except ImportError:
            log_error("PyYAML not installed. Install with: pip install PyYAML")
            sys.exit(1)
    else:
        if not args.quiet:
            print(
                f"\n{Colors.BOLD}GitHub Actions Runner Costs "
                f"(per minute){Colors.NC}"
            )
            print("=" * 50)

        for runner, cost in costs.items():
            print(f"  {runner:<20} ${cost:.3f}")

        if not args.quiet:
            print(f"\n{Colors.BLUE}Cost Optimization Tips:{Colors.NC}")
            print("  - Use ubuntu-latest for most jobs (cheapest)")
            print(
                "  - Only use macOS/Windows when platform-specific " "testing is needed"
            )
            print("  - Enable concurrency controls to prevent redundant runs")
            print("  - Use caching to reduce build times")
            print("  - Consider self-hosted runners for high-volume usage")

        if args.output:
            with open(args.output, "w") as f:
                f.write("GitHub Actions Runner Costs (per minute)\n")
                f.write("=" * 50 + "\n\n")
                for runner, cost in costs.items():
                    f.write(f"  {runner:<20} ${cost:.3f}\n")
                f.write("\nCost Optimization Tips:\n")
                f.write("  - Use ubuntu-latest for most jobs (cheapest)\n")
                f.write(
                    "  - Only use macOS/Windows when platform-specific "
                    "testing is needed\n"
                )
                f.write(
                    "  - Enable concurrency controls to prevent " "redundant runs\n"
                )
                f.write("  - Use caching to reduce build times\n")
                f.write("  - Consider self-hosted runners for high-volume " "usage\n")
            log_success(f"Cost analysis saved to {args.output}")

    if args.web:
        # Open GitHub Actions pricing page
        subprocess.run(
            [
                "gh",
                "browse",
                "https://docs.github.com/en/billing/managing-billing-for-"
                "github-actions/about-billing-for-github-actions",
            ],
            check=False,
        )


def cmd_security(args: argparse.Namespace) -> None:
    """Run security audit of workflows."""
    repo = get_repo_for_command(args)

    if not args.quiet:
        log_info(f"Running security audit for {repo}...")

    if args.sample_data:
        if not args.quiet:
            log_info("Using sample data for security audit...")
        repo_info = get_sample_security_data()
    else:
        # Check repository security settings
        result = run_gh_command(
            [
                "api",
                f"repos/{repo}",
                "--jq",
                (
                    "{has_vulnerability_alerts, has_dependabot_alerts, "
                    "has_automated_security_fixes, security_and_analysis}"
                ),
            ]
        )

        try:
            repo_info = json.loads(result.stdout)
        except json.JSONDecodeError:
            log_error("Failed to parse repository information")
            return

    security_features = {
        "has_vulnerability_alerts": "Vulnerability Alerts",
        "has_dependabot_alerts": "Dependabot Alerts",
        "has_automated_security_fixes": "Automated Security Fixes",
    }

    if args.format == "json":
        output: Dict[str, Any] = {"repository": repo, "security_features": {}}
        for feature_key, feature_name in security_features.items():
            enabled = repo_info.get(feature_key, False)
            output["security_features"][feature_key] = {
                "name": feature_name,
                "enabled": enabled,
            }

        # Add advanced security features
        security_analysis = repo_info.get("security_and_analysis", {})
        if security_analysis:
            advanced_features = {
                "secret_scanning": "Secret Scanning",
                "dependabot_security_updates": "Dependabot Security Updates",
            }
            for feature_key, feature_name in advanced_features.items():
                feature_info = security_analysis.get(feature_key, {})
                enabled = feature_info.get("status") == "enabled"
                output["security_features"][feature_key] = {
                    "name": feature_name,
                    "enabled": enabled,
                }

        if args.output:
            with open(args.output, "w") as f:
                json.dump(output, f, indent=2)
            log_success(f"Security audit saved to {args.output}")
        else:
            print(json.dumps(output, indent=2))
    elif args.format == "yaml":
        try:
            yaml_output: Dict[str, Any] = {"repository": repo, "security_features": {}}
            for feature_key, feature_name in security_features.items():
                enabled = repo_info.get(feature_key, False)
                yaml_output["security_features"][feature_key] = {
                    "name": feature_name,
                    "enabled": enabled,
                }

            # Add advanced security features
            security_analysis = repo_info.get("security_and_analysis", {})
            if security_analysis:
                advanced_features = {
                    "secret_scanning": "Secret Scanning",
                    "dependabot_security_updates": "Dependabot Security Updates",
                }
                for feature_key, feature_name in advanced_features.items():
                    feature_info = security_analysis.get(feature_key, {})
                    enabled = feature_info.get("status") == "enabled"
                    yaml_output["security_features"][feature_key] = {
                        "name": feature_name,
                        "enabled": enabled,
                    }

            yaml_output_str = yaml.dump(yaml_output, default_flow_style=False)
            if args.output:
                with open(args.output, "w") as f:
                    f.write(yaml_output_str)
                log_success(f"Security audit saved to {args.output}")
            else:
                print(yaml_output_str)
        except ImportError:
            log_error("PyYAML not installed. Install with: pip install PyYAML")
            sys.exit(1)
    else:
        if not args.quiet:
            print(f"\n{Colors.BOLD}Security Audit for {repo}{Colors.NC}")
            print("=" * 50)

        for feature_key, feature_name in security_features.items():
            enabled = repo_info.get(feature_key, False)
            status = (
                f"{Colors.GREEN}✓ Enabled{Colors.NC}"
                if enabled
                else f"{Colors.RED}✗ Disabled{Colors.NC}"
            )
            print(f"  {feature_name:<25} {status}")

        # Check security_and_analysis features
        security_analysis = repo_info.get("security_and_analysis", {})
        if security_analysis:
            if not args.quiet:
                print(f"\n{Colors.BOLD}Advanced Security Features:{Colors.NC}")

            advanced_features = {
                "secret_scanning": "Secret Scanning",
                "dependabot_security_updates": "Dependabot Security Updates",
            }

            for feature_key, feature_name in advanced_features.items():
                feature_info = security_analysis.get(feature_key, {})
                enabled = feature_info.get("status") == "enabled"
                status = (
                    f"{Colors.GREEN}✓ Enabled{Colors.NC}"
                    if enabled
                    else f"{Colors.RED}✗ Disabled{Colors.NC}"
                )
                print(f"  {feature_name:<25} {status}")

    if args.web and not args.sample_data:
        # Open repository security settings in browser
        subprocess.run(
            ["gh", "browse", f"/{repo}/settings/security_analysis"], check=False
        )


def cmd_runners(args: argparse.Namespace) -> None:
    """Provide runner optimization recommendations."""
    recommendations = [
        "Use ubuntu-latest for most jobs (cheapest at $0.008/min)",
        "Only use macos-latest when absolutely necessary ($0.08/min)",
        "Use windows-latest only for Windows-specific testing ($0.016/min)",
        "Consider self-hosted runners for high-volume usage",
        "Use concurrency controls to prevent redundant runs",
        "Enable caching to reduce build times",
        "Use matrix strategies to test multiple versions efficiently",
        "Minimize job duration by optimizing build steps",
        "Use conditional job execution to skip unnecessary runs",
    ]

    if args.format == "json":
        output = {
            "runner_recommendations": recommendations,
            "cost_optimization": True,
            "focus": "minimize_costs_maximize_efficiency",
        }

        if args.output:
            with open(args.output, "w") as f:
                json.dump(output, f, indent=2)
            log_success(f"Runner recommendations saved to {args.output}")
        else:
            print(json.dumps(output, indent=2))
    elif args.format == "yaml":
        try:
            output = {
                "runner_recommendations": recommendations,
                "cost_optimization": True,
                "focus": "minimize_costs_maximize_efficiency",
            }
            yaml_output = yaml.dump(output, default_flow_style=False)
            if args.output:
                with open(args.output, "w") as f:
                    f.write(yaml_output)
                log_success(f"Runner recommendations saved to {args.output}")
            else:
                print(yaml_output)
        except ImportError:
            log_error("PyYAML not installed. Install with: pip install PyYAML")
            sys.exit(1)
    else:
        if not args.quiet:
            print(f"\n{Colors.BOLD}Runner Optimization Recommendations{Colors.NC}")
            print("=" * 50)

        for i, rec in enumerate(recommendations, 1):
            print(f"  {i}. {rec}")

        if args.output:
            with open(args.output, "w") as f:
                f.write("Runner Optimization Recommendations\n")
                f.write("=" * 50 + "\n\n")
                for i, rec in enumerate(recommendations, 1):
                    f.write(f"  {i}. {rec}\n")
            log_success(f"Runner recommendations saved to {args.output}")

    if args.web:
        # Open GitHub Actions documentation
        subprocess.run(
            [
                "gh",
                "browse",
                "https://docs.github.com/en/actions/using-github-hosted-runners",
            ],
            check=False,
        )


def cmd_generate_runner_setup(args: argparse.Namespace) -> None:
    """Generate optimal runner configuration."""
    repo = get_repo_for_command(args)

    if not args.quiet:
        # Generate optimal runner matrix
        log_info(f"Generating runner setup for {repo}...")
    runner_config = {
        "strategy": {
            "matrix": {
                "os": ["ubuntu-latest"],
                "python-version": ["3.8", "3.9", "3.10", "3.11", "3.12", "3.13"],
            }
        },
        "runs-on": "${{ matrix.os }}",
        "timeout-minutes": 10,
        "concurrency": {
            "group": "${{ github.workflow }}-${{ github.ref }}",
            "cancel-in-progress": True,
        },
    }

    if args.format == "json":
        if args.output:
            with open(args.output, "w") as f:
                json.dump(runner_config, f, indent=2)
            log_success(f"Runner configuration saved to {args.output}")
        else:
            print(json.dumps(runner_config, indent=2))
    elif args.format == "yaml":
        try:
            yaml_output = yaml.dump(runner_config, default_flow_style=False)
            if args.output:
                with open(args.output, "w") as f:
                    f.write(yaml_output)
                log_success(f"Runner configuration saved to {args.output}")
            else:
                print(yaml_output)
        except ImportError:
            log_error("PyYAML not installed. Install with: pip install PyYAML")
            sys.exit(1)
    else:
        print(f"\n{Colors.BOLD}Optimal Runner Configuration for {repo}{Colors.NC}")
        print("=" * 60)
        print("strategy:")
        print("  matrix:")
        print("    os: [ubuntu-latest]")
        print("    python-version: ['3.8', '3.9', '3.10', '3.11', '3.12', '3.13']")
        print("runs-on: ${{ matrix.os }}")
        print("timeout-minutes: 10")
        print("concurrency:")
        print("  group: ${{ github.workflow }}-${{ github.ref }}")
        print("  cancel-in-progress: true")


def cmd_generate_workflow_patch(args: argparse.Namespace) -> None:
    """Generate workflow optimization patches."""
    repo = get_repo_for_command(args)

    if not args.quiet:
        # Sample patch recommendations
        log_info(f"Generating workflow patches for {repo}...")
    patches = {
        "concurrency_control": {
            "description": "Add concurrency control to prevent redundant runs",
            "patch": {
                "concurrency": {
                    "group": "${{ github.workflow }}-${{ github.ref }}",
                    "cancel-in-progress": True,
                }
            },
        },
        "caching": {
            "description": "Add dependency caching",
            "patch": {
                "steps": [
                    {
                        "name": "Cache dependencies",
                        "uses": "actions/cache@v4",
                        "with": {
                            "path": "~/.cache/pip",
                            "key": "${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}",
                        },
                    }
                ]
            },
        },
        "timeout": {
            "description": "Add reasonable timeouts",
            "patch": {"timeout-minutes": 15},
        },
    }

    if args.format == "json":
        if args.output:
            with open(args.output, "w") as f:
                json.dump(patches, f, indent=2)
            log_success(f"Workflow patches saved to {args.output}")
        else:
            print(json.dumps(patches, indent=2))
    else:
        print(f"\n{Colors.BOLD}Workflow Optimization Patches for {repo}{Colors.NC}")
        print("=" * 60)
        for patch_name, patch_data in patches.items():
            print(f"\n{Colors.YELLOW}{patch_name}:{Colors.NC}")
            print(f"  Description: {patch_data['description']}")


def cmd_validate_runners(args: argparse.Namespace) -> None:
    """Validate runner configurations."""
    repo = get_repo_for_command(args)

    if not args.quiet:
        # Get workflows and validate runner usage
        log_info(f"Validating runner configurations for {repo}...")
    workflows = get_workflows(repo)
    validation_results = []

    for workflow in workflows:
        name = workflow.get("name", "unknown")
        download_url = workflow.get("download_url")

        if download_url:
            content = download_workflow_content(download_url)
            if content:
                # Check for expensive runners
                issues = []
                if "macos" in content.lower():
                    issues.append("Uses expensive macOS runners")
                if "windows" in content.lower():
                    issues.append("Uses expensive Windows runners")
                if "timeout-minutes" not in content:
                    issues.append("Missing timeout configuration")
                if "concurrency" not in content:
                    issues.append("Missing concurrency control")

                validation_results.append(
                    {
                        "workflow": name,
                        "issues": issues,
                        "status": "PASS" if not issues else "FAIL",
                    }
                )

    if args.format == "json":
        output = {"repository": repo, "validation_results": validation_results}
        if args.output:
            with open(args.output, "w") as f:
                json.dump(output, f, indent=2)
            log_success(f"Validation results saved to {args.output}")
        else:
            print(json.dumps(output, indent=2))
    else:
        print(f"\n{Colors.BOLD}Runner Validation Results for {repo}{Colors.NC}")
        print("=" * 60)
        for result in validation_results:
            status_color = Colors.GREEN if result["status"] == "PASS" else Colors.RED
            print(
                f"\n{Colors.BOLD}{result['workflow']}{Colors.NC}: "
                f"{status_color}{result['status']}{Colors.NC}"
            )
            if result["issues"]:
                for issue in result["issues"]:
                    print(f"  - {issue}")


def cmd_benchmark(args: argparse.Namespace) -> None:
    """Benchmark and performance analysis."""
    repo = get_repo_for_command(args)

    if not args.quiet:
        log_info(
            f"Running benchmark analysis for {repo} "
            # Simulate benchmark data
            f"(duration: {args.duration} days)..."
        )
    benchmark_data = {
        "repository": repo,
        "analysis_period_days": args.duration,
        "metrics": {
            "total_workflow_runs": 150,
            "average_duration_minutes": 8.5,
            "estimated_monthly_cost": 12.75,
            "success_rate": 0.94,
            "most_expensive_runner": "ubuntu-latest",
            "optimization_potential": "15% cost reduction possible",
        },
        "recommendations": [
            "Enable concurrency controls on 3 workflows",
            "Add caching to reduce build times by 25%",
            "Consider matrix optimization for test jobs",
        ],
    }

    if args.format == "json":
        if args.output:
            with open(args.output, "w") as f:
                json.dump(benchmark_data, f, indent=2)
            log_success(f"Benchmark results saved to {args.output}")
        else:
            print(json.dumps(benchmark_data, indent=2))
    else:
        print(f"\n{Colors.BOLD}Benchmark Analysis for {repo}{Colors.NC}")
        print("=" * 60)
        print(f"Analysis Period: {args.duration} days")
        print(
            f"Total Workflow Runs: {benchmark_data['metrics']['total_workflow_runs']}"
        )
        print(
            f"Average Duration: {benchmark_data['metrics']['average_duration_minutes']} min"
        )
        print(
            f"Estimated Monthly Cost: ${benchmark_data['metrics']['estimated_monthly_cost']}"
        )
        print(f"Success Rate: {benchmark_data['metrics']['success_rate']*100:.1f}%")
        print(f"\n{Colors.BLUE}Optimization Potential:{Colors.NC}")
        print(f"  {benchmark_data['metrics']['optimization_potential']}")


def create_parser() -> argparse.ArgumentParser:
    """Create the argument parser."""
    parser = argparse.ArgumentParser(
        prog=f"gh {EXTENSION_NAME}",
        description=EXTENSION_DESCRIPTION,
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=f"""
Examples:
  gh {EXTENSION_NAME} analyze --repo owner/repo
  gh {EXTENSION_NAME} analyze                    # auto-detect current repo
  gh {EXTENSION_NAME} cost --format json --web
  gh {EXTENSION_NAME} security --repo owner/repo --quiet
  gh {EXTENSION_NAME} runners --format yaml --output recommendations.yaml
  gh {EXTENSION_NAME} generate runner-setup --repo owner/repo
  gh {EXTENSION_NAME} validate runners --repo owner/repo

For more information: https://github.com/datas-world/github-actions-optimizer
        """,
    )

    parser.add_argument(
        "--version", action="version", version=f"{EXTENSION_NAME} {EXTENSION_VERSION}"
    )

    subparsers = parser.add_subparsers(dest="command", help="Available commands")

    # Common argument groups
    def add_common_args(
        parser_obj: argparse.ArgumentParser, repo_required: bool = False
    ) -> None:
        """Add common arguments to a parser."""
        parser_obj.add_argument(
            "-R",
            "--repo",
            required=repo_required,
            help="Select another repository using the [HOST/]OWNER/REPO format",
        )
        parser_obj.add_argument(
            "-f",
            "--format",
            choices=["table", "json", "yaml"],
            default="table",
            help="Output format (default: table)",
        )
        parser_obj.add_argument("-o", "--output", help="Output file (default: stdout)")
        parser_obj.add_argument(
            "-v", "--verbose", action="store_true", help="Verbose output"
        )
        parser_obj.add_argument(
            "-q", "--quiet", action="store_true", help="Minimal output"
        )
        parser_obj.add_argument(
            "-w", "--web", action="store_true", help="Open results in web browser"
        )

    def add_output_args(parser_obj: argparse.ArgumentParser) -> None:
        """Add output-only arguments."""
        parser_obj.add_argument(
            "-f",
            "--format",
            choices=["table", "json", "yaml"],
            default="table",
            help="Output format (default: table)",
        )
        parser_obj.add_argument("-o", "--output", help="Output file (default: stdout)")
        parser_obj.add_argument(
            "-q", "--quiet", action="store_true", help="Minimal output"
        )

    # Analyze command
    analyze_parser = subparsers.add_parser(
        "analyze", help="Analyze workflows for optimization opportunities"
    )
    add_common_args(analyze_parser)
    analyze_parser.add_argument(
        "--sample-data",
        action="store_true",
        help="Use sample data for testing (when repo access is limited)",
    )

    # Cost command
    cost_parser = subparsers.add_parser(
        "cost", help="Calculate and optimize workflow costs"
    )
    add_output_args(cost_parser)
    cost_parser.add_argument(
        "-w",
        "--web",
        action="store_true",
        help="Open GitHub Actions pricing in web browser",
    )

    # Security command
    security_parser = subparsers.add_parser(
        "security", help="Security audit of workflows and repository settings"
    )
    add_common_args(security_parser)
    security_parser.add_argument(
        "--sample-data",
        action="store_true",
        help="Use sample data for testing (when repo access is limited)",
    )

    # Runners command
    runners_parser = subparsers.add_parser(
        "runners", help="Optimize runner usage and provide recommendations"
    )
    add_output_args(runners_parser)
    runners_parser.add_argument(
        "-w",
        "--web",
        action="store_true",
        help="Open GitHub Actions documentation in web browser",
    )

    # Generate subcommand group
    generate_parser = subparsers.add_parser(
        "generate", help="Generate configuration files and templates"
    )
    generate_subparsers = generate_parser.add_subparsers(
        dest="generate_command", help="Generate commands"
    )

    # Generate runner-setup
    runner_setup_parser = generate_subparsers.add_parser(
        "runner-setup", help="Generate optimal runner configuration"
    )
    add_common_args(runner_setup_parser)

    # Generate workflow-patch
    workflow_patch_parser = generate_subparsers.add_parser(
        "workflow-patch", help="Generate workflow optimization patches"
    )
    add_common_args(workflow_patch_parser)
    workflow_patch_parser.add_argument(
        "--workflow", help="Specific workflow file to patch"
    )

    # Validate subcommand group
    validate_parser = subparsers.add_parser(
        "validate", help="Validate configurations and setups"
    )
    validate_subparsers = validate_parser.add_subparsers(
        dest="validate_command", help="Validate commands"
    )

    # Validate runners
    validate_runners_parser = validate_subparsers.add_parser(
        "runners", help="Validate runner configurations"
    )
    add_common_args(validate_runners_parser)

    # Benchmark command
    benchmark_parser = subparsers.add_parser(
        "benchmark", help="Benchmark and performance analysis"
    )
    add_common_args(benchmark_parser)
    benchmark_parser.add_argument(
        "--duration",
        type=int,
        default=30,
        help="Analysis duration in days (default: 30)",
    )

    return parser


def main() -> None:
    """Execute the main entry point for the GitHub Actions optimizer."""
    check_dependencies()

    parser = create_parser()
    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        return

    try:
        if args.command == "analyze":
            cmd_analyze(args)
        elif args.command == "cost":
            cmd_cost(args)
        elif args.command == "security":
            cmd_security(args)
        elif args.command == "runners":
            cmd_runners(args)
        elif args.command == "generate":
            if not hasattr(args, "generate_command") or not args.generate_command:
                log_error("Generate subcommand required. Use --help for options.")
                sys.exit(1)
            elif args.generate_command == "runner-setup":
                cmd_generate_runner_setup(args)
            elif args.generate_command == "workflow-patch":
                cmd_generate_workflow_patch(args)
            else:
                log_error(f"Unknown generate command: {args.generate_command}")
                sys.exit(1)
        elif args.command == "validate":
            if not hasattr(args, "validate_command") or not args.validate_command:
                log_error("Validate subcommand required. Use --help for options.")
                sys.exit(1)
            elif args.validate_command == "runners":
                cmd_validate_runners(args)
            else:
                log_error(f"Unknown validate command: {args.validate_command}")
                sys.exit(1)
        elif args.command == "benchmark":
            cmd_benchmark(args)
        else:
            log_error(f"Unknown command: {args.command}")
            sys.exit(1)
    except KeyboardInterrupt:
        log_info("Operation cancelled by user")
        sys.exit(1)
    except Exception as e:
        log_error(f"Unexpected error: {e}")
        if hasattr(args, "verbose") and args.verbose:
            import traceback

            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
