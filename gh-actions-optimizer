#!/usr/bin/env python3

# Copyright (C) 2025 Torsten Marco Knodt
# This file is part of github-actions-optimizer.
#
# github-actions-optimizer is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

"""
GitHub Actions Optimizer - Comprehensive Workflow Optimization Platform
======================================================================

import subprocess
import json
A comprehensive platform for optimizing GitHub Actions workflows through cost analysis,
performance optimization, and resource management. Supports both live API data collection
and historical analysis with actionable recommendations and automated improvements.

Features:
- Real-time GitHub API integration with cost analysis
- Statistical analysis with variance and distribution metrics
- Cost optimization for all runner types (standard, larger, ARM64, GPU)
- Self-hosted runner setup and configuration generation
- Workflow patch generation for optimal runner usage
- Machine specification detection and optimization
- Multiple output formats (JSON, YAML, CSV, table)
- GitHub Copilot prompt generation for improvements
- Comprehensive resource efficiency analysis

Usage:
    python github-actions-optimizer.py --sample-data --format table
    python github-actions-optimizer.py --sample-data --format json --output analysis.json
    python github-actions-optimizer.py --sample-data --format yaml --output analysis.yaml
    python github-actions-optimizer.py --sample-data --format csv --output report.csv
    python github-actions-optimizer.py --sample-data --format table --detailed
    python github-actions-optimizer.py --sample-data --include-raw-data

Library Usage Documentation Check Pattern
=========================================

MANDATORY: Before using any library function or class, verify correct usage:

Example for tqdm logging integration:
$ python -c "import tqdm.contrib.logging; help(tqdm.contrib.logging)"
$ python -c "import tqdm.contrib.logging; print(dir(tqdm.contrib.logging))"

This revealed that TqdmLoggingHandler doesn't exist - use logging_redirect_tqdm() instead.

Apply this pattern to all library usage to prevent import errors and deprecated API usage.
"""

import argparse
import asyncio
import concurrent.futures
import csv
import hashlib
import inspect
import io
import json
import logging
import os
import pickle
import platform
import platform as py_platform
import re
import shutil
import statistics
import subprocess
import sys
import time
import tomllib  # Python 3.13+ built-in TOML support
import urllib.error
import urllib.request
from collections import defaultdict
from dataclasses import dataclass  # Keep for backwards compatibility if needed
from datetime import datetime, timedelta, timezone
from enum import Enum, StrEnum
from pathlib import Path
from typing import (
    Any,
    ClassVar,
    Dict,
    Final,
    List,
    Literal,
    Optional,
    Tuple,
    Union,
    get_args,
    get_origin,
)
from urllib.parse import urlparse

import distro
import numpy as np
import pandas as pd
import psutil  # For system information
import requests
import tqdm.contrib.logging
import yaml  # YAML output support

# GitHub API client using official PyGithub library (required dependency)
from github import Auth, Github, RateLimitExceededException, UnknownObjectException
from github.Repository import Repository
from github.WorkflowJob import WorkflowJob
from github.WorkflowRun import WorkflowRun
from pydantic import BaseModel, Field, field_validator, model_validator
from pydantic.types import NonNegativeInt, PositiveFloat, PositiveInt
from pydantic_core import PydanticUndefined
from tqdm import tqdm
from tqdm.contrib.concurrent import thread_map

# GitHub CLI Extension Utilities
# =============================

def get_github_token() -> Optional[str]:
    """Get GitHub token from GitHub CLI authentication."""
    try:
        result = subprocess.run(
            ["gh", "auth", "token"], 
            capture_output=True, 
            text=True, 
            check=True
        )
        return result.stdout.strip()
    except (subprocess.CalledProcessError, FileNotFoundError):
        return None

def get_current_repository() -> Optional[str]:
    """Get current repository context using GitHub CLI or git."""
    # Try GitHub CLI first
    try:
        result = subprocess.run(
            ["gh", "repo", "view", "--json", "nameWithOwner"],
            capture_output=True,
            text=True,
            check=True
        )
        repo_data = json.loads(result.stdout)
        return repo_data.get("nameWithOwner")
    except (subprocess.CalledProcessError, FileNotFoundError, json.JSONDecodeError):
        pass
    
    # Fallback to git remote
    try:
        result = subprocess.run(
            ["git", "remote", "get-url", "origin"],
            capture_output=True,
            text=True,
            timeout=5,
        )
        if result.returncode == 0:
            repo_url = result.stdout.strip()
            # Parse GitHub URL to get owner/repo
            if "github.com" in repo_url:
                # Handle SSH and HTTPS URLs
                if repo_url.startswith("git@github.com:"):
                    repo_path = repo_url.replace("git@github.com:", "")
                elif "github.com/" in repo_url:
                    repo_path = repo_url.split("github.com/")[1]
                else:
                    return None
                
                # Remove .git suffix
                if repo_path.endswith(".git"):
                    repo_path = repo_path[:-4]
                
                return repo_path
    except Exception:
        pass
    
    return None

def resolve_repository(repo_arg: Optional[str]) -> str:
    """Resolve repository from argument or current context."""
    if repo_arg:
        # Handle different formats: owner/repo, full URL, etc.
        if "/" in repo_arg and not repo_arg.startswith("http"):
            return repo_arg  # Already in owner/repo format
        elif "github.com" in repo_arg:
            # Extract owner/repo from URL
            if "github.com/" in repo_arg:
                repo_path = repo_arg.split("github.com/")[1]
                if repo_path.endswith(".git"):
                    repo_path = repo_path[:-4]
                return repo_path
    
    # Try to get from current context
    current_repo = get_current_repository()
    if current_repo:
        return current_repo
    
    # No repository found
    raise ValueError(
        "No repository specified. Use --repo owner/repo or run from a git repository directory."
    )

def setup_github_api(repo: Optional[str] = None) -> Tuple[str, Optional[str]]:
    """Setup GitHub API access with repository and token."""
    # Resolve repository
    repository = resolve_repository(repo)
    
    # Get token
    token = get_github_token()
    if not token:
        raise ValueError(
            "GitHub authentication required. Run 'gh auth login' first."
        )
    
    return repository, token


def output_result(data: Any, format_type: str, output_file: Optional[str] = None) -> None:
    """Output data in specified format to file or stdout."""
    import json
    import yaml
    import csv
    import io
    from tabulate import tabulate
    
    if format_type == "json":
        output = json.dumps(data, indent=2)
    elif format_type == "yaml":
        output = yaml.dump(data, default_flow_style=False)
    elif format_type == "csv":
        if isinstance(data, list) and data and isinstance(data[0], dict):
            output_buffer = io.StringIO()
            writer = csv.DictWriter(output_buffer, fieldnames=data[0].keys())
            writer.writeheader()
            writer.writerows(data)
            output = output_buffer.getvalue()
        else:
            output = str(data)
    elif format_type == "table":
        if isinstance(data, list) and data and isinstance(data[0], dict):
            output = tabulate(data, headers="keys", tablefmt="grid")
        elif isinstance(data, dict):
            output = tabulate(data.items(), headers=["Key", "Value"], tablefmt="grid")
        else:
            output = str(data)
    else:
        output = str(data)
    
    if output_file:
        with open(output_file, 'w') as f:
            f.write(output)
    else:
        print(output)


def add_common_arguments(parser: argparse.ArgumentParser) -> None:
    """Add common arguments used across subcommands."""
    parser.add_argument(
        "--repo", "-R",
        help="Target repository (owner/repo). Auto-detected if in git repository."
    )
    parser.add_argument(
        "--format",
        choices=["json", "yaml", "table", "csv"],
        default="table",
        help="Output format (default: table)"
    )
    parser.add_argument(
        "--output", "-o",
        help="Output to file instead of stdout"
    )
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="Verbose output"
    )
    parser.add_argument(
        "--quiet", "-q", 
        action="store_true",
        help="Minimal output"
    )


# Note: tqdm logging integration handled via context managers, not handlers

# Cache directory for persistent storage
CACHE_DIR: Final[Path] = Path.home() / ".cache" / "github_actions_optimizer"
CACHE_DIR.mkdir(parents=True, exist_ok=True)
REDIRECTIONS_CACHE_FILE: Final[Path] = CACHE_DIR / "redirections.json"
API_CACHE_DIR: Final[Path] = CACHE_DIR / "api_cache"
API_CACHE_DIR.mkdir(parents=True, exist_ok=True)

# Configure logging with tqdm integration


def setup_logging() -> logging.Logger:
    """Setup logging with tqdm integration."""
    logger = logging.getLogger(__name__)
    if not logger.handlers:
        # Use custom handler that integrates with tqdm progress bars
        class TqdmCompatibleHandler(logging.StreamHandler):
            """Custom logging handler that uses tqdm.write() for output."""

            def emit(self, record):
                try:
                    msg = self.format(record)
                    tqdm.write(msg, file=sys.stderr)
                except Exception:
                    self.handleError(record)

        handler = TqdmCompatibleHandler()

        formatter = logging.Formatter(
            "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)

    # Suppress verbose GitHub redirection messages from PyGithub/urllib3
    logging.getLogger("github").setLevel(logging.WARNING)
    logging.getLogger("urllib3").setLevel(logging.WARNING)
    logging.getLogger("requests").setLevel(logging.WARNING)

    return logger


logger = setup_logging()


def create_progress_bar_with_logging(
    iterable=None, total=None, desc=None, disable=False, **kwargs
):
    """Create a tqdm progress bar with proper logging integration.

    Usage pattern with logging:
    from tqdm.contrib.logging import logging_redirect_tqdm

    with logging_redirect_tqdm():
        for item in create_progress_bar_with_logging(items, desc="Processing"):
            logger.info(f"Processing {item}")
            process_item(item)
    """
    if iterable is not None:
        return tqdm(iterable, desc=desc, disable=disable, **kwargs)
    else:
        return tqdm(total=total, desc=desc, disable=disable, **kwargs)


# Base specification classes for unified runner/machine handling
@dataclass(frozen=True)
class BaseRunnerSpec:
    """Base specification for any compute resource (runner or machine)."""

    cores: int
    ram_gb: float
    storage_gb: float
    arch: Union[str, "CPUArchitecture"]

    @property
    def total_ram_mb(self) -> float:
        """Convert RAM to MB."""
        return self.ram_gb * 1024

    @property
    def is_arm_based(self) -> bool:
        """Check if architecture is ARM-based."""
        if isinstance(self.arch, str):
            return self.arch.lower() in ("arm64", "arm")
        # Handle CPUArchitecture enum after it's defined
        try:
            return self.arch in (CPUArchitecture.ARM64, CPUArchitecture.ARM)
        except NameError:
            # CPUArchitecture not yet defined, use string comparison
            return str(self.arch).lower() in ("arm64", "arm")

    @property
    def hourly_cost(self) -> float:
        """Get hourly cost in USD. Override in subclasses that have cost."""
        return 0.0


class BaseRunnerSpecModel(BaseModel):
    """Pydantic base model for runner specifications."""

    cores: PositiveInt = Field(le=64, description="Number of CPU cores")
    ram_gb: PositiveFloat = Field(le=1000, description="RAM in GB")
    storage_gb: PositiveFloat = Field(le=10000, description="Storage in GB")

    model_config = {
        "use_enum_values": True,  # Serialize enums by value, not type
        "validate_assignment": True,
    }

    @property
    def total_ram_mb(self) -> float:
        """Convert RAM to MB."""
        return self.ram_gb * 1024

    @property
    def is_arm_based(self) -> bool:
        """Check if architecture is ARM-based."""
        arch_attr = getattr(self, "arch", "x64")
        if isinstance(arch_attr, str):
            return arch_attr.lower() in ("arm64", "arm")
        # Handle CPUArchitecture enum
        try:
            return arch_attr in (CPUArchitecture.ARM64, CPUArchitecture.ARM)
        except (NameError, AttributeError):
            return str(arch_attr).lower() in ("arm64", "arm")

    @property
    def hourly_cost(self) -> float:
        """Get hourly cost in USD."""
        cost_per_minute = getattr(self, "cost_per_minute", 0.0)
        return cost_per_minute * 60


class OSFamily(StrEnum):
    """Operating system families."""

    UBUNTU = "ubuntu"
    FEDORA = "fedora"
    CENTOS = "centos"
    DEBIAN = "debian"
    WINDOWS = "windows"
    MACOS = "macos"


class CPUArchitecture(StrEnum):
    """CPU architectures."""

    X64 = "x64"
    X86 = "x86"
    ARM64 = "arm64"
    ARM = "arm"


class Platform(StrEnum):
    """Platform identifiers."""

    LINUX = "linux"
    WINDOWS = "windows"
    DARWIN = "darwin"


class PackageManager(StrEnum):
    """Package managers."""

    APT = "apt"
    DNF = "dnf"
    YUM = "yum"
    BREW = "brew"
    PACMAN = "pacman"
    ZYPPER = "zypper"
    CHOCO = "choco"
    WINGET = "winget"


class Shell(StrEnum):
    """Shell types."""

    BASH = "bash"
    ZSH = "zsh"
    FISH = "fish"
    PWSH = "pwsh"
    CMD = "cmd"


class FirewallType(StrEnum):
    """Firewall software types."""

    UFW = "ufw"
    FIREWALLD = "firewalld"
    IPTABLES = "iptables"
    WINDOWS_FIREWALL = "windows_firewall"
    PF = "pf"  # macOS pfctl
    IPFW = "ipfw"  # macOS ipfw (legacy)
    NETSH = "netsh"  # Windows netsh (legacy)
    NFTABLES = "nftables"  # Modern Linux
    NONE = "none"


class RunnerType(StrEnum):
    """GitHub runner types."""

    # Standard runners
    UBUNTU_LATEST = "ubuntu-latest"
    UBUNTU_20_04 = "ubuntu-20.04"
    UBUNTU_22_04 = "ubuntu-22.04"
    WINDOWS_LATEST = "windows-latest"
    WINDOWS_2019 = "windows-2019"
    WINDOWS_2022 = "windows-2022"
    MACOS_LATEST = "macos-latest"
    MACOS_12 = "macos-12"
    MACOS_13 = "macos-13"
    MACOS_14 = "macos-14"

    # x64-powered larger runners
    UBUNTU_LATEST_4_CORES = "ubuntu-latest-4-cores"
    UBUNTU_LATEST_8_CORES = "ubuntu-latest-8-cores"
    UBUNTU_LATEST_16_CORES = "ubuntu-latest-16-cores"
    UBUNTU_LATEST_32_CORES = "ubuntu-latest-32-cores"
    UBUNTU_LATEST_64_CORES = "ubuntu-latest-64-cores"
    WINDOWS_LATEST_4_CORES = "windows-latest-4-cores"
    WINDOWS_LATEST_8_CORES = "windows-latest-8-cores"
    WINDOWS_LATEST_16_CORES = "windows-latest-16-cores"
    WINDOWS_LATEST_32_CORES = "windows-latest-32-cores"
    WINDOWS_LATEST_64_CORES = "windows-latest-64-cores"
    MACOS_LATEST_12_CORES = "macos-latest-12-cores"

    # ARM64-powered larger runners
    UBUNTU_LATEST_2_CORES_ARM = "ubuntu-latest-2-cores-arm"
    UBUNTU_LATEST_4_CORES_ARM = "ubuntu-latest-4-cores-arm"
    UBUNTU_LATEST_8_CORES_ARM = "ubuntu-latest-8-cores-arm"
    UBUNTU_LATEST_16_CORES_ARM = "ubuntu-latest-16-cores-arm"
    UBUNTU_LATEST_32_CORES_ARM = "ubuntu-latest-32-cores-arm"
    UBUNTU_LATEST_64_CORES_ARM = "ubuntu-latest-64-cores-arm"
    WINDOWS_LATEST_2_CORES_ARM = "windows-latest-2-cores-arm"
    WINDOWS_LATEST_4_CORES_ARM = "windows-latest-4-cores-arm"
    WINDOWS_LATEST_8_CORES_ARM = "windows-latest-8-cores-arm"
    WINDOWS_LATEST_16_CORES_ARM = "windows-latest-16-cores-arm"
    WINDOWS_LATEST_32_CORES_ARM = "windows-latest-32-cores-arm"
    WINDOWS_LATEST_64_CORES_ARM = "windows-latest-64-cores-arm"
    MACOS_LATEST_6_CORES_M1 = "macos-latest-6-cores-m1"

    # GPU-powered larger runners
    UBUNTU_LATEST_4_CORES_GPU = "ubuntu-latest-4-cores-gpu"
    WINDOWS_LATEST_4_CORES_GPU = "windows-latest-4-cores-gpu"

    # Self-hosted runners
    SELF_HOSTED = "self-hosted"

    # Legacy/alias mappings
    UBUNTU_18_04 = "ubuntu-18.04"
    MACOS_11 = "macos-11"
    MACOS_10_15 = "macos-10.15"


@dataclass(frozen=True)
class RunnerSpecification:
    """Complete runner specification including cost and hardware.

    Note: Does not inherit from BaseRunnerSpec to maintain existing interface.
    """

    name: RunnerType
    cores: int
    ram_gb: float
    storage_gb: float
    arch: CPUArchitecture
    cost_per_minute: float
    os_family: OSFamily
    gpu: Optional[str] = None

    @property
    def is_github_hosted(self) -> bool:
        """Check if this is a GitHub-hosted runner."""
        return self.name != RunnerType.SELF_HOSTED

    @property
    def is_arm(self) -> bool:
        """Check if this runner uses ARM architecture."""
        return self.arch in (CPUArchitecture.ARM64, CPUArchitecture.ARM)

    @property
    def is_arm_based(self) -> bool:
        """Alias for is_arm for consistency with other spec classes."""
        return self.is_arm

    @property
    def total_ram_mb(self) -> float:
        """Convert RAM to MB."""
        return self.ram_gb * 1024

    @property
    def has_gpu(self) -> bool:
        """Check if this runner has GPU acceleration."""
        return self.gpu is not None

    @property
    def hourly_cost(self) -> float:
        """Get hourly cost in USD."""
        return self.cost_per_minute * 60

    @property
    def daily_cost(self) -> float:
        """Get daily cost in USD."""
        return self.hourly_cost * 24


# Unified runner specifications with pricing and hardware details
# Updated from official GitHub documentation: https://docs.github.com/en/billing/managing-billing-for-github-actions/about-billing-for-github-actions
UNIFIED_RUNNER_SPECS: Final[Dict[RunnerType, RunnerSpecification]] = {
    # Standard runners (most common)
    RunnerType.UBUNTU_LATEST: RunnerSpecification(
        name=RunnerType.UBUNTU_LATEST,
        cores=2,
        ram_gb=7,
        storage_gb=14,
        arch=CPUArchitecture.X64,
        cost_per_minute=0.008,
        os_family=OSFamily.UBUNTU,
    ),
    RunnerType.UBUNTU_20_04: RunnerSpecification(
        name=RunnerType.UBUNTU_20_04,
        cores=2,
        ram_gb=7,
        storage_gb=14,
        arch=CPUArchitecture.X64,
        cost_per_minute=0.008,
        os_family=OSFamily.UBUNTU,
    ),
    RunnerType.UBUNTU_22_04: RunnerSpecification(
        name=RunnerType.UBUNTU_22_04,
        cores=2,
        ram_gb=7,
        storage_gb=14,
        arch=CPUArchitecture.X64,
        cost_per_minute=0.008,
        os_family=OSFamily.UBUNTU,
    ),
    RunnerType.WINDOWS_LATEST: RunnerSpecification(
        name=RunnerType.WINDOWS_LATEST,
        cores=2,
        ram_gb=7,
        storage_gb=14,
        arch=CPUArchitecture.X64,
        cost_per_minute=0.016,
        os_family=OSFamily.WINDOWS,
    ),
    RunnerType.WINDOWS_2019: RunnerSpecification(
        name=RunnerType.WINDOWS_2019,
        cores=2,
        ram_gb=7,
        storage_gb=14,
        arch=CPUArchitecture.X64,
        cost_per_minute=0.016,
        os_family=OSFamily.WINDOWS,
    ),
    RunnerType.WINDOWS_2022: RunnerSpecification(
        name=RunnerType.WINDOWS_2022,
        cores=2,
        ram_gb=7,
        storage_gb=14,
        arch=CPUArchitecture.X64,
        cost_per_minute=0.016,
        os_family=OSFamily.WINDOWS,
    ),
    RunnerType.MACOS_LATEST: RunnerSpecification(
        name=RunnerType.MACOS_LATEST,
        cores=3,
        ram_gb=14,
        storage_gb=14,
        arch=CPUArchitecture.X64,
        cost_per_minute=0.08,
        os_family=OSFamily.MACOS,
    ),
    RunnerType.MACOS_12: RunnerSpecification(
        name=RunnerType.MACOS_12,
        cores=3,
        ram_gb=14,
        storage_gb=14,
        arch=CPUArchitecture.X64,
        cost_per_minute=0.08,
        os_family=OSFamily.MACOS,
    ),
    RunnerType.MACOS_13: RunnerSpecification(
        name=RunnerType.MACOS_13,
        cores=3,
        ram_gb=14,
        storage_gb=14,
        arch=CPUArchitecture.X64,
        cost_per_minute=0.08,
        os_family=OSFamily.MACOS,
    ),
    RunnerType.MACOS_14: RunnerSpecification(
        name=RunnerType.MACOS_14,
        cores=4,
        ram_gb=14,
        storage_gb=14,
        arch=CPUArchitecture.ARM64,
        cost_per_minute=0.08,
        os_family=OSFamily.MACOS,
    ),
    # x64-powered larger runners
    RunnerType.UBUNTU_LATEST_4_CORES: RunnerSpecification(
        name=RunnerType.UBUNTU_LATEST_4_CORES,
        cores=4,
        ram_gb=16,
        storage_gb=14,
        arch=CPUArchitecture.X64,
        cost_per_minute=0.016,
        os_family=OSFamily.UBUNTU,
    ),
    RunnerType.UBUNTU_LATEST_8_CORES: RunnerSpecification(
        name=RunnerType.UBUNTU_LATEST_8_CORES,
        cores=8,
        ram_gb=32,
        storage_gb=14,
        arch=CPUArchitecture.X64,
        cost_per_minute=0.032,
        os_family=OSFamily.UBUNTU,
    ),
    RunnerType.UBUNTU_LATEST_16_CORES: RunnerSpecification(
        name=RunnerType.UBUNTU_LATEST_16_CORES,
        cores=16,
        ram_gb=64,
        storage_gb=14,
        arch=CPUArchitecture.X64,
        cost_per_minute=0.064,
        os_family=OSFamily.UBUNTU,
    ),
    RunnerType.UBUNTU_LATEST_32_CORES: RunnerSpecification(
        name=RunnerType.UBUNTU_LATEST_32_CORES,
        cores=32,
        ram_gb=128,
        storage_gb=14,
        arch=CPUArchitecture.X64,
        cost_per_minute=0.128,
        os_family=OSFamily.UBUNTU,
    ),
    RunnerType.UBUNTU_LATEST_64_CORES: RunnerSpecification(
        name=RunnerType.UBUNTU_LATEST_64_CORES,
        cores=64,
        ram_gb=256,
        storage_gb=14,
        arch=CPUArchitecture.X64,
        cost_per_minute=0.256,
        os_family=OSFamily.UBUNTU,
    ),
    RunnerType.WINDOWS_LATEST_4_CORES: RunnerSpecification(
        name=RunnerType.WINDOWS_LATEST_4_CORES,
        cores=4,
        ram_gb=16,
        storage_gb=14,
        arch=CPUArchitecture.X64,
        cost_per_minute=0.032,
        os_family=OSFamily.WINDOWS,
    ),
    RunnerType.WINDOWS_LATEST_8_CORES: RunnerSpecification(
        name=RunnerType.WINDOWS_LATEST_8_CORES,
        cores=8,
        ram_gb=32,
        storage_gb=14,
        arch=CPUArchitecture.X64,
        cost_per_minute=0.064,
        os_family=OSFamily.WINDOWS,
    ),
    RunnerType.WINDOWS_LATEST_16_CORES: RunnerSpecification(
        name=RunnerType.WINDOWS_LATEST_16_CORES,
        cores=16,
        ram_gb=64,
        storage_gb=14,
        arch=CPUArchitecture.X64,
        cost_per_minute=0.128,
        os_family=OSFamily.WINDOWS,
    ),
    RunnerType.WINDOWS_LATEST_32_CORES: RunnerSpecification(
        name=RunnerType.WINDOWS_LATEST_32_CORES,
        cores=32,
        ram_gb=128,
        storage_gb=14,
        arch=CPUArchitecture.X64,
        cost_per_minute=0.256,
        os_family=OSFamily.WINDOWS,
    ),
    RunnerType.WINDOWS_LATEST_64_CORES: RunnerSpecification(
        name=RunnerType.WINDOWS_LATEST_64_CORES,
        cores=64,
        ram_gb=256,
        storage_gb=14,
        arch=CPUArchitecture.X64,
        cost_per_minute=0.512,
        os_family=OSFamily.WINDOWS,
    ),
    RunnerType.MACOS_LATEST_12_CORES: RunnerSpecification(
        name=RunnerType.MACOS_LATEST_12_CORES,
        cores=12,
        ram_gb=30,
        storage_gb=14,
        arch=CPUArchitecture.X64,
        cost_per_minute=0.12,
        os_family=OSFamily.MACOS,
    ),
    # ARM64-powered larger runners (more cost-effective)
    RunnerType.UBUNTU_LATEST_2_CORES_ARM: RunnerSpecification(
        name=RunnerType.UBUNTU_LATEST_2_CORES_ARM,
        cores=2,
        ram_gb=8,
        storage_gb=14,
        arch=CPUArchitecture.ARM64,
        cost_per_minute=0.005,
        os_family=OSFamily.UBUNTU,
    ),
    RunnerType.UBUNTU_LATEST_4_CORES_ARM: RunnerSpecification(
        name=RunnerType.UBUNTU_LATEST_4_CORES_ARM,
        cores=4,
        ram_gb=16,
        storage_gb=14,
        arch=CPUArchitecture.ARM64,
        cost_per_minute=0.01,
        os_family=OSFamily.UBUNTU,
    ),
    RunnerType.UBUNTU_LATEST_8_CORES_ARM: RunnerSpecification(
        name=RunnerType.UBUNTU_LATEST_8_CORES_ARM,
        cores=8,
        ram_gb=32,
        storage_gb=14,
        arch=CPUArchitecture.ARM64,
        cost_per_minute=0.02,
        os_family=OSFamily.UBUNTU,
    ),
    RunnerType.UBUNTU_LATEST_16_CORES_ARM: RunnerSpecification(
        name=RunnerType.UBUNTU_LATEST_16_CORES_ARM,
        cores=16,
        ram_gb=64,
        storage_gb=14,
        arch=CPUArchitecture.ARM64,
        cost_per_minute=0.04,
        os_family=OSFamily.UBUNTU,
    ),
    RunnerType.UBUNTU_LATEST_32_CORES_ARM: RunnerSpecification(
        name=RunnerType.UBUNTU_LATEST_32_CORES_ARM,
        cores=32,
        ram_gb=128,
        storage_gb=14,
        arch=CPUArchitecture.ARM64,
        cost_per_minute=0.08,
        os_family=OSFamily.UBUNTU,
    ),
    RunnerType.UBUNTU_LATEST_64_CORES_ARM: RunnerSpecification(
        name=RunnerType.UBUNTU_LATEST_64_CORES_ARM,
        cores=64,
        ram_gb=256,
        storage_gb=14,
        arch=CPUArchitecture.ARM64,
        cost_per_minute=0.16,
        os_family=OSFamily.UBUNTU,
    ),
    RunnerType.WINDOWS_LATEST_2_CORES_ARM: RunnerSpecification(
        name=RunnerType.WINDOWS_LATEST_2_CORES_ARM,
        cores=2,
        ram_gb=8,
        storage_gb=14,
        arch=CPUArchitecture.ARM64,
        cost_per_minute=0.01,
        os_family=OSFamily.WINDOWS,
    ),
    RunnerType.WINDOWS_LATEST_4_CORES_ARM: RunnerSpecification(
        name=RunnerType.WINDOWS_LATEST_4_CORES_ARM,
        cores=4,
        ram_gb=16,
        storage_gb=14,
        arch=CPUArchitecture.ARM64,
        cost_per_minute=0.02,
        os_family=OSFamily.WINDOWS,
    ),
    RunnerType.WINDOWS_LATEST_8_CORES_ARM: RunnerSpecification(
        name=RunnerType.WINDOWS_LATEST_8_CORES_ARM,
        cores=8,
        ram_gb=32,
        storage_gb=14,
        arch=CPUArchitecture.ARM64,
        cost_per_minute=0.04,
        os_family=OSFamily.WINDOWS,
    ),
    RunnerType.WINDOWS_LATEST_16_CORES_ARM: RunnerSpecification(
        name=RunnerType.WINDOWS_LATEST_16_CORES_ARM,
        cores=16,
        ram_gb=64,
        storage_gb=14,
        arch=CPUArchitecture.ARM64,
        cost_per_minute=0.08,
        os_family=OSFamily.WINDOWS,
    ),
    RunnerType.WINDOWS_LATEST_32_CORES_ARM: RunnerSpecification(
        name=RunnerType.WINDOWS_LATEST_32_CORES_ARM,
        cores=32,
        ram_gb=128,
        storage_gb=14,
        arch=CPUArchitecture.ARM64,
        cost_per_minute=0.16,
        os_family=OSFamily.WINDOWS,
    ),
    RunnerType.WINDOWS_LATEST_64_CORES_ARM: RunnerSpecification(
        name=RunnerType.WINDOWS_LATEST_64_CORES_ARM,
        cores=64,
        ram_gb=256,
        storage_gb=14,
        arch=CPUArchitecture.ARM64,
        cost_per_minute=0.32,
        os_family=OSFamily.WINDOWS,
    ),
    RunnerType.MACOS_LATEST_6_CORES_M1: RunnerSpecification(
        name=RunnerType.MACOS_LATEST_6_CORES_M1,
        cores=6,
        ram_gb=24,
        storage_gb=14,
        arch=CPUArchitecture.ARM64,
        cost_per_minute=0.16,
        os_family=OSFamily.MACOS,
    ),
    # GPU-powered larger runners
    RunnerType.UBUNTU_LATEST_4_CORES_GPU: RunnerSpecification(
        name=RunnerType.UBUNTU_LATEST_4_CORES_GPU,
        cores=4,
        ram_gb=16,
        storage_gb=14,
        arch=CPUArchitecture.X64,
        cost_per_minute=0.07,
        os_family=OSFamily.UBUNTU,
        gpu="T4",
    ),
    RunnerType.WINDOWS_LATEST_4_CORES_GPU: RunnerSpecification(
        name=RunnerType.WINDOWS_LATEST_4_CORES_GPU,
        cores=4,
        ram_gb=16,
        storage_gb=14,
        arch=CPUArchitecture.X64,
        cost_per_minute=0.14,
        os_family=OSFamily.WINDOWS,
        gpu="T4",
    ),
    # Self-hosted runners (free)
    RunnerType.SELF_HOSTED: RunnerSpecification(
        name=RunnerType.SELF_HOSTED,
        cores=2,
        ram_gb=7,
        storage_gb=14,
        arch=CPUArchitecture.X64,
        cost_per_minute=0.0,
        os_family=OSFamily.UBUNTU,
    ),
    # Legacy/alias mappings
    RunnerType.UBUNTU_18_04: RunnerSpecification(
        name=RunnerType.UBUNTU_18_04,
        cores=2,
        ram_gb=7,
        storage_gb=14,
        arch=CPUArchitecture.X64,
        cost_per_minute=0.008,
        os_family=OSFamily.UBUNTU,
    ),
    RunnerType.MACOS_11: RunnerSpecification(
        name=RunnerType.MACOS_11,
        cores=3,
        ram_gb=14,
        storage_gb=14,
        arch=CPUArchitecture.X64,
        cost_per_minute=0.08,
        os_family=OSFamily.MACOS,
    ),
    RunnerType.MACOS_10_15: RunnerSpecification(
        name=RunnerType.MACOS_10_15,
        cores=3,
        ram_gb=14,
        storage_gb=14,
        arch=CPUArchitecture.X64,
        cost_per_minute=0.08,
        os_family=OSFamily.MACOS,
    ),
}

# Legacy dictionaries for backward compatibility
PRICING: Final[Dict[str, float]] = {
    spec.name.value: spec.cost_per_minute for spec in UNIFIED_RUNNER_SPECS.values()
}

RUNNER_SPECS: Final[Dict[str, Dict[str, Union[int, float, str]]]] = {
    spec.name.value: {
        "cores": spec.cores,
        "ram_gb": spec.ram_gb,
        "storage_gb": spec.storage_gb,
        "arch": spec.arch.value,
        **({"gpu": spec.gpu} if spec.gpu else {}),
    }
    for spec in UNIFIED_RUNNER_SPECS.values()
}

# Failure rate thresholds for risk assessment
RISK_THRESHOLDS: Final[Dict[str, float]] = {
    "CRITICAL": 0.8,  # 80%+ failure rate
    "HIGH": 0.5,  # 50%+ failure rate
    "MEDIUM": 0.3,  # 30%+ failure rate
    "LOW": 0.1,  # 10%+ failure rate
}


class RepositoryRedirectionDetector:
    """Detects and handles GitHub repository redirections with persistent caching."""

    def __init__(self, github_client: Github):
        self.github = github_client
        self._redirection_cache = self._load_redirections_cache()

    def _load_redirections_cache(self) -> Dict[str, Tuple[str, str]]:
        """Load redirections cache from disk."""
        try:
            if REDIRECTIONS_CACHE_FILE.exists():
                with open(REDIRECTIONS_CACHE_FILE, "r") as f:
                    data = json.load(f)
                    return {k: tuple(v) for k, v in data.items()}
        except Exception as e:
            logger.warning(f"Could not load redirections cache: {e}")
        return {}

    def _save_redirections_cache(self) -> None:
        """Save redirections cache to disk."""
        try:
            with open(REDIRECTIONS_CACHE_FILE, "w") as f:
                # Convert tuples to lists for JSON serialization
                data = {k: list(v) for k, v in self._redirection_cache.items()}
                json.dump(data, f, indent=2)
        except Exception as e:
            logger.warning(f"Could not save redirections cache: {e}")

    def get_canonical_repo_name(self, owner: str, repo: str) -> Tuple[str, str]:
        """Get the canonical repository name, handling redirections."""
        repo_key = f"{owner}/{repo}"

        # Check persistent cache first
        if repo_key in self._redirection_cache:
            cached_owner, cached_repo = self._redirection_cache[repo_key]
            if cached_owner != owner or cached_repo != repo:
                logger.debug(
                    f"ðŸ“ Using cached redirection: {repo_key} â†’ {cached_owner}/{cached_repo}"
                )
            return cached_owner, cached_repo

        try:
            repo_obj = self.github.get_repo(repo_key)
            canonical_owner = repo_obj.owner.login
            canonical_name = repo_obj.name
            canonical_key = f"{canonical_owner}/{canonical_name}"

            if canonical_key != repo_key:
                logger.info(f"ðŸ“ Repository redirected: {repo_key} â†’ {canonical_key}")
                self._redirection_cache[repo_key] = (canonical_owner, canonical_name)
                # Save to persistent cache immediately
                self._save_redirections_cache()
                return canonical_owner, canonical_name
            else:
                self._redirection_cache[repo_key] = (owner, repo)
                # Save to persistent cache
                self._save_redirections_cache()
                return owner, repo

        except Exception as e:
            logger.warning(f"Could not detect redirection for {repo_key}: {e}")
            return owner, repo


def get_github_token() -> Optional[str]:
    """Get GitHub token from various sources in order of preference."""
    # 1. Environment variable (explicit override)
    token = os.getenv("GITHUB_TOKEN")
    if token:
        logger.info("ðŸ”‘ Using GITHUB_TOKEN environment variable")
        return token

    # 2. GitHub CLI authentication
    try:
        result = subprocess.run(
            ["gh", "auth", "token"], capture_output=True, text=True, timeout=10
        )
        if result.returncode == 0:
            token = result.stdout.strip()
            if token:
                logger.info("ðŸ”‘ Using GitHub CLI authentication token")
                return token
    except (
        subprocess.TimeoutExpired,
        subprocess.CalledProcessError,
        FileNotFoundError,
    ):
        pass

    # 3. No authentication available
    logger.warning("âš ï¸  No GitHub authentication found")
    logger.info("ðŸ’¡ Options:")
    logger.info("   - Set GITHUB_TOKEN environment variable")
    logger.info("   - Authenticate with GitHub CLI: gh auth login")
    logger.info("   - Create token at: https://github.com/settings/tokens")
    return None


def get_latest_github_runner_version() -> str:
    """Get the latest GitHub Actions runner version from GitHub API."""
    try:
        token = get_github_token()
        if token:
            github_client = Github(token)
        else:
            github_client = Github()  # Anonymous access with rate limits

        # Get the latest release from actions/runner repository
        repo = github_client.get_repo("actions/runner")
        latest_release = repo.get_latest_release()
        version = latest_release.tag_name

        # Remove 'v' prefix if present
        if version.startswith("v"):
            version = version[1:]

        logger.info(f"Latest GitHub runner version: {version}")
        return version

    except Exception as e:
        logger.warning(f"Failed to fetch latest runner version: {e}")
        # Fallback to a reasonable default
        return "2.311.0"


def parse_repository_url(repo_url: str) -> Tuple[Optional[str], Optional[str]]:
    """
    Parse a repository URL to extract owner and repo name.

    Args:
        repo_url: Repository URL (https://github.com/owner/repo or owner/repo format)

    Returns:
        Tuple of (owner, repo) or (None, None) if parsing fails
    """
    try:
        # Handle owner/repo format
        if "/" in repo_url and not repo_url.startswith(("http://", "https://")):
            parts = repo_url.strip().split("/")
            if len(parts) == 2:
                return parts[0], parts[1]

        # Handle full URL format
        if repo_url.startswith(("http://", "https://")):
            parsed = urlparse(repo_url)

            # Extract path and remove leading/trailing slashes
            path = parsed.path.strip("/")

            # Remove .git suffix if present
            if path.endswith(".git"):
                path = path[:-4]

            # Split into owner/repo
            parts = path.split("/")
            if len(parts) >= 2:
                return parts[0], parts[1]

        return None, None
    except Exception as e:
        logger.debug(f"Failed to parse repository URL '{repo_url}': {e}")
        return None, None


def resolve_canonical_repository_url(repo_url: str) -> str:
    """
    Resolve the canonical repository URL by following GitHub redirects.

    GitHub may redirect repositories that have been transferred or renamed.
    This function uses the GitHub CLI to get the canonical URL.

    Args:
        repo_url: Repository URL to resolve

    Returns:
        Canonical repository URL, or original URL if resolution fails
    """
    try:
        # Parse the original URL to get owner/repo
        owner, repo = parse_repository_url(repo_url)
        if not owner or not repo:
            return repo_url

        # Use GitHub CLI to get the canonical repository information
        result = subprocess.run(
            ["gh", "repo", "view", f"{owner}/{repo}", "--json", "nameWithOwner,url"],
            capture_output=True,
            text=True,
            timeout=10,
        )

        if result.returncode == 0:
            import json

            repo_info = json.loads(result.stdout)
            canonical_url = repo_info.get("url", repo_url)
            logger.info(f"Resolved repository URL: {repo_url} -> {canonical_url}")
            return canonical_url
        else:
            logger.warning(
                f"Could not resolve repository URL via GitHub CLI: {result.stderr}"
            )
            return repo_url

    except Exception as e:
        logger.debug(
            f"Failed to resolve canonical repository URL for '{repo_url}': {e}"
        )
        return repo_url


def extract_repository_url_from_directory(
    directory: Optional[str] = None,
) -> Optional[str]:
    """Extract repository URL from git configuration in the specified directory."""
    if directory is None:
        directory = os.getcwd()

    try:
        # Check if we're in a git repository
        result = subprocess.run(
            ["git", "rev-parse", "--is-inside-work-tree"],
            cwd=directory,
            capture_output=True,
            text=True,
            timeout=10,
        )

        if result.returncode != 0:
            logger.debug(f"Not a git repository: {directory}")
            return None

        # Get the origin remote URL
        result = subprocess.run(
            ["git", "remote", "get-url", "origin"],
            cwd=directory,
            capture_output=True,
            text=True,
            timeout=10,
        )

        if result.returncode != 0:
            logger.debug("No origin remote found")
            return None

        remote_url = result.stdout.strip()
        logger.info(f"Detected git remote origin: {remote_url}")

        # Convert SSH URLs to HTTPS
        if remote_url.startswith("git@github.com:"):
            remote_url = remote_url.replace("git@github.com:", "https://github.com/")

        # Remove .git suffix if present
        if remote_url.endswith(".git"):
            remote_url = remote_url[:-4]

        return remote_url

    except Exception as e:
        logger.debug(f"Failed to extract repository URL from directory: {e}")
        return None


def verify_runners_status(
    config_files: List[str], repo_url: Optional[str] = None
) -> None:
    """
    Verify the status of all runners defined in MachineSpecs against GitHub repository.

    Args:
        config_files: List of YAML configuration files containing runner definitions
        repo_url: Repository URL to check (auto-detected if not provided)
    """
    # Use centralized repository URL detection
    if not repo_url:
        repo_url = extract_repository_url_from_directory()
        if repo_url:
            # Resolve canonical URL to handle redirects/transfers
            repo_url = resolve_canonical_repository_url(repo_url)

    if not repo_url:
        print("âŒ Could not determine repository URL. Please specify with --repo-url")
        return

    # Parse repository URL to get owner/repo
    owner, repo_name = parse_repository_url(repo_url)
    if not owner or not repo_name:
        print(f"âŒ Invalid repository URL format: {repo_url}")
        return

    print(f"ðŸ” Verifying runners for repository: {owner}/{repo_name}")

    # Collect all runner names from configurations using centralized config loading
    expected_runners = set()

    if config_files:
        try:
            # First try to load as runner configuration files using centralized merge_configs
            merged_config = merge_configs(config_files)
            for runner in merged_config["runners"]:
                expected_runners.add(runner["name"])

            if expected_runners:
                print(
                    f"ðŸ“‹ Found {len(expected_runners)} runner(s) in configuration files:"
                )
                for name in sorted(expected_runners):
                    print(f"   - {name}")
            else:
                # If no runners found, try loading as machine specification files
                print(
                    "ðŸ“‹ No runners found in config files, checking for machine specifications..."
                )
                for config_file in config_files:
                    try:
                        # Use centralized config loading
                        config = load_config_file(config_file)
                        # Check if this is a machine specification file
                        if config and "machine_spec" in config:
                            machine_spec = config["machine_spec"]
                            if "hostname" in machine_spec:
                                expected_runners.add(machine_spec["hostname"])
                                print(
                                    f"   - Found machine spec for: {machine_spec['hostname']} (from {config_file})"
                                )
                        elif config and "hostname" in config:
                            # Direct machine spec format
                            expected_runners.add(config["hostname"])
                            print(
                                f"   - Found machine spec for: {config['hostname']} (from {config_file})"
                            )
                    except Exception as e:
                        print(f"âŒ Error loading {config_file}: {e}")
                        continue

                if expected_runners:
                    print(f"ðŸ“‹ Found {len(expected_runners)} machine specification(s):")
                    for name in sorted(expected_runners):
                        print(f"   - {name}")

        except Exception as e:
            print(f"âŒ Failed to load configuration files: {e}")
            return
    else:
        print(
            "âš ï¸  No configuration files provided. Use --self-hosted-runner-config to specify."
        )
        return

    # Use PyGithub instead of gh binary
    try:
        # Initialize GitHub API client
        github_api = GitHubAPI()
        repo = github_api.github.get_repo(f"{owner}/{repo_name}")

        # Get actual runners using PyGithub
        actual_runners = {}
        for runner in repo.get_self_hosted_runners():
            actual_runners[runner.name] = {
                "name": runner.name,
                "status": runner.status,
                "busy": runner.busy,
                "labels": [label["name"] for label in runner.labels],
            }

        print(f"\nðŸ¤– GitHub repository has {len(actual_runners)} registered runner(s):")
        for name, info in sorted(actual_runners.items()):
            status_emoji = "ðŸŸ¢" if info["status"] == "online" else "ðŸ”´"
            busy_status = "ðŸƒ BUSY" if info["busy"] else "ðŸ’¤ IDLE"
            labels = ", ".join(info["labels"]) if info["labels"] else "none"
            print(
                f"   {status_emoji} {name} - {info['status'].upper()} - {busy_status} - Labels: {labels}"
            )

        # Compare expected vs actual
        print(f"\nðŸ“Š Runner Status Verification:")

        # Check for missing runners
        missing_runners = expected_runners - set(actual_runners.keys())
        if missing_runners:
            print(f"âŒ Missing runners (defined in config but not registered):")
            for name in sorted(missing_runners):
                print(f"   - {name}")

        # Check for extra runners
        extra_runners = set(actual_runners.keys()) - expected_runners
        if extra_runners:
            print(f"âš ï¸  Extra runners (registered but not in config):")
            for name in sorted(extra_runners):
                info = actual_runners[name]
                status_emoji = "ðŸŸ¢" if info["status"] == "online" else "ðŸ”´"
                print(f"   {status_emoji} {name} - {info['status'].upper()}")

        # Check status of expected runners
        online_expected = []
        offline_expected = []
        for name in expected_runners:
            if name in actual_runners:
                if actual_runners[name]["status"] == "online":
                    online_expected.append(name)
                else:
                    offline_expected.append(name)

        if online_expected:
            print(f"âœ… Online runners ({len(online_expected)}):")
            for name in sorted(online_expected):
                info = actual_runners[name]
                busy_status = "ðŸƒ BUSY" if info["busy"] else "ðŸ’¤ IDLE"
                print(f"   ðŸŸ¢ {name} - {busy_status}")

        if offline_expected:
            print(f"ðŸ”´ Offline runners ({len(offline_expected)}):")
            for name in sorted(offline_expected):
                print(f"   ðŸ”´ {name}")

        # Summary
        total_expected = len(expected_runners)
        total_registered = len(expected_runners & set(actual_runners.keys()))
        total_online = len(online_expected)

        print(f"\nðŸ“ˆ Summary:")
        print(f"   Expected runners: {total_expected}")
        print(
            f"   Registered: {total_registered}/{total_expected} ({100*total_registered//total_expected if total_expected > 0 else 0}%)"
        )
        print(
            f"   Online: {total_online}/{total_registered} ({100*total_online//total_registered if total_registered > 0 else 0}%)"
        )

        if total_registered == total_expected and total_online == total_registered:
            print("ðŸŽ‰ All runners are registered and online!")
        elif total_registered == total_expected:
            print("âš ï¸  All runners are registered but some are offline")
        else:
            print("âŒ Some runners are missing from GitHub repository")

    except Exception as e:
        print(f"âŒ Error verifying runners: {e}")


class GitHubAPI:
    """GitHub API client using PyGithub with pandas-based data processing and tqdm progress bars."""

    def __init__(self, token: Optional[str] = None):
        """Initialize GitHub API client with authentication."""
        self.token = token or get_github_token()

        if not self.token:
            logger.error("No GitHub token available")
            logger.info("Options to authenticate:")
            logger.info("  1. Set GITHUB_TOKEN environment variable")
            logger.info("  2. Use GitHub CLI: gh auth login")
            logger.info("  3. Create token at: https://github.com/settings/tokens")
            raise Exception("GitHub authentication required")

        logger.info("ðŸ™ Using PyGithub library with authentication")
        # Use new Auth.Token method to avoid deprecation warning
        auth = Auth.Token(self.token)
        self.github = Github(auth=auth)

        # Initialize redirection detector
        self.redirection_detector = RepositoryRedirectionDetector(self.github)

        # Initialize API caching
        self.use_cache_only = False  # Will be set by command line argument

    def _get_cache_key(self, method: str, *args) -> str:
        """Generate cache key for API call."""
        key_data = f"{method}:{':'.join(str(arg) for arg in args)}"
        return hashlib.md5(key_data.encode()).hexdigest()

    def _get_cache_file(self, cache_key: str) -> Path:
        """Get cache file path for given key."""
        return API_CACHE_DIR / f"{cache_key}.pkl"

    def _load_from_cache(self, cache_key: str) -> Optional[Any]:
        """Load data from cache if available and fresh."""
        cache_file = self._get_cache_file(cache_key)
        if cache_file.exists():
            try:
                with open(cache_file, "rb") as f:
                    cached_data = pickle.load(f)
                    # Check if cache is less than 1 hour old for workflows, 10 minutes for runs
                    cache_age = time.time() - cache_file.stat().st_mtime
                    # 1h for workflows, 10m for runs
                    max_age = 3600 if "workflow" in cache_key else 600

                    if cache_age < max_age:
                        logger.debug(f"ðŸ“‹ Using cached data (age: {cache_age:.0f}s)")
                        return cached_data
                    else:
                        logger.debug(f"ðŸ—‘ï¸ Cache expired (age: {cache_age:.0f}s)")
            except Exception as e:
                logger.warning(f"Could not load cache: {e}")
        return None

    def _save_to_cache(self, cache_key: str, data: Any) -> None:
        """Save data to cache."""
        try:
            cache_file = self._get_cache_file(cache_key)
            with open(cache_file, "wb") as f:
                pickle.dump(data, f)
            logger.debug(f"ðŸ’¾ Saved to cache: {cache_file.name}")
        except Exception as e:
            logger.warning(f"Could not save to cache: {e}")

    def set_cache_only_mode(self, enabled: bool) -> None:
        """Enable or disable cache-only mode."""
        self.use_cache_only = enabled
        if enabled:
            logger.info("ðŸ“‹ Using cache-only mode - no new API requests will be made")

        # Check rate limits
        try:
            rate_limit = self.github.get_rate_limit()
            core_remaining = rate_limit.core.remaining
            print(f"ï¿½ GitHub API rate limit: {core_remaining} requests remaining")

            if core_remaining < 100:
                print(f"âš ï¸  Warning: Low rate limit remaining ({core_remaining})")
                print(
                    "ðŸ’¡ Consider waiting or using authenticated requests for higher limits"
                )
        except Exception as e:
            print(f"âš ï¸  Could not check rate limits: {e}")

    def get_repository_workflows(self, owner: str, repo: str) -> List[Dict[str, Any]]:
        """Get all workflows for a repository (alias for get_workflows)."""
        return self.get_workflows(owner, repo)

    def get_workflows(self, owner: str, repo: str) -> pd.DataFrame:
        """Get all workflows for a repository using PyGithub, handling redirections."""
        # Handle repository redirection
        canonical_owner, canonical_repo = (
            self.redirection_detector.get_canonical_repo_name(owner, repo)
        )

        # Check cache first
        cache_key = self._get_cache_key("workflows", canonical_owner, canonical_repo)
        cached_data = self._load_from_cache(cache_key)
        if cached_data is not None:
            logger.info(f"ðŸ“‹ Found {len(cached_data)} workflows (cached)")
            return pd.DataFrame(cached_data)

        if self.use_cache_only:
            logger.warning("No cached workflows found and cache-only mode enabled")
            return pd.DataFrame()

        try:
            repo_obj = self.github.get_repo(f"{canonical_owner}/{canonical_repo}")
            workflows = repo_obj.get_workflows()

            workflow_data = []
            for workflow in tqdm(
                workflows, desc="ðŸ“‹ Fetching workflows", unit="workflow"
            ):
                workflow_data.append(
                    {
                        "id": workflow.id,
                        "name": workflow.name,
                        "path": workflow.path,
                        "state": workflow.state,
                    }
                )

            # Save to cache
            self._save_to_cache(cache_key, workflow_data)

            df = pd.DataFrame(workflow_data)
            logger.info(f"ðŸ“‹ Found {len(df)} workflows")
            return df

        except RateLimitExceededException:
            logger.error("GitHub API rate limit exceeded!")
            logger.info(
                "ðŸ’¡ Set GITHUB_TOKEN or authenticate with GitHub CLI for higher limits"
            )
            sys.exit(1)
        except UnknownObjectException:
            error_msg = (
                f"Repository '{canonical_owner}/{canonical_repo}' not found or not accessible. Check if:\n"
                f"  - Repository name is correct\n"
                f"  - Repository is public or you have access\n"
                f"  - GitHub token has necessary permissions"
            )
            raise Exception(error_msg)
        except Exception as e:
            raise Exception(f"Error fetching workflows: {e}")

    def get_workflow_runs_dataframe(
        self, owner: str, repo: str, workflow_id: int, per_page: int = 100
    ) -> pd.DataFrame:
        """Get workflow runs for a specific workflow using PyGithub, returning DataFrame."""
        canonical_owner, canonical_repo = (
            self.redirection_detector.get_canonical_repo_name(owner, repo)
        )

        try:
            repo_obj = self.github.get_repo(f"{canonical_owner}/{canonical_repo}")
            workflow = repo_obj.get_workflow(workflow_id)
            runs = workflow.get_runs()

            run_data = []
            count = 0
            for run in runs:
                if count >= per_page:
                    break

                run_data.append(
                    {
                        "id": run.id,
                        "status": run.status,
                        "conclusion": run.conclusion,
                        "created_at": (
                            run.created_at.isoformat() if run.created_at else None
                        ),
                        "updated_at": (
                            run.updated_at.isoformat() if run.updated_at else None
                        ),
                        "event": run.event,
                        "head_sha": run.head_sha,
                        "head_branch": run.head_branch,
                        "workflow_id": workflow_id,
                    }
                )
                count += 1

            return pd.DataFrame(run_data)

        except RateLimitExceededException:
            logger.error("Rate limit exceeded!")
            sys.exit(1)
        except Exception as e:
            if "404" in str(e):
                logger.warning(f"Workflow {workflow_id} not found or not accessible")
                return pd.DataFrame()  # Return empty DataFrame
            else:
                raise Exception(f"GitHub API error: {e}")

    def get_workflow_runs(
        self, owner: str, repo: str, workflow_id: int, per_page: int = 100
    ) -> List[Dict[str, Any]]:
        """Get workflow runs for a specific workflow using PyGithub."""
        df = self.get_workflow_runs_dataframe(owner, repo, workflow_id, per_page)
        return df.to_dict("records")
        try:
            repo_obj = self.github.get_repo(f"{owner}/{repo}")
            workflow = repo_obj.get_workflow(workflow_id)
            runs = workflow.get_runs()

            run_list = []
            count = 0
            for run in runs:
                if count >= 500:  # Limit to avoid excessive API calls
                    break

                run_list.append(
                    {
                        "id": run.id,
                        "status": run.status,
                        "conclusion": run.conclusion,
                        "created_at": (
                            run.created_at.isoformat() if run.created_at else None
                        ),
                        "updated_at": (
                            run.updated_at.isoformat() if run.updated_at else None
                        ),
                        "event": run.event,
                        "head_sha": run.head_sha,
                        "head_branch": run.head_branch,
                    }
                )
                count += 1

            return run_list

        except RateLimitExceededException:
            print("âŒ GitHub API rate limit exceeded!")
            print(
                "ðŸ’¡ Set GITHUB_TOKEN or authenticate with GitHub CLI for higher limits"
            )
            sys.exit(1)
        except Exception as e:
            raise Exception(f"Error fetching workflow runs: {e}")

    def _get_workflow_runs_fallback(
        self, owner: str, repo: str, workflow_id: int, per_page: int = 100
    ) -> List[Dict[str, Any]]:
        """Fallback implementation for workflow runs."""
        all_runs = []
        page = 1

        while len(all_runs) < 500:  # Limit to avoid excessive API calls
            url = f"https://api.github.com/repos/{owner}/{repo}/actions/workflows/{workflow_id}/runs?per_page={per_page}&page={page}"
            headers = {
                "Accept": "application/vnd.github.v3+json",
                "User-Agent": "GitHub-Actions-Cost-Analyzer/1.0",
            }

            if self.token:
                headers["Authorization"] = f"token {self.token}"

            try:
                req = urllib.request.Request(url, headers=headers)
                with urllib.request.urlopen(req) as response:
                    data = json.loads(response.read().decode())
                    runs = data.get("workflow_runs", [])

                    if not runs:
                        break

                    all_runs.extend(runs)
                    page += 1

                    # GitHub API pagination limit
                    if len(runs) < per_page:
                        break

            except urllib.error.HTTPError as e:
                if e.code == 403:
                    print("âŒ GitHub API rate limit exceeded!")
                    print(
                        "ðŸ’¡ Set GITHUB_TOKEN or authenticate with GitHub CLI for higher limits"
                    )
                    sys.exit(1)
                else:
                    raise Exception(f"GitHub API error {e.code}: {e.reason}")

        return all_runs

    def get_workflow_run_jobs(
        self, owner: str, repo: str, run_id: int
    ) -> List[Dict[str, Any]]:
        """Get jobs for a specific workflow run using PyGithub."""
        return self._get_workflow_run_jobs_pygithub(owner, repo, run_id)

    def _get_workflow_run_jobs_pygithub(
        self, owner: str, repo: str, run_id: int
    ) -> List[Dict[str, Any]]:
        """Get workflow run jobs using PyGithub."""
        try:
            repo_obj = self.github.get_repo(f"{owner}/{repo}")
            run = repo_obj.get_workflow_run(run_id)
            jobs = run.jobs()

            job_list = []
            for job in jobs:
                job_list.append(
                    {
                        "id": job.id,
                        "name": job.name,
                        "labels": job.labels,
                        "runner_name": job.runner_name,
                        "started_at": (
                            job.started_at.isoformat() if job.started_at else None
                        ),
                        "completed_at": (
                            job.completed_at.isoformat() if job.completed_at else None
                        ),
                    }
                )
            return job_list

        except RateLimitExceededException:
            print("âŒ GitHub API rate limit exceeded!")
            print(
                "ðŸ’¡ Set GITHUB_TOKEN or authenticate with GitHub CLI for higher limits"
            )
            sys.exit(1)
        except Exception as e:
            raise Exception(f"Error fetching workflow run jobs: {e}")

    def _get_workflow_run_jobs_fallback(
        self, owner: str, repo: str, run_id: int
    ) -> List[Dict[str, Any]]:
        """Fallback implementation for workflow run jobs."""
        url = f"https://api.github.com/repos/{owner}/{repo}/actions/runs/{run_id}/jobs"
        headers = {
            "Accept": "application/vnd.github.v3+json",
            "User-Agent": "GitHub-Actions-Cost-Analyzer/1.0",
        }

        if self.token:
            headers["Authorization"] = f"token {self.token}"

        try:
            req = urllib.request.Request(url, headers=headers)
            with urllib.request.urlopen(req) as response:
                data = json.loads(response.read().decode())
                return data.get("jobs", [])
        except urllib.error.HTTPError as e:
            if e.code == 403:
                print("âŒ GitHub API rate limit exceeded!")
                print(
                    "ðŸ’¡ Set GITHUB_TOKEN or authenticate with GitHub CLI for higher limits"
                )
                sys.exit(1)
            else:
                raise Exception(f"GitHub API error {e.code}: {e.reason}")

    def get_workflow_runs_parallel_dataframe(
        self,
        owner: str,
        repo: str,
        workflows_df: pd.DataFrame,
        max_workers: int = 5,
    ) -> pd.DataFrame:
        """Get workflow runs for multiple workflows in parallel using tqdm.contrib.concurrent."""

        def fetch_runs_for_workflow(workflow_row):
            """Fetch runs for a single workflow."""
            try:
                df = self.get_workflow_runs_dataframe(
                    owner, repo, workflow_row["id"], per_page=100
                )
                df["workflow_name"] = workflow_row["name"]
                return df
            except RateLimitExceededException:
                logger.error("Rate limit exceeded! Aborting.")
                raise
            except Exception as e:
                logger.warning(
                    f"Error fetching runs for workflow {workflow_row['name']}: {e}"
                )
                return pd.DataFrame()

        logger.info(
            f"ðŸ”„ Fetching runs for {len(workflows_df)} workflows in parallel..."
        )

        # Use tqdm.contrib.concurrent for efficient parallel processing with progress bar
        workflow_rows = [row for _, row in workflows_df.iterrows()]

        all_dataframes = thread_map(
            fetch_runs_for_workflow,
            workflow_rows,
            desc="âš¡ Fetching workflow runs",
            unit="workflow",
            max_workers=max_workers,
            tqdm_class=tqdm,
        )

        # Filter out empty DataFrames and combine
        valid_dataframes = [df for df in all_dataframes if not df.empty]

        if valid_dataframes:
            combined_df = pd.concat(valid_dataframes, ignore_index=True)
            logger.info(f"ðŸ“Š Found {len(combined_df)} total workflow runs")
            return combined_df
        else:
            logger.warning("No workflow runs found")
            return pd.DataFrame()

    def get_workflow_runs_parallel(
        self,
        owner: str,
        repo: str,
        workflows: List[Dict[str, Any]],
        max_workers: int = 5,
    ) -> List[Dict[str, Any]]:
        """Get workflow runs for multiple workflows in parallel (legacy method)."""
        workflows_df = pd.DataFrame(workflows)
        runs_df = self.get_workflow_runs_parallel_dataframe(
            owner, repo, workflows_df, max_workers
        )
        return runs_df.to_dict("records")

    def get_workflow_jobs_parallel(
        self, owner: str, repo: str, runs: List[Dict[str, Any]], max_workers: int = 10
    ) -> List[Dict[str, Any]]:
        """Get jobs for multiple workflow runs in parallel using tqdm.contrib.concurrent."""

        def fetch_jobs_for_run(run):
            """Fetch jobs for a single run."""
            try:
                jobs = self.get_workflow_run_jobs(owner, repo, run["id"])
                return jobs
            except RateLimitExceededException:
                logger.error("Rate limit exceeded! Aborting.")
                raise
            except Exception as e:
                logger.warning(f"Error fetching jobs for run {run['id']}: {e}")
                return []

        logger.info(f"ðŸ”„ Fetching jobs for {len(runs)} runs in parallel...")

        # Use tqdm.contrib.concurrent for efficient parallel processing
        all_job_lists = thread_map(
            fetch_jobs_for_run,
            runs,
            desc="ðŸ”§ Fetching workflow jobs",
            unit="run",
            max_workers=max_workers,
            tqdm_class=tqdm,
        )

        # Flatten the list of job lists
        all_jobs = [job for job_list in all_job_lists for job in job_list]

        logger.info(f"âœ… Fetched {len(all_jobs)} total jobs")
        return all_jobs

    def create_runner_registration_token(self, owner: str, repo: str) -> Optional[str]:
        """
        Create a runner registration token using the GitHub API.

        Args:
            owner: Repository owner (username or organization)
            repo: Repository name

        Returns:
            Registration token if successful, None otherwise
        """
        try:
            # Handle repository redirection
            canonical_owner, canonical_repo = (
                self.redirection_detector.get_canonical_repo_name(owner, repo)
            )

            repository = self.github.get_repo(f"{canonical_owner}/{canonical_repo}")

            # Create a runner registration token
            # Note: This requires appropriate permissions (repo admin or organization admin)
            response = repository._requester.requestJsonAndCheck(
                "POST", f"{repository.url}/actions/runners/registration-token", None
            )

            # DEBUG: Log the actual response to see what we're getting
            logger.info(f"ðŸ” DEBUG: Token creation response: {response}")

            # Handle the case where PyGithub returns a tuple (headers, data)
            if isinstance(response, tuple) and len(response) == 2:
                headers, response_data = response
                logger.info(f"ðŸ” DEBUG: Got tuple response, using data part")
                response = response_data

            logger.info(
                f"ðŸ” DEBUG: Response keys: {list(response.keys()) if response else 'None'}"
            )

            if "token" in response:
                token = response["token"]
                expires_at = response.get("expires_at", "Unknown")
                logger.info(
                    f"âœ… Generated runner registration token for {canonical_owner}/{canonical_repo}"
                )
                logger.info(f"ðŸ• Token expires at: {expires_at}")
                return token
            else:
                logger.error("âŒ Token creation response missing 'token' field")
                logger.error(f"ðŸ” Full response: {response}")
                return None

        except Exception as e:
            logger.error(f"âŒ Failed to create runner registration token: {e}")
            logger.info(
                "ðŸ’¡ Ensure you have repository admin rights or organization admin rights"
            )
            logger.info(
                "ðŸ’¡ The token used for API access needs 'repo' or 'admin:org' scope"
            )
            return None


# Multi-format file handling utilities
def get_supported_file_formats() -> List[str]:
    """Get list of supported file formats for config files."""
    formats = ["json", "yaml", "yml"]
    if tomllib:
        formats.append("toml")
    return formats


def detect_file_format(filepath: Union[str, Path]) -> str:
    """Detect file format from extension."""
    path = Path(filepath)
    ext = path.suffix.lower()

    if ext in [".yaml", ".yml"]:
        return "yaml"
    elif ext == ".json":
        return "json"
    elif ext == ".toml" and tomllib:
        return "toml"
    else:
        # Default to YAML for unknown extensions
        return "yaml"


def load_config_file(filepath: Union[str, Path]) -> Dict[str, Any]:
    """Load configuration from file, auto-detecting format."""
    path = Path(filepath)
    if not path.exists():
        raise FileNotFoundError(f"Configuration file not found: {filepath}")

    file_format = detect_file_format(path)

    try:
        with open(path, "r", encoding="utf-8") as f:
            if file_format == "yaml":
                return yaml.safe_load(f) or {}
            elif file_format == "json":
                return json.load(f) or {}
            elif file_format == "toml" and tomllib:
                # TOML files need to be opened in binary mode for tomllib
                pass

        # Handle TOML separately as it needs binary mode
        if file_format == "toml" and tomllib:
            with open(path, "rb") as f:
                return tomllib.load(f) or {}

        raise ValueError(f"Unsupported file format: {file_format}")

    except Exception as e:
        raise ValueError(f"Error loading {file_format.upper()} file {filepath}: {e}")


def save_config_file(data: Dict[str, Any], filepath: Union[str, Path]) -> None:
    """Save configuration to file, auto-detecting format."""
    path = Path(filepath)
    file_format = detect_file_format(path)

    # Ensure parent directory exists
    path.parent.mkdir(parents=True, exist_ok=True)

    try:
        with open(path, "w", encoding="utf-8") as f:
            if file_format == "yaml":
                yaml.dump(data, f, default_flow_style=False, indent=2, sort_keys=False)
            elif file_format == "json":
                json.dump(data, f, indent=2, ensure_ascii=False)
            elif file_format == "toml":
                # For saving TOML, we'd need tomli-w or similar
                # For now, convert to YAML as fallback
                print(f"âš ï¸  TOML writing not supported, saving as YAML instead")
                yaml_path = path.with_suffix(".yaml")
                with open(yaml_path, "w", encoding="utf-8") as yaml_f:
                    yaml.dump(
                        data,
                        yaml_f,
                        default_flow_style=False,
                        indent=2,
                        sort_keys=False,
                    )
                print(f"ðŸ“ Saved to {yaml_path} instead of {path}")
                return
            else:
                raise ValueError(f"Unsupported file format: {file_format}")

        print(f"ðŸ“ Configuration saved to {path}")

    except Exception as e:
        raise ValueError(f"Error saving {file_format.upper()} file {filepath}: {e}")


class SelfHostedRunnerSpec(BaseRunnerSpecModel):
    """Specification for a self-hosted runner."""

    name: str = Field(min_length=1, max_length=100, description="Runner name")
    arch: Literal["x64", "arm64"] = "x64"
    cost_per_minute: float = Field(
        ge=0, default=0.0, description="Cost per minute if any"
    )
    description: str = Field(
        default="", max_length=500, description="Runner description"
    )
    token: Optional[str] = Field(
        default=None, description="GitHub runner registration token"
    )

    @field_validator("cost_per_minute")
    @classmethod
    def validate_cost(cls, v):
        if v > 1.0:  # $1/minute seems excessive
            raise ValueError("Cost per minute exceeds reasonable range")
        return v


class SelfHostedRunnerConfig(BaseModel):
    """Configuration for self-hosted runners."""

    runners: List[SelfHostedRunnerSpec] = Field(
        min_items=1, description="List of runner specifications"
    )

    model_config = {
        "env_prefix": "RUNNER_",
        "populate_by_name": True,
        "validate_assignment": True,
        "use_enum_values": True,  # Serialize enums by value, not type
    }

    @classmethod
    def from_yaml_file(cls, yaml_path: Path) -> "SelfHostedRunnerConfig":
        """Load and validate configuration from YAML file."""
        try:
            with open(yaml_path, "r") as f:
                data = yaml.safe_load(f)
            return cls.model_validate(data)  # Pydantic v2 syntax
        except Exception as e:
            raise ValueError(f"Invalid YAML configuration: {e}")

    @classmethod
    def from_config_files(cls, config_paths: List[str]) -> "SelfHostedRunnerConfig":
        """Load and merge configurations from multiple files (supports YAML, JSON, TOML)."""
        try:
            merged_config = merge_configs(config_paths)
            return cls.model_validate(merged_config)
        except Exception as e:
            raise ValueError(f"Invalid configuration files: {e}")

    @classmethod
    def from_args(
        cls,
        name: str,
        cores: int,
        ram_gb: float,
        storage_gb: float,
        arch: str = "x64",
        cost_per_minute: float = 0.0,
        description: str = "",
        token: Optional[str] = None,
    ) -> "SelfHostedRunnerConfig":
        """Create configuration from command line arguments."""
        runner = SelfHostedRunnerSpec(
            name=name,
            cores=cores,
            ram_gb=ram_gb,
            storage_gb=storage_gb,
            arch=arch,
            cost_per_minute=cost_per_minute,
            description=description,
            token=token,
        )
        return cls(runners=[runner])

    def to_yaml(self, yaml_path: Path) -> None:
        """Export configuration to YAML with validation."""
        with open(yaml_path, "w") as f:
            # Use model_dump with mode='python' to get serializable data
            data = self.model_dump(mode="python")
            yaml.dump(data, f, default_flow_style=False)


class WorkflowStatus(str, Enum):
    COMPLETED = "completed"
    IN_PROGRESS = "in_progress"
    QUEUED = "queued"
    CANCELLED = "cancelled"


class WorkflowConclusion(str, Enum):
    SUCCESS = "success"
    FAILURE = "failure"
    CANCELLED = "cancelled"
    SKIPPED = "skipped"
    NEUTRAL = "neutral"
    TIMED_OUT = "timed_out"


class WorkflowRunStats(BaseModel):
    """Statistics for a single workflow run."""

    run_id: PositiveInt = Field(description="Positive workflow run ID")
    workflow_name: str = Field(
        min_length=1, max_length=255, description="Workflow name"
    )
    workflow_id: PositiveInt = Field(description="Workflow ID")
    duration_seconds: int = Field(ge=0, description="Non-negative duration")
    billable_minutes: float = Field(ge=0, finite=True, description="Billable minutes")
    cost_usd: float = Field(ge=0, description="Cost in USD")
    status: WorkflowStatus = Field(description="Workflow status")
    conclusion: Optional[WorkflowConclusion] = Field(
        None, description="Workflow conclusion"
    )
    event: str = Field(min_length=1, description="Trigger event")
    created_at: datetime = Field(description="Creation timestamp")
    runner_type: str = Field(default="ubuntu-latest", description="Runner type")
    head_sha: str = Field(default="", description="Head SHA")
    head_branch: str = Field(default="", description="Head branch")

    @model_validator(mode="after")
    def validate_cost(self):
        if (
            hasattr(self, "billable_minutes")
            and self.cost_usd > self.billable_minutes * 1.0
        ):
            raise ValueError("Cost cannot exceed reasonable billing rate")
        return self

    @field_validator("created_at", mode="before")
    @classmethod
    def parse_datetime(cls, v):
        if isinstance(v, str):
            try:
                return datetime.fromisoformat(v.replace("Z", "+00:00"))
            except ValueError:
                return datetime.strptime(v, "%Y-%m-%dT%H:%M:%SZ").replace(
                    tzinfo=timezone.utc
                )
        return v


class ConfidenceLevel(str, Enum):
    HIGH = "HIGH"
    MEDIUM = "MEDIUM"
    LOW = "LOW"


class RunnerResourceUsage(BaseModel):
    """Resource usage analysis for a runner type."""

    runner_type: str = Field(
        pattern=r"^[a-zA-Z0-9\-]+$", description="Runner type identifier"
    )
    total_runs: PositiveInt = Field(description="Total number of runs")
    avg_duration_seconds: float = Field(ge=0, description="Average duration in seconds")
    estimated_cpu_usage_percent: float = Field(
        ge=0, le=100, description="Estimated CPU usage percentage"
    )
    estimated_memory_usage_percent: float = Field(
        ge=0, le=100, description="Estimated memory usage percentage"
    )
    utilization_efficiency: float = Field(
        ge=0, le=1, description="Utilization efficiency (0-1)"
    )
    recommended_runner: Optional[RunnerSpecification] = Field(
        None, description="Recommended runner specification"
    )
    potential_savings_usd: float = Field(ge=0, description="Potential savings in USD")
    confidence_level: ConfidenceLevel = Field(description="Confidence level")

    @model_validator(mode="before")
    @classmethod
    def validate_efficiency_consistency(cls, values):
        cpu = values.get("estimated_cpu_usage_percent", 0)
        memory = values.get("estimated_memory_usage_percent", 0)
        efficiency = values.get("utilization_efficiency", 0)

        if cpu < 50 and memory < 50 and efficiency > 0.8:
            raise ValueError("High efficiency inconsistent with low resource usage")
        return values


class MatrixJobAnalysis(BaseModel):
    """Analysis of matrix job patterns and optimization opportunities."""

    workflow_name: str = Field(min_length=1, description="Workflow name")
    matrix_dimensions: List[str] = Field(
        min_items=1, description="Matrix dimension names"
    )
    total_matrix_jobs: NonNegativeInt = Field(description="Total number of matrix jobs")
    failed_matrix_jobs: NonNegativeInt = Field(
        description="Number of failed matrix jobs"
    )
    avg_job_duration: float = Field(ge=0, description="Average job duration in seconds")
    early_failure_potential: bool = Field(
        description="Whether early failure detection is possible"
    )
    pre_matrix_candidates: List[str] = Field(
        default_factory=list, description="Candidates for pre-matrix execution"
    )
    matrix_necessity_score: float = Field(
        ge=0, le=1, description="Matrix necessity score (0-1)"
    )
    optimization_recommendations: List[str] = Field(
        default_factory=list, description="Optimization recommendations"
    )

    @model_validator(mode="after")
    def validate_failed_jobs(self):
        if self.failed_matrix_jobs > self.total_matrix_jobs:
            raise ValueError("Failed jobs cannot exceed total jobs")
        return self

    @property
    def failure_rate(self) -> float:
        """Calculate failure rate as percentage."""
        if self.total_matrix_jobs == 0:
            return 0.0
        return (self.failed_matrix_jobs / self.total_matrix_jobs) * 100


class ServerType(str, Enum):
    VPS = "vps"
    LOCAL = "local"
    CLOUD = "cloud"


class OSType(str, Enum):
    UBUNTU = "ubuntu"
    FEDORA = "fedora"
    CENTOS = "centos"
    DEBIAN = "debian"


class SelfHostedRunnerRecommendation(BaseModel):
    """Recommendation for self-hosted runner setup."""

    server_type: ServerType = Field(description="Server deployment type")
    os_type: OSType = Field(description="Operating system type")
    recommended_specs: Dict[str, Any] = Field(
        description="Recommended hardware specifications"
    )
    cost_comparison: Dict[str, float] = Field(description="Cost comparison data")
    setup_commands: List[str] = Field(
        default_factory=list, description="Setup commands"
    )
    security_considerations: List[str] = Field(
        default_factory=list, description="Security considerations"
    )
    estimated_monthly_savings: float = Field(
        ge=0, description="Estimated monthly savings in USD"
    )

    @field_validator("recommended_specs")
    @classmethod
    def validate_specs(cls, v):
        required_keys = ["cores", "ram_gb", "storage_gb"]
        for key in required_keys:
            if key not in v:
                raise ValueError(f"Missing required spec: {key}")
            if not isinstance(v[key], (int, float)) or v[key] <= 0:
                raise ValueError(f"Spec {key} must be a positive number")
        return v

    @field_validator("cost_comparison")
    @classmethod
    def validate_cost_comparison(cls, v):
        if "github_hosted" not in v or "self_hosted" not in v:
            raise ValueError(
                "Cost comparison must include github_hosted and self_hosted costs"
            )
        return v


class RiskLevel(str, Enum):
    MINIMAL = "MINIMAL"
    LOW = "LOW"
    MEDIUM = "MEDIUM"
    HIGH = "HIGH"
    CRITICAL = "CRITICAL"


class CostImpact(str, Enum):
    LOW = "LOW"
    MODERATE = "MODERATE"
    HIGH = "HIGH"
    SEVERE = "SEVERE"


class WorkflowSummary(BaseModel):
    """Summary statistics for a workflow."""

    workflow_name: str = Field(min_length=1, description="Workflow name")
    workflow_id: PositiveInt = Field(description="Workflow ID")
    total_runs: NonNegativeInt = Field(description="Total number of runs")
    sampled_runs: NonNegativeInt = Field(description="Number of sampled runs")
    total_duration_seconds: NonNegativeInt = Field(
        description="Total duration in seconds"
    )
    total_billable_minutes: float = Field(ge=0, description="Total billable minutes")
    total_cost_usd: float = Field(ge=0, description="Total cost in USD")
    avg_duration_seconds: float = Field(ge=0, description="Average duration in seconds")
    min_duration_seconds: NonNegativeInt = Field(
        description="Minimum duration in seconds"
    )
    max_duration_seconds: NonNegativeInt = Field(
        description="Maximum duration in seconds"
    )
    std_duration_seconds: float = Field(
        ge=0, description="Standard deviation of duration"
    )
    success_rate: float = Field(ge=0, le=100, description="Success rate percentage")
    failure_rate: float = Field(ge=0, le=100, description="Failure rate percentage")
    cancellation_rate: float = Field(
        ge=0, le=100, description="Cancellation rate percentage"
    )
    failure_count: NonNegativeInt = Field(description="Number of failures")
    success_count: NonNegativeInt = Field(description="Number of successes")
    cancelled_count: NonNegativeInt = Field(description="Number of cancellations")
    cost_per_run: float = Field(ge=0, description="Cost per run in USD")
    waste_from_failures: float = Field(ge=0, description="Waste from failures in USD")
    risk_level: RiskLevel = Field(description="Risk level assessment")
    cost_impact: CostImpact = Field(description="Cost impact level")
    monthly_projection: float = Field(ge=0, description="Monthly cost projection")
    annual_projection: float = Field(ge=0, description="Annual cost projection")

    @model_validator(mode="after")
    def validate_sampled_runs(self):
        if self.sampled_runs > self.total_runs:
            raise ValueError("Sampled runs cannot exceed total runs")
        return self

    @model_validator(mode="before")
    @classmethod
    def validate_run_counts(cls, values):
        total = values.get("total_runs", 0)
        success = values.get("success_count", 0)
        failure = values.get("failure_count", 0)
        cancelled = values.get("cancelled_count", 0)

        if success + failure + cancelled != total:
            raise ValueError(
                "Sum of success, failure, and cancelled counts must equal total runs"
            )
        return values

    @model_validator(mode="before")
    @classmethod
    def validate_rates(cls, values):
        success_rate = values.get("success_rate", 0)
        failure_rate = values.get("failure_rate", 0)
        cancellation_rate = values.get("cancellation_rate", 0)

        total_rate = success_rate + failure_rate + cancellation_rate
        if abs(total_rate - 100) > 0.01:  # Allow small floating point errors
            raise ValueError("Sum of all rates must equal 100%")
        return values


class RepositoryAnalysis(BaseModel):
    """Complete repository analysis results."""

    repository: str = Field(
        pattern=r"^[a-zA-Z0-9\-_./]+$", description="Repository name"
    )
    analysis_date: str = Field(description="Analysis date in ISO format")
    total_workflows: NonNegativeInt = Field(description="Total number of workflows")
    total_runs: NonNegativeInt = Field(description="Total number of runs")
    total_cost: float = Field(ge=0, description="Total cost in USD")
    total_waste: float = Field(ge=0, description="Total waste in USD")
    waste_percentage: float = Field(ge=0, le=100, description="Waste percentage")
    monthly_projection: float = Field(ge=0, description="Monthly cost projection")
    annual_projection: float = Field(ge=0, description="Annual cost projection")
    workflows: List[WorkflowSummary] = Field(
        default_factory=list, description="Workflow summaries"
    )
    high_priority_issues: List[Dict[str, Any]] = Field(
        default_factory=list, description="High priority issues"
    )
    recommendations: List[Dict[str, Any]] = Field(
        default_factory=list, description="Recommendations"
    )
    raw_data: List[WorkflowRunStats] = Field(
        default_factory=list, description="Raw workflow data"
    )
    runner_efficiency: List[RunnerResourceUsage] = Field(
        default_factory=list, description="Runner efficiency analysis"
    )
    matrix_optimization: List[MatrixJobAnalysis] = Field(
        default_factory=list, description="Matrix optimization analysis"
    )
    self_hosted_recommendations: List[SelfHostedRunnerRecommendation] = Field(
        default_factory=list, description="Self-hosted runner recommendations"
    )

    @field_validator("analysis_date")
    @classmethod
    def validate_analysis_date(cls, v):
        try:
            datetime.fromisoformat(v.replace("Z", "+00:00"))
        except ValueError:
            raise ValueError("Analysis date must be in ISO format")
        return v

    @model_validator(mode="after")
    def validate_waste_percentage(self):
        if self.total_cost > 0:
            expected_percentage = (self.total_waste / self.total_cost) * 100
            if abs(self.waste_percentage - expected_percentage) > 0.01:
                raise ValueError(
                    "Waste percentage inconsistent with total cost and waste"
                )
        return self

    def model_dump_with_iso_dates(self, **kwargs):
        """Override model_dump method to provide ISO serialization."""
        return self.model_dump(**kwargs)

    model_config = {
        "json_encoders": {datetime: lambda v: v.isoformat()},
        "use_enum_values": True,
    }


class GitHubRunnerInfo(BaseRunnerSpecModel):
    """Information about a GitHub-hosted runner."""

    name: str = Field(min_length=1, description="Runner name")
    os: str = Field(
        pattern=r"^(ubuntu|windows|macos).*", description="Operating system"
    )
    arch: str = Field(pattern=r"^(x64|arm64)$", description="CPU architecture")
    cost_per_minute: PositiveFloat = Field(description="Cost per minute in USD")
    labels: List[str] = Field(default_factory=list, description="Runner labels")
    available: bool = Field(True, description="Whether runner is available")

    @field_validator("labels")
    @classmethod
    def validate_labels(cls, v):
        # Ensure all labels are lowercase and valid
        return [label.lower().strip() for label in v if label.strip()]

    # Remove hourly_cost property (inherited from BaseRunnerSpecModel)

    @property
    def daily_cost(self) -> float:
        """Calculate daily cost at full utilization."""
        return self.hourly_cost * 24


class ActualRunnerInfo(BaseModel):
    """Information about actual runners available in a repository."""

    github_hosted: List[GitHubRunnerInfo] = Field(
        default_factory=list, description="GitHub-hosted runners"
    )
    self_hosted: List[SelfHostedRunnerSpec] = Field(
        default_factory=list, description="Self-hosted runner specs"
    )
    organization_runners: List[Dict[str, Any]] = Field(
        default_factory=list, description="Organization runners"
    )
    repository_runners: List[Dict[str, Any]] = Field(
        default_factory=list, description="Repository runners"
    )

    @property
    def total_runner_count(self) -> int:
        """Calculate total number of available runners."""
        return (
            len(self.github_hosted)
            + len(self.self_hosted)
            + len(self.organization_runners)
            + len(self.repository_runners)
        )

    @property
    def github_hosted_count(self) -> int:
        """Count of GitHub-hosted runners."""
        return len(self.github_hosted)

    def get_runner_by_name(self, name: str) -> Optional[GitHubRunnerInfo]:
        """Find a GitHub-hosted runner by name."""
        for runner in self.github_hosted:
            if runner.name == name:
                return runner
        return None


class PerformanceImprovement(str, Enum):
    SIGNIFICANT = "significant"
    MODERATE = "moderate"
    MINIMAL = "minimal"
    NONE = "none"


class RunnerRecommendation(BaseModel):
    """Recommendation for optimal runner selection."""

    job_name: str = Field(min_length=1, description="Job name")
    current_runner: RunnerSpecification = Field(
        description="Current runner specification"
    )
    recommended_runner: RunnerSpecification = Field(
        description="Recommended runner specification"
    )
    reason: str = Field(min_length=10, description="Reason for recommendation")
    cost_savings: float = Field(decimal_places=2, description="Cost savings in USD")
    performance_improvement: PerformanceImprovement = Field(
        description="Performance improvement level"
    )
    patch_content: str = Field(description="YAML patch content for implementation")

    @field_validator("patch_content")
    @classmethod
    def validate_patch_content(cls, v: str) -> str:
        """Validate patch content is not empty."""
        if not v.strip():
            raise ValueError("Patch content cannot be empty")
        return v.strip()

    @property
    def savings_percentage(self) -> float:
        """Calculate savings as percentage based on current vs recommended cost."""
        if self.current_runner.cost_per_minute <= 0:
            return 0.0
        return (self.cost_savings / self.current_runner.cost_per_minute) * 100

    @property
    def estimated_monthly_savings(self) -> float:
        """Estimate monthly savings assuming 8 hours usage per weekday."""
        # Assume 8 hours * 5 days * 4.33 weeks = ~173 hours per month
        monthly_hours = 173
        return self.cost_savings * 60 * monthly_hours  # Convert to hourly and multiply


class MachineSpec(BaseRunnerSpecModel):
    """Current machine specifications with enhanced type safety."""

    # Inherit cores, ram_gb, storage_gb from BaseRunnerSpecModel
    hostname: str = Field(description="Machine hostname")
    arch: CPUArchitecture = Field(description="CPU architecture")
    os_name: str = Field(min_length=1, description="Operating system name")
    os_version: str = Field(min_length=1, description="Operating system version")
    os_family: OSFamily = Field(description="OS family")
    platform: Platform = Field(description="Platform identifier")
    cpu_model: str = Field(min_length=1, description="CPU model information")
    package_manager: PackageManager = Field(
        default=PackageManager.APT, description="Package manager"
    )
    shell: Shell = Field(default=Shell.BASH, description="Default shell")

    # Firewall detection fields
    primary_firewall: Optional[FirewallType] = Field(
        default=None, description="Primary firewall software detected"
    )
    available_firewalls: List[FirewallType] = Field(
        default_factory=list, description="All firewall software detected on system"
    )
    active_firewalls: List[FirewallType] = Field(
        default_factory=list, description="Currently active/running firewall services"
    )
    firewall_conflicts: bool = Field(
        default=False, description="Whether multiple conflicting firewalls are detected"
    )
    firewall_warnings: List[str] = Field(
        default_factory=list, description="Firewall configuration warnings"
    )

    @field_validator("os_name")
    @classmethod
    def validate_os_name(cls, v: str) -> str:
        """Validate and normalize OS name."""
        return v.lower().strip()

    # Remove duplicated total_ram_mb property (inherited from BaseRunnerSpecModel)
    # Remove duplicated is_arm_based property (inherited from BaseRunnerSpecModel)

    @property
    def is_linux_based(self) -> bool:
        """Check if platform is Linux-based."""
        return self.platform == Platform.LINUX

    @property
    def supports_docker(self) -> bool:
        """Check if platform supports Docker."""
        return self.platform in (Platform.LINUX, Platform.WINDOWS)

    @property
    def default_runner_equivalent(self) -> Optional[RunnerType]:
        """Get the closest GitHub-hosted runner equivalent."""
        # Find the closest match based on cores and OS family
        candidates = [
            spec
            for spec in UNIFIED_RUNNER_SPECS.values()
            if spec.os_family == self.os_family and spec.arch == self.arch
        ]

        if not candidates:
            # Fallback to same OS family, different arch
            candidates = [
                spec
                for spec in UNIFIED_RUNNER_SPECS.values()
                if spec.os_family == self.os_family
            ]

        if not candidates:
            return None

        # Find closest by core count
        return min(candidates, key=lambda s: abs(s.cores - self.cores)).name

    @classmethod
    def detect_current_machine(cls) -> "MachineSpec":
        """Auto-detect current machine specifications."""
        # Detect architecture
        machine = py_platform.machine().lower()
        if machine in ("x86_64", "amd64"):
            arch = CPUArchitecture.X64
        elif machine in ("aarch64", "arm64"):
            arch = CPUArchitecture.ARM64
        elif machine.startswith("arm"):
            arch = CPUArchitecture.ARM
        else:
            arch = CPUArchitecture.X64  # Default fallback

        # Detect platform and OS family
        system = py_platform.system().lower()
        if system == "linux":
            platform_val = Platform.LINUX
            # Try to detect specific Linux distribution
            try:
                dist_name = distro.id().lower()
                if "ubuntu" in dist_name or "debian" in dist_name:
                    os_family = OSFamily.UBUNTU
                elif "fedora" in dist_name:
                    os_family = OSFamily.FEDORA
                elif "centos" in dist_name or "rhel" in dist_name:
                    os_family = OSFamily.CENTOS
                else:
                    os_family = OSFamily.UBUNTU  # Default for Linux
            except ImportError:
                # Fallback without distro package
                os_family = OSFamily.UBUNTU
        elif system == "darwin":
            platform_val = Platform.DARWIN
            os_family = OSFamily.MACOS
        elif system == "windows":
            platform_val = Platform.WINDOWS
            os_family = OSFamily.WINDOWS
        else:
            platform_val = Platform.LINUX
            os_family = OSFamily.UBUNTU

        # Detect package manager
        package_manager = PackageManager.APT  # Default
        if os_family == OSFamily.FEDORA:
            package_manager = PackageManager.DNF
        elif os_family == OSFamily.CENTOS:
            package_manager = PackageManager.YUM
        elif os_family == OSFamily.MACOS:
            package_manager = PackageManager.BREW
        elif os_family == OSFamily.WINDOWS:
            package_manager = PackageManager.CHOCO

        # Detect shell
        shell = Shell.BASH  # Default
        shell_env = os.environ.get("SHELL", "").split("/")[-1]
        if shell_env in Shell:
            shell = Shell(shell_env)

        # Get system specs
        cores = psutil.cpu_count(logical=True) or 2
        ram_gb = psutil.virtual_memory().total / (1024**3)

        # Estimate storage (root filesystem)
        try:
            storage_gb = psutil.disk_usage("/").total / (1024**3)
        except (OSError, PermissionError, FileNotFoundError):
            storage_gb = 100.0  # Default fallback

        # Get CPU model
        try:
            if platform_val == Platform.LINUX:
                with open("/proc/cpuinfo", "r") as f:
                    for line in f:
                        if line.startswith("model name"):
                            cpu_model = line.split(":", 1)[1].strip()
                            break
                    else:
                        cpu_model = "Unknown CPU"
            else:
                cpu_model = py_platform.processor() or "Unknown CPU"
        except (OSError, IOError, FileNotFoundError, PermissionError):
            cpu_model = "Unknown CPU"

        return cls(
            cores=cores,
            ram_gb=ram_gb,
            storage_gb=storage_gb,
            arch=arch,
            os_name=py_platform.release(),
            os_version=py_platform.version(),
            os_family=os_family,
            platform=platform_val,
            cpu_model=cpu_model,
            package_manager=package_manager,
            shell=shell,
        )

    def __str__(self) -> str:
        """String representation of machine specs."""

        # Helper function to safely get string value from enum or string
        def get_string_value(field):
            if hasattr(field, "value"):
                return field.value
            return str(field)

        return f"{self.cores}C/{self.ram_gb:.1f}GB/{get_string_value(self.arch)}/{get_string_value(self.os_family)}"


class MachineConfigGenerator:
    """Generates machine-specific configurations and setup scripts."""

    SUPPORTED_ARCHITECTURES = {
        "x64": ["ubuntu-latest", "windows-latest", "macos-latest"],
        "arm64": ["ubuntu-latest", "windows-latest", "macos-latest-large"],
    }

    SETUP_TEMPLATES = {
        "ubuntu": {
            "pre_setup": [
                # Security hardening before runner installation
                "sudo apt update && sudo apt upgrade -y",
                "sudo apt install -y fail2ban ufw apparmor-utils",
                "sudo systemctl enable fail2ban",
                "sudo systemctl start fail2ban",
            ],
            "user_setup": [
                # Create dedicated user for runner with minimal privileges
                "sudo groupadd github-runners",
                "sudo useradd -m -g github-runners -s /bin/bash github-runner",
                "sudo usermod -aG docker github-runner",  # If Docker is needed
                "sudo mkdir -p /opt/actions-runner",
                "sudo chown github-runner:github-runners /opt/actions-runner",
                "sudo chmod 755 /opt/actions-runner",
            ],
            "container_setup": [
                # Set up container isolation (if Docker/Podman available)
                "sudo apt install -y docker.io containerd",
                "sudo systemctl enable docker",
                "sudo systemctl start docker",
                "sudo usermod -aG docker github-runner",
                # Create AppArmor profile for runner
                "sudo aa-complain /usr/bin/docker",  # Start in complain mode
            ],
            "install_commands": [
                "cd /opt/actions-runner",
                "sudo -u github-runner curl -o actions-runner-linux-{arch}-{version}.tar.gz -L https://github.com/actions/runner/releases/download/v{version}/actions-runner-linux-{arch}-{version}.tar.gz",
                "sudo -u github-runner tar xzf actions-runner-linux-{arch}-{version}.tar.gz",
                "sudo -u github-runner ./config.sh --url https://github.com/{owner}/{repo} --token {token} --unattended --replace",
                "sudo ./svc.sh install github-runner",
                "sudo ./svc.sh start",
            ],
            "security_setup": [
                # Network security
                "sudo ufw --force reset",
                "sudo ufw default deny incoming",
                "sudo ufw default deny outgoing",
                "sudo ufw allow out 53",  # DNS
                "sudo ufw allow out 80",  # HTTP
                "sudo ufw allow out 443",  # HTTPS
                "sudo ufw allow out 22",  # SSH (for git operations)
                "sudo ufw allow out 9418",  # Git protocol
                "sudo ufw allow in on lo",  # Loopback
                "sudo ufw allow ssh",  # SSH access (adjust as needed)
                "sudo ufw --force enable",
                # File system security
                "sudo chown -R github-runner:github-runners /opt/actions-runner",
                "sudo chmod -R 750 /opt/actions-runner",
                "sudo chmod 644 /opt/actions-runner/*.json",
                # Disable unnecessary services
                "sudo systemctl disable avahi-daemon 2>/dev/null || true",
                "sudo systemctl disable cups 2>/dev/null || true",
                # Set up log monitoring
                "sudo mkdir -p /var/log/github-runner",
                "sudo chown github-runner:github-runners /var/log/github-runner",
                "sudo chmod 750 /var/log/github-runner",
            ],
            "monitoring_setup": [
                # Set up monitoring and alerting
                "sudo apt install -y auditd",
                "sudo systemctl enable auditd",
                "sudo systemctl start auditd",
                # Monitor runner directory
                'echo "-w /opt/actions-runner/ -p wa -k github-runner-access" | sudo tee -a /etc/audit/rules.d/github-runner.rules',
                "sudo service auditd restart",
            ],
        },
        "fedora": {
            "pre_setup": [
                "sudo dnf update -y",
                "sudo dnf install -y fail2ban firewalld",
                "sudo systemctl enable fail2ban firewalld",
                "sudo systemctl start fail2ban firewalld",
            ],
            "user_setup": [
                "sudo groupadd github-runners",
                "sudo useradd -m -g github-runners -s /bin/bash github-runner",
                "sudo usermod -aG docker github-runner",
                "sudo mkdir -p /opt/actions-runner",
                "sudo chown github-runner:github-runners /opt/actions-runner",
                "sudo chmod 755 /opt/actions-runner",
            ],
            "container_setup": [
                "sudo dnf install -y podman buildah",
                "sudo systemctl enable podman",
                "sudo usermod -aG wheel github-runner",
                # SELinux configuration for containers
                "sudo setsebool -P container_manage_cgroup on",
            ],
            "install_commands": [
                "cd /opt/actions-runner",
                "sudo -u github-runner curl -o actions-runner-linux-{arch}-{version}.tar.gz -L https://github.com/actions/runner/releases/download/v{version}/actions-runner-linux-{arch}-{version}.tar.gz",
                "sudo -u github-runner tar xzf actions-runner-linux-{arch}-{version}.tar.gz",
                "sudo -u github-runner ./config.sh --url https://github.com/{owner}/{repo} --token {token} --unattended --replace",
                "sudo ./svc.sh install github-runner",
                "sudo ./svc.sh start",
            ],
            "security_setup": [
                # Firewall configuration
                "sudo firewall-cmd --set-default-zone=drop",
                "sudo firewall-cmd --permanent --add-service=ssh",
                "sudo firewall-cmd --permanent --add-service=http",
                "sudo firewall-cmd --permanent --add-service=https",
                "sudo firewall-cmd --permanent --add-service=dns",
                "sudo firewall-cmd --reload",
                # SELinux configuration
                "sudo setsebool -P httpd_can_network_connect on",
                'sudo semanage fcontext -a -t bin_t "/opt/actions-runner/bin/.*" 2>/dev/null || true',
                "sudo restorecon -R /opt/actions-runner",
                # File permissions
                "sudo chown -R github-runner:github-runners /opt/actions-runner",
                "sudo chmod -R 750 /opt/actions-runner",
            ],
            "monitoring_setup": [
                "sudo dnf install -y audit",
                "sudo systemctl enable auditd",
                "sudo systemctl start auditd",
                'echo "-w /opt/actions-runner/ -p wa -k github-runner-access" | sudo tee -a /etc/audit/rules.d/github-runner.rules',
                "sudo service auditd restart",
            ],
        },
        "windows": {
            "pre_setup": [
                "Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser",
                "Install-WindowsFeature -Name Windows-Defender-Features",
                "Set-MpPreference -DisableRealtimeMonitoring $false",
            ],
            "user_setup": [
                'New-LocalGroup -Name "GitHubRunners" -Description "GitHub Actions Runners"',
                'New-LocalUser -Name "github-runner" -Description "GitHub Actions Runner Service Account" -PasswordNeverExpires',
                'Add-LocalGroupMember -Group "GitHubRunners" -Member "github-runner"',
                'New-Item -Path "C:\\actions-runner" -ItemType Directory -Force',
                'icacls "C:\\actions-runner" /grant "github-runner:(OI)(CI)F"',
            ],
            "container_setup": [
                "Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Hyper-V -All",
                "Enable-WindowsOptionalFeature -Online -FeatureName Containers -All",
                "Install-Module -Name DockerMsftProvider -Repository PSGallery -Force",
                "Install-Package -Name docker -ProviderName DockerMsftProvider -Force",
            ],
            "install_commands": [
                'Set-Location "C:\\actions-runner"',
                "Invoke-WebRequest -Uri https://github.com/actions/runner/releases/download/v{version}/actions-runner-win-{arch}-{version}.zip -OutFile actions-runner-win-{arch}-{version}.zip",
                "Expand-Archive -Path actions-runner-win-{arch}-{version}.zip -DestinationPath . -Force",
                '.\\config.cmd --url https://github.com/{owner}/{repo} --token {token} --unattended --replace --runasservice --windowslogonaccount "github-runner"',
            ],
            "security_setup": [
                "Set-NetFirewallProfile -Profile Domain,Public,Private -Enabled True",
                "Set-NetFirewallProfile -Profile Domain,Public,Private -DefaultInboundAction Block",
                "Set-NetFirewallProfile -Profile Domain,Public,Private -DefaultOutboundAction Block",
                'New-NetFirewallRule -DisplayName "Allow DNS" -Direction Outbound -Protocol UDP -RemotePort 53 -Action Allow',
                'New-NetFirewallRule -DisplayName "Allow HTTP" -Direction Outbound -Protocol TCP -RemotePort 80 -Action Allow',
                'New-NetFirewallRule -DisplayName "Allow HTTPS" -Direction Outbound -Protocol TCP -RemotePort 443 -Action Allow',
                'New-NetFirewallRule -DisplayName "Allow SSH" -Direction Outbound -Protocol TCP -RemotePort 22 -Action Allow',
                'Set-ItemProperty -Path "HKLM:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Policies\\System" -Name "EnableLUA" -Value 1',
                'icacls "C:\\actions-runner" /inheritance:r /grant:r "github-runner:(OI)(CI)RX" /grant:r "Administrators:(OI)(CI)F"',
            ],
            "monitoring_setup": [
                'auditpol /set /category:"Object Access" /success:enable /failure:enable',
                'auditpol /set /category:"Privilege Use" /success:enable /failure:enable',
                'New-EventLog -LogName "GitHubRunner" -Source "GitHubRunnerService"',
            ],
        },
    }

    @staticmethod
    def get_current_machine_spec() -> MachineSpec:
        """Get specifications of the current machine."""
        # Get CPU information
        cores = psutil.cpu_count(logical=False)

        # Get memory information
        memory = psutil.virtual_memory()
        ram_gb = memory.total / (1024**3)

        # Get disk information
        disk = psutil.disk_usage("/")
        storage_gb = disk.free / (1024**3)

        # Get architecture
        arch = platform.machine()
        if arch in ["x86_64", "AMD64"]:
            arch = "x64"
        elif arch in ["arm64", "aarch64"]:
            arch = "arm64"

        # Get OS information
        platform_system = platform.system().lower()
        os_version = platform.release()

        # Determine OS family and package manager
        os_family = ""
        package_manager = ""
        shell = "bash"

        if platform_system == "linux":
            os_name = "linux"
            # Detect Linux distribution
            try:
                if os.path.exists("/etc/os-release"):
                    with open("/etc/os-release", "r") as f:
                        os_release = f.read().lower()
                        if "ubuntu" in os_release or "debian" in os_release:
                            os_family = "ubuntu"
                            package_manager = "apt"
                        elif "fedora" in os_release:
                            os_family = "fedora"
                            package_manager = "dnf"
                        elif (
                            "centos" in os_release
                            or "rhel" in os_release
                            or "rocky" in os_release
                        ):
                            os_family = "centos"
                            package_manager = "yum"
                        elif "arch" in os_release:
                            os_family = "arch"
                            package_manager = "pacman"
                        else:
                            os_family = "linux"
                            package_manager = "unknown"
            except Exception:
                os_family = "linux"
                package_manager = "unknown"
        elif platform_system == "darwin":
            os_name = "macos"
            os_family = "macos"
            package_manager = "brew"
            shell = "zsh"
        elif platform_system == "windows":
            os_name = "windows"
            os_family = "windows"
            package_manager = "choco"
            shell = "pwsh"
        else:
            os_name = platform_system
            os_family = platform_system
            package_manager = "unknown"

        # Get CPU model (best effort)
        cpu_model = "Unknown"
        try:
            if platform_system == "linux":
                with open("/proc/cpuinfo", "r") as f:
                    for line in f:
                        if "model name" in line:
                            cpu_model = line.split(":")[1].strip()
                            break
            elif platform_system == "darwin":
                result = subprocess.run(
                    ["sysctl", "-n", "machdep.cpu.brand_string"],
                    capture_output=True,
                    text=True,
                )
                if result.returncode == 0:
                    cpu_model = result.stdout.strip()
            elif platform_system == "windows":
                result = subprocess.run(
                    ["wmic", "cpu", "get", "name"], capture_output=True, text=True
                )
                if result.returncode == 0:
                    lines = result.stdout.strip().split("\n")
                    if len(lines) > 1:
                        cpu_model = lines[1].strip()
        except Exception:
            pass

        # Get hostname
        hostname = "unknown-host"
        try:
            import socket

            hostname = socket.gethostname()
        except Exception:
            try:
                hostname = platform.node()
            except Exception:
                pass

        # Detect firewall configuration
        firewall_info = MachineConfigGenerator._detect_firewall_configuration(
            platform_system
        )

        return MachineSpec(
            hostname=hostname,
            cores=cores,
            ram_gb=ram_gb,
            storage_gb=storage_gb,
            arch=arch,
            os_name=os_name,
            os_version=os_version,
            os_family=os_family,
            platform=platform_system,
            cpu_model=cpu_model,
            package_manager=package_manager,
            shell=shell,
            primary_firewall=firewall_info["primary_firewall"],
            available_firewalls=firewall_info["available_firewalls"],
            active_firewalls=firewall_info["active_firewalls"],
            firewall_conflicts=firewall_info["firewall_conflicts"],
            firewall_warnings=firewall_info["firewall_warnings"],
        )

    @staticmethod
    def _detect_firewall_configuration(platform_system: str) -> dict:
        """
        Comprehensive firewall detection with conflict analysis.

        Returns:
            dict: Firewall configuration information
        """
        available_firewalls = []
        active_firewalls = []
        warnings = []
        primary_firewall = None

        def run_command(cmd: list, capture_output: bool = True) -> tuple[bool, str]:
            """Safely run a command and return success status and output."""
            try:
                result = subprocess.run(
                    cmd,
                    capture_output=capture_output,
                    text=True,
                    timeout=10,
                )
                return result.returncode == 0, result.stdout.strip()
            except Exception as e:
                return False, str(e)

        if platform_system == "linux":
            # Check for UFW
            if shutil.which("ufw"):
                available_firewalls.append(FirewallType.UFW)
                success, output = run_command(["ufw", "status"])
                if success and "Status: active" in output:
                    active_firewalls.append(FirewallType.UFW)
                    if not primary_firewall:
                        primary_firewall = FirewallType.UFW

            # Check for firewalld
            if shutil.which("firewall-cmd"):
                available_firewalls.append(FirewallType.FIREWALLD)
                success, output = run_command(["systemctl", "is-active", "firewalld"])
                if success and output.strip() == "active":
                    active_firewalls.append(FirewallType.FIREWALLD)
                    if not primary_firewall:
                        primary_firewall = FirewallType.FIREWALLD

            # Check for nftables
            if shutil.which("nft"):
                available_firewalls.append(FirewallType.NFTABLES)
                success, output = run_command(["nft", "list", "tables"])
                if success and output.strip():
                    active_firewalls.append(FirewallType.NFTABLES)
                    if not primary_firewall:
                        primary_firewall = FirewallType.NFTABLES

            # Check for iptables (always present on Linux but may not be actively used)
            if shutil.which("iptables"):
                available_firewalls.append(FirewallType.IPTABLES)
                success, output = run_command(["iptables", "-L", "-n"])
                if success and (
                    "ACCEPT" in output or "DROP" in output or "REJECT" in output
                ):
                    # Check if there are non-default rules
                    lines = output.split("\n")
                    non_default_rules = [
                        l
                        for l in lines
                        if l.strip()
                        and not any(
                            default in l
                            for default in [
                                "Chain INPUT",
                                "Chain FORWARD",
                                "Chain OUTPUT",
                                "target",
                                "ACCEPT     all",
                            ]
                        )
                    ]
                    # More than just chain headers
                    if non_default_rules and len(non_default_rules) > 3:
                        active_firewalls.append(FirewallType.IPTABLES)
                        if not primary_firewall:
                            primary_firewall = FirewallType.IPTABLES

        elif platform_system == "windows":
            # Windows Firewall is always available
            available_firewalls.append(FirewallType.WINDOWS_FIREWALL)
            success, output = run_command(
                ["netsh", "advfirewall", "show", "allprofiles", "state"]
            )
            if success and "ON" in output:
                active_firewalls.append(FirewallType.WINDOWS_FIREWALL)
                primary_firewall = FirewallType.WINDOWS_FIREWALL

        elif platform_system == "darwin":
            # Check for pfctl (macOS built-in)
            if shutil.which("pfctl"):
                available_firewalls.append(FirewallType.PF)
                success, output = run_command(["pfctl", "-s", "info"])
                if success and "Status: Enabled" in output:
                    active_firewalls.append(FirewallType.PF)
                    primary_firewall = FirewallType.PF

            # Check Application Firewall
            success, output = run_command(
                [
                    "defaults",
                    "read",
                    "/Library/Preferences/com.apple.alf",
                    "globalstate",
                ]
            )
            if success and output.strip() != "0":
                if FirewallType.PF not in available_firewalls:
                    available_firewalls.append(FirewallType.PF)
                if FirewallType.PF not in active_firewalls:
                    active_firewalls.append(FirewallType.PF)
                if not primary_firewall:
                    primary_firewall = FirewallType.PF

        # Analyze conflicts and generate warnings
        firewall_conflicts = False

        if len(active_firewalls) > 1:
            # Check for known problematic combinations
            problematic_combinations = [
                (FirewallType.UFW, FirewallType.FIREWALLD),
                (FirewallType.UFW, FirewallType.IPTABLES),
                (FirewallType.FIREWALLD, FirewallType.IPTABLES),
            ]

            for fw1, fw2 in problematic_combinations:
                if fw1 in active_firewalls and fw2 in active_firewalls:
                    firewall_conflicts = True
                    warnings.append(
                        f"Potential conflict detected: {fw1.value} and {fw2.value} are both active"
                    )

        # Check for layered firewalls (acceptable scenarios)
        layered_acceptable = [
            # UFW uses nftables backend
            (FirewallType.UFW, FirewallType.NFTABLES),
            # firewalld can use nftables
            (FirewallType.FIREWALLD, FirewallType.NFTABLES),
        ]

        for fw1, fw2 in layered_acceptable:
            if fw1 in active_firewalls and fw2 in active_firewalls:
                warnings.append(
                    f"Layered firewall detected: {fw1.value} using {fw2.value} backend - this is normal"
                )

        # NetworkManager interference check (Linux only)
        if platform_system == "linux" and shutil.which("nmcli"):
            success, output = run_command(["systemctl", "is-active", "NetworkManager"])
            if success and output.strip() == "active":
                warnings.append(
                    "NetworkManager is active - may interfere with manual firewall rules"
                )

        # If no firewall detected, add warning
        if not active_firewalls:
            warnings.append("No active firewall detected - security risk")

        return {
            "primary_firewall": primary_firewall,
            "available_firewalls": available_firewalls,
            "active_firewalls": active_firewalls,
            "firewall_conflicts": firewall_conflicts,
            "firewall_warnings": warnings,
        }

    @staticmethod
    def detect_existing_runner_token() -> Optional[str]:
        """
        Detect GitHub runner token from an existing installation.

        Returns:
            The runner token if found, None otherwise.
        """
        # Common GitHub runner installation paths
        common_paths = [
            "/opt/actions-runner",
            "/home/actions-runner",
            "/home/runner/actions-runner",
            os.path.expanduser("~/actions-runner"),
            "C:\\actions-runner",
            "/usr/local/actions-runner",
        ]

        # Add current user's common directories
        if "HOME" in os.environ:
            home = os.environ["HOME"]
            common_paths.extend(
                [
                    f"{home}/actions-runner",
                    f"{home}/.actions-runner",
                    f"{home}/work/actions-runner",
                ]
            )

        for runner_path in common_paths:
            if not os.path.exists(runner_path):
                continue

            # Look for common GitHub runner files
            config_files = [
                os.path.join(runner_path, ".runner"),
                os.path.join(runner_path, ".credentials"),
                os.path.join(runner_path, ".credentials_rsaparams"),
                os.path.join(runner_path, "config.json"),
            ]

            for config_file in config_files:
                try:
                    if os.path.exists(config_file):
                        logger.info(f"ðŸ” Found runner configuration at: {config_file}")

                        # Try to extract token from .runner file (JSON format)
                        if config_file.endswith(".runner"):
                            with open(config_file, "r") as f:
                                data = json.load(f)
                                if "runnerToken" in data:
                                    return data["runnerToken"]
                                elif "token" in data:
                                    return data["token"]

                        # Try to extract from .credentials file
                        elif config_file.endswith(".credentials"):
                            with open(config_file, "r") as f:
                                data = json.load(f)
                                if "token" in data:
                                    return data["token"]
                                elif "runnerToken" in data:
                                    return data["runnerToken"]

                except (json.JSONDecodeError, PermissionError, KeyError) as e:
                    logger.debug(f"Could not read {config_file}: {e}")
                    continue
                except Exception as e:
                    logger.debug(f"Error processing {config_file}: {e}")
                    continue

        # Check if runner service is running and try to get info from process
        try:
            for proc in psutil.process_iter(["pid", "name", "cmdline"]):
                try:
                    if proc.info["name"] and "runner" in proc.info["name"].lower():
                        cmdline = proc.info["cmdline"]
                        if cmdline and "--token" in " ".join(cmdline):
                            logger.info(f"ðŸ” Found runner process: {proc.info['name']}")
                            # Extract token from command line (though this is usually not visible for security)
                            for i, arg in enumerate(cmdline):
                                if arg == "--token" and i + 1 < len(cmdline):
                                    return cmdline[i + 1]
                except (
                    psutil.NoSuchProcess,
                    psutil.AccessDenied,
                    psutil.ZombieProcess,
                ):
                    continue
        except ImportError:
            logger.debug("psutil not available for process scanning")
        except Exception as e:
            logger.debug(f"Error scanning processes: {e}")

        logger.info(
            "ðŸ’¡ No existing runner token found. Token will be auto-generated when configuring runners."
        )
        return None

    @staticmethod
    def generate_machine_config_yaml(
        machine_spec: MachineSpec,
        output_path: str,
        cost_per_minute: float = 0.0,
        token: Optional[str] = None,
    ) -> None:
        """Generate a YAML configuration file for the current machine."""

        # Helper function to safely get string value from enum or string
        def get_string_value(field):
            if hasattr(field, "value"):
                return field.value
            return str(field)

        # Create a SelfHostedRunnerSpec using Pydantic model
        runner_spec = SelfHostedRunnerSpec(
            name=f"{get_string_value(machine_spec.os_family)}-{get_string_value(machine_spec.arch)}-local",
            cores=machine_spec.cores,
            ram_gb=round(machine_spec.ram_gb, 1),
            storage_gb=round(machine_spec.storage_gb, 1),
            # Convert enum to string safely
            arch=get_string_value(machine_spec.arch),
            cost_per_minute=cost_per_minute,
            description=f"{machine_spec.cpu_model}, {machine_spec.os_name} {machine_spec.os_version}, local machine",
            token=token,
        )

        # Create config using Pydantic model
        config = SelfHostedRunnerConfig(runners=[runner_spec])

        # Use Pydantic's to_yaml method
        config.to_yaml(Path(output_path))

        logger.info(f"âœ… Machine specification saved to: {output_path}")


class GitHubRunnerManager:
    """Manages GitHub runner information and recommendations."""

    def __init__(self, api: "GitHubAPI"):
        self.api = api
        self.github_hosted_runners = self._get_github_hosted_runners()

    def _get_github_hosted_runners(self) -> List[GitHubRunnerInfo]:
        """Get all available GitHub-hosted runners with specifications."""
        return [
            # Standard runners
            GitHubRunnerInfo(
                "ubuntu-latest", "ubuntu", "x64", 2, 7, 14, 0.008, ["ubuntu-latest"]
            ),
            GitHubRunnerInfo(
                "windows-latest", "windows", "x64", 2, 7, 14, 0.016, ["windows-latest"]
            ),
            GitHubRunnerInfo(
                "macos-latest", "macos", "x64", 3, 14, 14, 0.08, ["macos-latest"]
            ),
            GitHubRunnerInfo("macos-13", "macos", "x64", 3, 14, 14, 0.08, ["macos-13"]),
            GitHubRunnerInfo("macos-12", "macos", "x64", 3, 14, 14, 0.08, ["macos-12"]),
            # Larger runners - Ubuntu
            GitHubRunnerInfo(
                "ubuntu-latest-4-cores",
                "ubuntu",
                "x64",
                4,
                16,
                14,
                0.016,
                ["ubuntu-latest-4-cores"],
            ),
            GitHubRunnerInfo(
                "ubuntu-latest-8-cores",
                "ubuntu",
                "x64",
                8,
                32,
                14,
                0.032,
                ["ubuntu-latest-8-cores"],
            ),
            GitHubRunnerInfo(
                "ubuntu-latest-16-cores",
                "ubuntu",
                "x64",
                16,
                64,
                14,
                0.064,
                ["ubuntu-latest-16-cores"],
            ),
            GitHubRunnerInfo(
                "ubuntu-latest-32-cores",
                "ubuntu",
                "x64",
                32,
                128,
                14,
                0.128,
                ["ubuntu-latest-32-cores"],
            ),
            GitHubRunnerInfo(
                "ubuntu-latest-64-cores",
                "ubuntu",
                "x64",
                64,
                256,
                14,
                0.256,
                ["ubuntu-latest-64-cores"],
            ),
            # Larger runners - Windows
            GitHubRunnerInfo(
                "windows-latest-4-cores",
                "windows",
                "x64",
                4,
                16,
                14,
                0.032,
                ["windows-latest-4-cores"],
            ),
            GitHubRunnerInfo(
                "windows-latest-8-cores",
                "windows",
                "x64",
                8,
                32,
                14,
                0.064,
                ["windows-latest-8-cores"],
            ),
            GitHubRunnerInfo(
                "windows-latest-16-cores",
                "windows",
                "x64",
                16,
                64,
                14,
                0.128,
                ["windows-latest-16-cores"],
            ),
            GitHubRunnerInfo(
                "windows-latest-32-cores",
                "windows",
                "x64",
                32,
                128,
                14,
                0.256,
                ["windows-latest-32-cores"],
            ),
            GitHubRunnerInfo(
                "windows-latest-64-cores",
                "windows",
                "x64",
                64,
                256,
                14,
                0.512,
                ["windows-latest-64-cores"],
            ),
            # macOS larger runners
            GitHubRunnerInfo(
                "macos-latest-12-cores",
                "macos",
                "x64",
                12,
                30,
                14,
                0.12,
                ["macos-latest-12-cores"],
            ),
            # ARM64 runners - Ubuntu
            GitHubRunnerInfo(
                "ubuntu-latest-2-cores-arm",
                "ubuntu",
                "arm64",
                2,
                8,
                14,
                0.005,
                ["ubuntu-latest-2-cores-arm"],
            ),
            GitHubRunnerInfo(
                "ubuntu-latest-4-cores-arm",
                "ubuntu",
                "arm64",
                4,
                16,
                14,
                0.01,
                ["ubuntu-latest-4-cores-arm"],
            ),
            GitHubRunnerInfo(
                "ubuntu-latest-8-cores-arm",
                "ubuntu",
                "arm64",
                8,
                32,
                14,
                0.02,
                ["ubuntu-latest-8-cores-arm"],
            ),
            GitHubRunnerInfo(
                "ubuntu-latest-16-cores-arm",
                "ubuntu",
                "arm64",
                16,
                64,
                14,
                0.04,
                ["ubuntu-latest-16-cores-arm"],
            ),
            GitHubRunnerInfo(
                "ubuntu-latest-32-cores-arm",
                "ubuntu",
                "arm64",
                32,
                128,
                14,
                0.08,
                ["ubuntu-latest-32-cores-arm"],
            ),
            GitHubRunnerInfo(
                "ubuntu-latest-64-cores-arm",
                "ubuntu",
                "arm64",
                64,
                256,
                14,
                0.16,
                ["ubuntu-latest-64-cores-arm"],
            ),
            # ARM64 runners - Windows
            GitHubRunnerInfo(
                "windows-latest-2-cores-arm",
                "windows",
                "arm64",
                2,
                8,
                14,
                0.01,
                ["windows-latest-2-cores-arm"],
            ),
            GitHubRunnerInfo(
                "windows-latest-4-cores-arm",
                "windows",
                "arm64",
                4,
                16,
                14,
                0.02,
                ["windows-latest-4-cores-arm"],
            ),
            GitHubRunnerInfo(
                "windows-latest-8-cores-arm",
                "windows",
                "arm64",
                8,
                32,
                14,
                0.04,
                ["windows-latest-8-cores-arm"],
            ),
            GitHubRunnerInfo(
                "windows-latest-16-cores-arm",
                "windows",
                "arm64",
                16,
                64,
                14,
                0.08,
                ["windows-latest-16-cores-arm"],
            ),
            GitHubRunnerInfo(
                "windows-latest-32-cores-arm",
                "windows",
                "arm64",
                32,
                128,
                14,
                0.16,
                ["windows-latest-32-cores-arm"],
            ),
            GitHubRunnerInfo(
                "windows-latest-64-cores-arm",
                "windows",
                "arm64",
                64,
                256,
                14,
                0.32,
                ["windows-latest-64-cores-arm"],
            ),
            # macOS ARM64 (M1/M2)
            GitHubRunnerInfo(
                "macos-latest-6-cores-m1",
                "macos",
                "arm64",
                6,
                24,
                14,
                0.16,
                ["macos-latest-6-cores-m1"],
            ),
        ]

    def get_repository_runners(self, repo: Repository) -> ActualRunnerInfo:
        """Get all available runners for a repository."""
        github_hosted = self.github_hosted_runners

        # Get self-hosted runners from repository
        try:
            repo_runners = []
            for runner in repo.get_self_hosted_runners():
                repo_runners.append(
                    {
                        "id": runner.id,
                        "name": runner.name,
                        "os": runner.os,
                        "status": runner.status,
                        "labels": [label["name"] for label in runner.labels],
                    }
                )
        except Exception as e:
            print(f"âš ï¸  Could not fetch repository self-hosted runners: {e}")
            repo_runners = []

        # Get organization runners (if available)
        try:
            org_runners = []
            if hasattr(repo, "organization") and repo.organization:
                for runner in repo.organization.get_self_hosted_runners():
                    org_runners.append(
                        {
                            "id": runner.id,
                            "name": runner.name,
                            "os": runner.os,
                            "status": runner.status,
                            "labels": [label["name"] for label in runner.labels],
                        }
                    )
        except Exception as e:
            print(f"âš ï¸  Could not fetch organization self-hosted runners: {e}")
            org_runners = []

        return ActualRunnerInfo(
            github_hosted=github_hosted,
            self_hosted=[],  # Will be populated from YAML if provided
            organization_runners=org_runners,
            repository_runners=repo_runners,
        )


class WorkflowPatchGenerator:
    """Generates patches for workflow files to use optimal runners."""

    @staticmethod
    def generate_runner_patches(
        repo: Repository, recommendations: List[RunnerRecommendation]
    ) -> List[Dict[str, str]]:
        """Generate patches for workflow files based on runner recommendations."""
        patches = []

        try:
            # Get workflow files
            contents = repo.get_contents(".github/workflows")
            if not isinstance(contents, list):
                contents = [contents]

            for content in contents:
                if content.name.endswith((".yml", ".yaml")):
                    workflow_content = content.decoded_content.decode("utf-8")

                    # Find relevant recommendations for this workflow
                    workflow_recommendations = [
                        rec
                        for rec in recommendations
                        if rec.job_name in workflow_content
                        or "all-jobs" in rec.job_name
                    ]

                    if workflow_recommendations:
                        patched_content = workflow_content
                        changes_made = []

                        for rec in workflow_recommendations:
                            # Apply the patch
                            if rec.current_runner in patched_content:
                                patched_content = patched_content.replace(
                                    f"runs-on: {rec.current_runner}",
                                    f"runs-on: {rec.recommended_runner}",
                                )
                                patched_content = patched_content.replace(
                                    f"runs-on: [{rec.current_runner}]",
                                    f"runs-on: [{rec.recommended_runner}]",
                                )
                                changes_made.append(
                                    f"{rec.current_runner} â†’ {rec.recommended_runner}"
                                )

                        if changes_made:
                            patches.append(
                                {
                                    "file": content.name,
                                    "original": workflow_content,
                                    "patched": patched_content,
                                    "changes": changes_made,
                                    "savings": sum(
                                        rec.cost_savings
                                        for rec in workflow_recommendations
                                    ),
                                }
                            )

        except Exception as e:
            print(f"âš ï¸  Could not generate workflow patches: {e}")

        return patches


class CopilotPromptGenerator:
    """Generates prompts for GitHub Copilot to implement improvements."""

    @staticmethod
    def generate_workflow_optimization_prompt(
        recommendations: List[RunnerRecommendation],
    ) -> str:
        """Generate a prompt for Copilot to optimize workflow runner usage."""
        runner_changes = {}
        total_savings = sum(rec.cost_savings for rec in recommendations)

        for rec in recommendations:
            if rec.current_runner not in runner_changes:
                runner_changes[rec.current_runner] = []
            runner_changes[rec.current_runner].append(rec.recommended_runner)

        prompt = f"""Please optimize the GitHub Actions workflow files to use more cost-effective runners.

ANALYSIS RESULTS:
- Total potential annual savings: ${total_savings:.2f}
- Number of optimizations: {len(recommendations)}

RECOMMENDED CHANGES:
"""

        for current, recommended_list in runner_changes.items():
            # Get most common recommendation
            recommended = list(set(recommended_list))[0]
            savings = sum(
                rec.cost_savings
                for rec in recommendations
                if rec.current_runner == current
            )
            prompt += f"""
- Replace `runs-on: {current}` with `runs-on: {recommended}`
  Reason: More cost-effective for the workload (saves ${savings:.2f}/year)"""

        prompt += f"""

IMPLEMENTATION STEPS:
1. Review all .github/workflows/*.yml files
2. Update the `runs-on` field for each job as specified above
3. Test the workflows to ensure compatibility
4. Monitor performance after changes

ADDITIONAL CONSIDERATIONS:
- Verify that the new runners support all required software/tools
- Update any runner-specific configuration if needed
- Consider matrix strategy optimizations for parallel jobs
"""

        return prompt

    @staticmethod
    def generate_repository_configuration_prompt(
        actual_runners: ActualRunnerInfo, analysis_results: Dict
    ) -> str:
        """Generate a prompt for Copilot to configure repository/organization settings."""

        prompt = f"""Please help configure this GitHub repository and organization for optimal Actions performance and cost efficiency.

CURRENT SITUATION:
- Repository has {len(actual_runners.repository_runners)} self-hosted runners
- Organization has {len(actual_runners.organization_runners)} self-hosted runners
- Current monthly Actions cost: ${analysis_results.get('monthly_projection', 0):.2f}
- Current efficiency: {analysis_results.get('efficiency', 0):.1f}%

CONFIGURATION RECOMMENDATIONS:

1. REPOSITORY SETTINGS:
   - Enable Actions cost monitoring and spending limits
   - Configure workflow run approval for external contributors
   - Set up branch protection rules to prevent expensive workflow runs on all branches

2. ORGANIZATION SETTINGS:
   - Create organization-level self-hosted runner groups for better resource sharing
   - Implement workflow spending policies and limits
   - Set up automated alerts for unusual spending patterns

3. WORKFLOW POLICIES:
   - Implement path-based triggering to avoid unnecessary runs
   - Add workflow timeout limits to prevent runaway costs
   - Enable workflow run concurrency limits

4. MONITORING SETUP:
   - Create custom Actions for monitoring workflow costs
   - Set up regular reporting on Actions usage and efficiency
   - Implement automated optimization suggestions

Please help implement these configurations step by step, starting with the most impactful changes.
"""

        return prompt


class RunnerOptimizer:
    """Analyzes and optimizes runner selection for workflows."""

    def __init__(self, actual_runners: ActualRunnerInfo):
        self.actual_runners = actual_runners

    def analyze_workflow_requirements(
        self, workflow_runs: List[Dict]
    ) -> Dict[str, Dict]:
        """Analyze resource requirements for each workflow/job."""
        requirements = {}

        for run in workflow_runs:
            workflow_name = run.get("workflow_name", "Unknown")
            duration = run.get("duration_minutes", 0)

            if workflow_name not in requirements:
                requirements[workflow_name] = {
                    "avg_duration": 0,
                    "max_duration": 0,
                    "run_count": 0,
                    "current_runner": run.get("runner_type", "ubuntu-latest"),
                    "estimated_cpu_usage": "medium",  # This would need job-level analysis
                    "estimated_memory_usage": "medium",
                }

            req = requirements[workflow_name]
            req["run_count"] += 1
            req["avg_duration"] = (
                req["avg_duration"] * (req["run_count"] - 1) + duration
            ) / req["run_count"]
            req["max_duration"] = max(req["max_duration"], duration)

        return requirements

    def recommend_optimal_runners(
        self, workflow_requirements: Dict[str, Dict]
    ) -> List[RunnerRecommendation]:
        """Recommend optimal runners for each workflow based on requirements and available runners."""
        recommendations = []

        for workflow_name, req in workflow_requirements.items():
            current_runner = req["current_runner"]
            current_runner_info = self._find_runner_info(current_runner)

            if not current_runner_info:
                continue

            # Find optimal runner based on requirements
            optimal_runner = self._find_optimal_runner(req, current_runner_info)

            if optimal_runner and optimal_runner.name != current_runner:
                cost_savings = self._calculate_cost_savings(
                    current_runner_info,
                    optimal_runner,
                    req["avg_duration"],
                    req["run_count"],
                )

                recommendations.append(
                    RunnerRecommendation(
                        job_name=workflow_name,
                        current_runner=current_runner,
                        recommended_runner=optimal_runner.name,
                        reason=self._generate_recommendation_reason(
                            current_runner_info, optimal_runner, req
                        ),
                        cost_savings=cost_savings,
                        performance_improvement=self._estimate_performance_improvement(
                            current_runner_info, optimal_runner
                        ),
                        patch_content=f"runs-on: {optimal_runner.name}",
                    )
                )

        return recommendations

    def _find_runner_info(self, runner_name: str) -> Optional[GitHubRunnerInfo]:
        """Find runner information by name."""
        for runner in self.actual_runners.github_hosted:
            if runner.name == runner_name:
                return runner
        return None

    def _find_optimal_runner(
        self, requirements: Dict, current_runner: GitHubRunnerInfo
    ) -> Optional[GitHubRunnerInfo]:
        """Find the most cost-effective runner for given requirements."""
        duration = requirements["avg_duration"]

        # For short jobs (< 5 minutes), prefer standard runners
        if duration < 5:
            candidates = [r for r in self.actual_runners.github_hosted if r.cores <= 4]
        # For medium jobs (5-20 minutes), consider larger runners if cost-effective
        elif duration < 20:
            candidates = [r for r in self.actual_runners.github_hosted if r.cores <= 8]
        # For long jobs (> 20 minutes), consider all runners including ARM64
        else:
            candidates = self.actual_runners.github_hosted

        # Filter by OS compatibility
        current_os = current_runner.os
        candidates = [r for r in candidates if r.os == current_os and r.available]

        if not candidates:
            return None

        # Find most cost-effective option
        best_runner = min(candidates, key=lambda r: r.cost_per_minute * duration)

        # Only recommend if it's actually better
        current_cost = current_runner.cost_per_minute * duration
        best_cost = best_runner.cost_per_minute * duration

        if best_cost < current_cost * 0.9:  # At least 10% savings
            return best_runner

        return None

    def _calculate_cost_savings(
        self,
        current: GitHubRunnerInfo,
        optimal: GitHubRunnerInfo,
        avg_duration: float,
        run_count: int,
    ) -> float:
        """Calculate annual cost savings from switching runners."""
        current_cost_per_run = current.cost_per_minute * avg_duration
        optimal_cost_per_run = optimal.cost_per_minute * avg_duration
        savings_per_run = current_cost_per_run - optimal_cost_per_run

        # Estimate annual runs (assuming current count is from last 30 days)
        annual_runs = run_count * 12
        return savings_per_run * annual_runs

    def _generate_recommendation_reason(
        self, current: GitHubRunnerInfo, optimal: GitHubRunnerInfo, requirements: Dict
    ) -> str:
        """Generate human-readable reason for the recommendation."""
        reasons = []

        if optimal.cost_per_minute < current.cost_per_minute:
            savings_pct = (1 - optimal.cost_per_minute / current.cost_per_minute) * 100
            reasons.append(f"{savings_pct:.0f}% lower cost per minute")

        if optimal.arch == "arm64" and current.arch == "x64":
            reasons.append("ARM64 provides better price/performance ratio")

        if requirements["avg_duration"] < 10 and optimal.cores < current.cores:
            reasons.append("Smaller runner sufficient for short-duration jobs")

        if not reasons:
            reasons.append("Better resource efficiency for workload")

        return "; ".join(reasons)

    def _estimate_performance_improvement(
        self, current: GitHubRunnerInfo, optimal: GitHubRunnerInfo
    ) -> str:
        """Estimate performance improvement/degradation."""
        if optimal.cores > current.cores:
            improvement = (optimal.cores / current.cores - 1) * 100
            return f"~{improvement:.0f}% faster (more cores)"
        elif optimal.cores < current.cores:
            degradation = (1 - optimal.cores / current.cores) * 100
            return f"~{degradation:.0f}% slower (fewer cores)"
        else:
            return "Similar performance expected"


class GitHubActionsOptimizer:
    """Comprehensive GitHub Actions workflow optimization platform."""

    def __init__(self, use_dynamic_pricing: bool = False, require_auth: bool = True):
        self.runs_data: List[WorkflowRunStats] = []
        self.workflows_summary: Dict[str, WorkflowSummary] = {}
        self.pricing = PRICING.copy()
        self.self_hosted_config: Optional[SelfHostedRunnerConfig] = None
        self.api = None
        if require_auth:
            self.api = GitHubAPI()  # Initialize GitHubAPI for caching
        if use_dynamic_pricing:
            self._update_pricing_from_docs()

    def set_self_hosted_config(self, config: SelfHostedRunnerConfig) -> None:
        """Set self-hosted runner configuration."""
        self.self_hosted_config = config
        # Add self-hosted runners to pricing table
        for runner in config.runners:
            self.pricing[runner.name] = runner.cost_per_minute

    def _update_pricing_from_docs(self) -> None:
        """Attempt to fetch latest pricing from GitHub documentation."""
        try:
            # This would be the ideal implementation but GitHub doesn't have a pricing API
            # For now, we use the comprehensive static pricing table updated from official docs
            print(
                "â„¹ï¸  Using comprehensive static pricing table (updated from GitHub docs)"
            )
        except Exception as e:
            print(f"âš ï¸  Could not fetch dynamic pricing, using static table: {e}")

    def _detect_runner_type(self, run_data: Dict[str, Any]) -> str:
        """Intelligently detect runner type from workflow run data."""
        # Direct specification in run data
        if "runner_type" in run_data:
            return run_data["runner_type"]

        # Try to extract from runner name or labels
        if "runner_name" in run_data:
            runner_name = run_data["runner_name"].lower()
            # Check for larger runners patterns
            if "arm" in runner_name or "aarch64" in runner_name:
                if "64-cores" in runner_name:
                    return "ubuntu-latest-64-cores-arm"
                elif "32-cores" in runner_name:
                    return "ubuntu-latest-32-cores-arm"
                elif "16-cores" in runner_name:
                    return "ubuntu-latest-16-cores-arm"
                elif "8-cores" in runner_name:
                    return "ubuntu-latest-8-cores-arm"
                elif "4-cores" in runner_name:
                    return "ubuntu-latest-4-cores-arm"
                elif "2-cores" in runner_name:
                    return "ubuntu-latest-2-cores-arm"
            elif "gpu" in runner_name:
                if "windows" in runner_name:
                    return "windows-latest-4-cores-gpu"
                else:
                    return "ubuntu-latest-4-cores-gpu"
            elif any(
                size in runner_name
                for size in ["64-cores", "32-cores", "16-cores", "8-cores", "4-cores"]
            ):
                # Extract core count
                core_match = re.search(r"(\d+)-cores?", runner_name)
                if core_match:
                    cores = core_match.group(1)
                    if "windows" in runner_name:
                        return f"windows-latest-{cores}-cores"
                    elif "macos" in runner_name:
                        return f"macos-latest-{cores}-cores"
                    else:
                        return f"ubuntu-latest-{cores}-cores"

        # Extract from workflow configuration or job labels
        if "labels" in run_data:
            labels = run_data["labels"]
            if isinstance(labels, list):
                for label in labels:
                    label_lower = str(label).lower()
                    if label_lower in self.pricing:
                        return label_lower
                    # Check for partial matches
                    for runner_type in self.pricing.keys():
                        if runner_type in label_lower or label_lower in runner_type:
                            return runner_type

        # Fallback: try to extract from OS information
        if "os" in run_data:
            os_name = run_data["os"].lower()
            if "windows" in os_name:
                return "windows-latest"
            elif "macos" in os_name:
                return "macos-latest"
            else:
                return "ubuntu-latest"

    def load_github_data(
        self, owner: str, repo: str, github_token: Optional[str] = None
    ) -> None:
        """Load real workflow data from GitHub API with pandas-based parallel processing."""
        logger.info(f"ðŸ”„ Fetching workflow data from GitHub API for {owner}/{repo}...")

        try:
            # Use instance API that may have cache-only mode set
            api = self.api

            # Get all workflows as DataFrame
            workflows_df = api.get_workflows(owner, repo)

            if workflows_df.empty:
                logger.warning("No workflows found in this repository")
                return

            # Fetch workflow runs in parallel using DataFrame
            runs_df = api.get_workflow_runs_parallel_dataframe(
                owner, repo, workflows_df
            )

            if runs_df.empty:
                logger.warning("No workflow runs found")
                return

            # Fetch job data for accurate runner type detection
            logger.info("ðŸ” Fetching job data for accurate runner type detection...")

            # Filter to successful runs only for meaningful resource analysis
            successful_runs_df = runs_df[
                (runs_df["conclusion"] == "success")
                & (runs_df["status"] == "completed")
            ].copy()

            logger.info(
                f"ðŸ“Š Processing {len(successful_runs_df)} successful runs out of {len(runs_df)} total runs"
            )

            # For large repositories, sample the most recent runs to avoid API rate limits
            if len(successful_runs_df) > 50:
                logger.info(
                    f"âš¡ Large repository detected. Sampling 50 most recent successful runs from {len(successful_runs_df)} total"
                )
                # Convert created_at to datetime for proper sorting
                successful_runs_df["created_at_dt"] = pd.to_datetime(
                    successful_runs_df["created_at"]
                )
                successful_runs_df = successful_runs_df.nlargest(50, "created_at_dt")

            # Get job data for each successful run to determine actual runner types
            jobs_data = {}
            for _, run_row in tqdm(
                list(successful_runs_df.iterrows()), desc="ðŸ” Fetching job details"
            ):
                try:
                    run_id = run_row["id"]
                    jobs = api.get_workflow_run_jobs(owner, repo, run_id)
                    jobs_data[run_id] = jobs
                    # Small delay to be API-friendly
                    time.sleep(0.1)
                except Exception as e:
                    logger.warning(f"Could not fetch jobs for run {run_id}: {e}")
                    jobs_data[run_id] = []

            # Use pandas to efficiently process runs and jobs
            logger.info("ðŸš€ Processing workflow data with pandas...")

            # Convert runs to the format expected by the analyzer
            with tqdm(
                runs_df.iterrows(),
                total=len(runs_df),
                desc="ðŸ“Š Processing runs",
                unit="run",
            ) as pbar:
                for _, run_row in pbar:
                    run_id = run_row["id"]

                    # Get job data for this run to determine runner types
                    run_jobs = jobs_data.get(run_id, [])

                    # If no jobs or multiple different runners, create separate entries per job
                    if not run_jobs:
                        # Fallback if no job data available
                        processed_run_data = {
                            "id": run_id,
                            "workflow_id": run_row["workflow_id"],
                            "status": run_row["status"],
                            "conclusion": run_row["conclusion"],
                            "created_at": run_row["created_at"],
                            "updated_at": run_row["updated_at"],
                            "event": run_row["event"],
                            "head_sha": run_row["head_sha"],
                            "head_branch": run_row["head_branch"],
                            "job_name": "unknown",
                            "labels": [],
                            "runner_name": "",
                            "started_at": run_row["created_at"],
                            "completed_at": run_row["updated_at"],
                        }

                        workflow_name = run_row.get("workflow_name", "Unknown")
                        self.add_workflow_run(processed_run_data, workflow_name)
                    else:
                        # Create entries for each job to get accurate runner types
                        for job in run_jobs:
                            processed_run_data = {
                                "id": run_id,
                                "workflow_id": run_row["workflow_id"],
                                "status": run_row["status"],
                                "conclusion": run_row["conclusion"],
                                "created_at": run_row["created_at"],
                                "updated_at": run_row["updated_at"],
                                "event": run_row["event"],
                                "head_sha": run_row["head_sha"],
                                "head_branch": run_row["head_branch"],
                                "job_name": job.get("name", "unknown"),
                                "labels": job.get("labels", []),
                                "runner_name": job.get("runner_name", ""),
                                "started_at": job.get(
                                    "started_at", run_row["created_at"]
                                ),
                                "completed_at": job.get(
                                    "completed_at", run_row["updated_at"]
                                ),
                            }

                            workflow_name = run_row.get("workflow_name", "Unknown")
                            self.add_workflow_run(processed_run_data, workflow_name)

                    pbar.set_postfix(
                        {"workflow": run_row.get("workflow_name", "Unknown")[:20]}
                    )

            logger.info(
                f"ï¿½ Processed {len(runs_df)} workflow runs with pandas optimization"
            )

        except Exception as e:
            logger.error(f"âŒ Error fetching GitHub data: {e}")
            raise

    def _extract_runner_from_job(self, job: Dict[str, Any]) -> str:
        """Extract runner type from job data."""
        # Check job labels
        labels = job.get("labels", [])
        if labels:
            for label in labels:
                label_str = str(label).lower()
                # Check if it matches any known runner type
                for runner_type in self.pricing.keys():
                    if runner_type in label_str or label_str in runner_type:
                        return runner_type

        # Check runner name
        runner_name = job.get("runner_name", "").lower()
        if runner_name:
            for runner_type in self.pricing.keys():
                if runner_type in runner_name:
                    return runner_type

        # Fallback based on OS detection
        return self._detect_runner_from_os(job)

    def _extract_os_from_job(self, job: Dict[str, Any]) -> str:
        """Extract OS information from job."""
        labels = job.get("labels", [])
        for label in labels:
            label_str = str(label).lower()
            if "windows" in label_str:
                return "windows"
            elif "macos" in label_str:
                return "macos"
            elif "ubuntu" in label_str or "linux" in label_str:
                return "ubuntu"
        return "ubuntu"  # Default

    def _detect_runner_from_os(self, job: Dict[str, Any]) -> str:
        """Detect runner type based on OS and other job information."""
        os_type = self._extract_os_from_job(job)

        if os_type == "windows":
            return "windows-latest"
        elif os_type == "macos":
            return "macos-latest"
        else:
            return "ubuntu-latest"

    def add_workflow_run(self, run_data: Dict[str, Any], workflow_name: str) -> None:
        """Add a workflow run to the analysis."""
        # Parse duration - handle both direct seconds and ISO duration
        duration_seconds = self._parse_duration(run_data)

        # Calculate billable minutes (rounded up to nearest minute)
        billable_minutes = max(1, (duration_seconds + 59) // 60)  # Round up

        # Intelligently determine runner type
        runner_type = self._detect_runner_type(run_data)

        # Calculate cost using comprehensive pricing table
        cost_usd = billable_minutes * self.pricing.get(
            runner_type, self.pricing["ubuntu-latest"]
        )

        # Create workflow run stats
        run_stats = WorkflowRunStats(
            run_id=run_data.get("id", 0),
            workflow_name=workflow_name,
            workflow_id=run_data.get("workflow_id", 0),
            duration_seconds=duration_seconds,
            billable_minutes=billable_minutes,
            cost_usd=cost_usd,
            status=run_data.get("status", "unknown"),
            conclusion=run_data.get("conclusion", "unknown"),
            event=run_data.get("event", "unknown"),
            created_at=run_data.get("created_at", ""),
            runner_type=runner_type,
            head_sha=run_data.get("head_sha", ""),
            head_branch=run_data.get("head_branch", ""),
        )

        self.runs_data.append(run_stats)

    def _parse_duration(self, run_data: Dict[str, Any]) -> int:
        """Parse duration from various formats."""
        # Try direct duration_seconds first
        if "duration_seconds" in run_data:
            return run_data["duration_seconds"]

        # Calculate from timestamps
        if "created_at" in run_data and "updated_at" in run_data:
            try:
                created = datetime.fromisoformat(
                    run_data["created_at"].replace("Z", "+00:00")
                )
                updated = datetime.fromisoformat(
                    run_data["updated_at"].replace("Z", "+00:00")
                )
                return int((updated - created).total_seconds())
            except ValueError:
                pass

        # Default to 60 seconds if no duration available
        return 60

    def calculate_workflow_summaries(self) -> None:
        """Calculate summary statistics for each workflow."""
        workflow_groups = defaultdict(list)

        # Group runs by workflow
        for run in self.runs_data:
            workflow_groups[run.workflow_name].append(run)

        # Calculate summaries
        for workflow_name, runs in workflow_groups.items():
            summary = self._calculate_single_workflow_summary(workflow_name, runs)
            self.workflows_summary[workflow_name] = summary

    def _calculate_single_workflow_summary(
        self, workflow_name: str, runs: List[WorkflowRunStats]
    ) -> WorkflowSummary:
        """Calculate summary for a single workflow."""
        if not runs:
            return self._empty_workflow_summary(workflow_name)

        # Basic metrics
        total_runs = len(runs)
        durations = [run.duration_seconds for run in runs]
        costs = [run.cost_usd for run in runs]

        # Status counts
        success_count = sum(1 for run in runs if run.conclusion == "success")
        failure_count = sum(1 for run in runs if run.conclusion == "failure")
        cancelled_count = sum(1 for run in runs if run.conclusion == "cancelled")

        # Rates (as percentages)
        success_rate = (success_count / total_runs * 100) if total_runs > 0 else 0
        failure_rate = (failure_count / total_runs * 100) if total_runs > 0 else 0
        cancellation_rate = (
            (cancelled_count / total_runs * 100) if total_runs > 0 else 0
        )

        # Statistical calculations
        total_duration = sum(durations)
        total_cost = sum(costs)
        avg_duration = statistics.mean(durations) if durations else 0
        std_duration = statistics.stdev(durations) if len(durations) > 1 else 0

        # Cost analysis
        cost_per_run = total_cost / total_runs if total_runs > 0 else 0
        waste_from_failures = sum(
            run.cost_usd for run in runs if run.conclusion == "failure"
        )

        # Risk assessment
        risk_level = self._assess_risk_level(failure_rate)
        cost_impact = self._assess_cost_impact(total_cost, total_runs)

        # Projections (assuming current sample is representative)
        monthly_projection = (
            total_cost * 30 / max(1, len(durations))
        )  # Rough monthly estimate
        annual_projection = monthly_projection * 12

        return WorkflowSummary(
            workflow_name=workflow_name,
            workflow_id=runs[0].workflow_id,
            total_runs=total_runs,
            sampled_runs=total_runs,
            total_duration_seconds=total_duration,
            total_billable_minutes=sum(run.billable_minutes for run in runs),
            total_cost_usd=total_cost,
            avg_duration_seconds=avg_duration,
            min_duration_seconds=min(durations) if durations else 0,
            max_duration_seconds=max(durations) if durations else 0,
            std_duration_seconds=std_duration,
            success_rate=success_rate,
            failure_rate=failure_rate,
            cancellation_rate=cancellation_rate,
            failure_count=failure_count,
            success_count=success_count,
            cancelled_count=cancelled_count,
            cost_per_run=cost_per_run,
            waste_from_failures=waste_from_failures,
            risk_level=risk_level,
            cost_impact=cost_impact,
            monthly_projection=monthly_projection,
            annual_projection=annual_projection,
        )

    def _empty_workflow_summary(self, workflow_name: str) -> WorkflowSummary:
        """Create empty summary for workflow with no runs."""
        return WorkflowSummary(
            workflow_name=workflow_name,
            workflow_id=0,
            total_runs=0,
            sampled_runs=0,
            total_duration_seconds=0,
            total_billable_minutes=0,
            total_cost_usd=0,
            avg_duration_seconds=0,
            min_duration_seconds=0,
            max_duration_seconds=0,
            std_duration_seconds=0,
            success_rate=0,
            failure_rate=0,
            cancellation_rate=0,
            failure_count=0,
            success_count=0,
            cancelled_count=0,
            cost_per_run=0,
            waste_from_failures=0,
            risk_level="UNKNOWN",
            cost_impact="UNKNOWN",
            monthly_projection=0,
            annual_projection=0,
        )

    def _assess_risk_level(self, failure_rate: float) -> str:
        """Assess risk level based on failure rate."""
        if failure_rate >= RISK_THRESHOLDS["CRITICAL"]:
            return "CRITICAL"
        elif failure_rate >= RISK_THRESHOLDS["HIGH"]:
            return "HIGH"
        elif failure_rate >= RISK_THRESHOLDS["MEDIUM"]:
            return "MEDIUM"
        elif failure_rate >= RISK_THRESHOLDS["LOW"]:
            return "LOW"
        else:
            return "MINIMAL"

    def _assess_cost_impact(self, total_cost: float, total_runs: int) -> str:
        """Assess cost impact based on total cost and frequency."""
        if total_cost > 10 or total_runs > 100:
            return "HIGH"
        elif total_cost > 1 or total_runs > 50:
            return "MEDIUM"
        else:
            return "LOW"

    def generate_analysis(
        self,
        repository: str,
        include_resource_analysis: bool = False,
        include_matrix_optimization: bool = False,
        include_self_hosted_advice: bool = False,
    ) -> RepositoryAnalysis:
        """Generate complete repository analysis."""
        self.calculate_workflow_summaries()

        # Calculate repository totals
        total_workflows = len(self.workflows_summary)
        total_runs = sum(ws.total_runs for ws in self.workflows_summary.values())
        total_cost = sum(ws.total_cost_usd for ws in self.workflows_summary.values())
        total_waste = sum(
            ws.waste_from_failures for ws in self.workflows_summary.values()
        )
        waste_percentage = (total_waste / total_cost * 100) if total_cost > 0 else 0

        # Projections
        monthly_projection = sum(
            ws.monthly_projection for ws in self.workflows_summary.values()
        )
        annual_projection = sum(
            ws.annual_projection for ws in self.workflows_summary.values()
        )

        # High priority issues
        high_priority_issues = [
            {
                "workflow_name": ws.workflow_name,
                "risk_level": ws.risk_level,
                "failure_rate": ws.failure_rate,
                "waste_cost": ws.waste_from_failures,
                "total_runs": ws.total_runs,
                "monthly_waste": ws.waste_from_failures * 30 / max(1, ws.sampled_runs),
            }
            for ws in self.workflows_summary.values()
            if ws.risk_level in ["CRITICAL", "HIGH"]
        ]
        high_priority_issues.sort(key=lambda x: x["waste_cost"], reverse=True)

        # Advanced analysis (conditional)
        runner_efficiency = []
        matrix_optimization = []
        self_hosted_recommendations = []

        if include_resource_analysis:
            resource_analyzer = RunnerResourceAnalyzer()
            runner_efficiency = resource_analyzer.analyze_runner_efficiency(
                self.runs_data
            )

        if include_matrix_optimization:
            matrix_analyzer = MatrixOptimizationAnalyzer()
            matrix_optimization = matrix_analyzer.analyze_matrix_workflows(
                list(self.workflows_summary.values())
            )

        if include_self_hosted_advice:
            # Convert repository name to URL for self-hosted advisor
            repo_url = (
                f"https://github.com/{repository}" if "/" in repository else repository
            )
            self_hosted_advisor = SelfHostedRunnerAdvisor(repository_url=repo_url)
            self_hosted_recommendations = (
                self_hosted_advisor.generate_setup_recommendations(
                    monthly_projection, runner_efficiency
                )
            )

        # Generate recommendations
        recommendations = self._generate_recommendations(
            high_priority_issues, total_waste
        )

        return RepositoryAnalysis(
            repository=repository,
            analysis_date=datetime.now().isoformat(),
            total_workflows=total_workflows,
            total_runs=total_runs,
            total_cost=total_cost,
            total_waste=total_waste,
            waste_percentage=waste_percentage,
            monthly_projection=monthly_projection,
            annual_projection=annual_projection,
            workflows=list(self.workflows_summary.values()),
            high_priority_issues=high_priority_issues,
            recommendations=recommendations,
            raw_data=self.runs_data,
            runner_efficiency=runner_efficiency,
            matrix_optimization=matrix_optimization,
            self_hosted_recommendations=self_hosted_recommendations,
        )

    def _generate_recommendations(
        self, high_priority_issues: List[Dict], total_waste: float
    ) -> List[Dict[str, Any]]:
        """Generate optimization recommendations."""
        recommendations = []

        if high_priority_issues:
            recommendations.append(
                {
                    "priority": "ðŸ”´ URGENT",
                    "category": "reliability",
                    "action": f"Fix systematic failures in {len(high_priority_issues)} workflows",
                    "impact": f"Potential savings: ${sum(issue['waste_cost'] for issue in high_priority_issues):.2f}",
                    "details": "Address workflows with critical failure rates",
                    "workflows": [
                        issue["workflow_name"] for issue in high_priority_issues
                    ],
                }
            )

        if total_waste > 1:
            recommendations.append(
                {
                    "priority": "ðŸŸ¡ HIGH",
                    "category": "optimization",
                    "action": "Implement workflow run caching",
                    "impact": "Reduce average run time by 20-40%",
                    "details": "Cache dependencies, build artifacts, and test results",
                    "workflows": [],
                }
            )

            recommendations.append(
                {
                    "priority": "ðŸŸ¡ HIGH",
                    "category": "optimization",
                    "action": "Optimize workflow triggers",
                    "impact": "Reduce unnecessary runs by 15-25%",
                    "details": "Use path-based triggering and skip duplicate runs",
                    "workflows": [],
                }
            )

        recommendations.append(
            {
                "priority": "ðŸŸ¢ MEDIUM",
                "category": "monitoring",
                "action": "Implement fail-fast strategies",
                "impact": "Reduce failed run costs by 50%",
                "details": "Stop workflows early when critical steps fail",
                "workflows": [],
            }
        )

        recommendations.append(
            {
                "priority": "ðŸŸ¢ MEDIUM",
                "category": "monitoring",
                "action": "Monitor and alert on workflow health",
                "impact": "Prevent cost accumulation from broken workflows",
                "details": "Set up alerts for failure rate thresholds",
                "workflows": [],
            }
        )

        return recommendations


class RunnerResourceAnalyzer:
    """Analyzes runner resource usage and provides scaling recommendations."""

    def __init__(self):
        self.usage_patterns = {}

    def analyze_runner_efficiency(
        self, runs_data: List[WorkflowRunStats]
    ) -> List[RunnerResourceUsage]:
        """Analyze runner efficiency and recommend optimizations."""
        runner_groups = defaultdict(list)

        # Group runs by runner type - ONLY include successful runs for resource analysis
        for run in runs_data:
            # Only analyze successful runs to get meaningful resource utilization data
            if run.conclusion == "success" and run.status == "completed":
                runner_groups[run.runner_type].append(run)

        analyses = []
        for runner_type, runs in runner_groups.items():
            if not runs:
                continue

            analysis = self._analyze_single_runner_type(runner_type, runs)
            analyses.append(analysis)

        return analyses

    def _analyze_single_runner_type(
        self, runner_type: str, runs: List[WorkflowRunStats]
    ) -> RunnerResourceUsage:
        """Analyze a single runner type for efficiency."""
        if not runs:
            return self._empty_runner_analysis(runner_type)

        specs = RUNNER_SPECS.get(runner_type, RUNNER_SPECS["ubuntu-latest"])

        # Calculate usage patterns from successful runs only
        durations = [run.duration_seconds for run in runs]
        avg_duration = statistics.mean(durations)
        workflow_names = [run.workflow_name for run in runs]

        # More intelligent resource estimation based on workflow patterns
        estimated_cpu_usage = self._estimate_cpu_usage(
            avg_duration, workflow_names, runner_type
        )
        estimated_memory_usage = self._estimate_memory_usage(
            avg_duration, workflow_names, runner_type
        )

        # Calculate utilization efficiency based on runner specs
        cpu_efficiency = estimated_cpu_usage / 100.0
        memory_efficiency = estimated_memory_usage / 100.0
        utilization_efficiency = (cpu_efficiency + memory_efficiency) / 2.0

        # Recommend optimal runner
        recommended_runner = self._recommend_optimal_runner(
            estimated_cpu_usage, estimated_memory_usage, runner_type
        )

        # Calculate potential savings
        current_cost_per_minute = PRICING.get(runner_type, PRICING["ubuntu-latest"])
        recommended_cost_per_minute = PRICING.get(
            recommended_runner, current_cost_per_minute
        )

        total_minutes = sum(run.billable_minutes for run in runs)
        current_cost = total_minutes * current_cost_per_minute
        recommended_cost = total_minutes * recommended_cost_per_minute
        potential_savings = max(0, current_cost - recommended_cost)

        # Determine confidence level
        confidence = self._determine_confidence_level(len(runs), utilization_efficiency)

        return RunnerResourceUsage(
            runner_type=runner_type,
            total_runs=len(runs),
            avg_duration_seconds=avg_duration,
            estimated_cpu_usage_percent=estimated_cpu_usage,
            estimated_memory_usage_percent=estimated_memory_usage,
            utilization_efficiency=utilization_efficiency,
            recommended_runner=recommended_runner,
            potential_savings_usd=potential_savings,
            confidence_level=confidence,
        )

    def _recommend_optimal_runner(
        self, cpu_usage: float, memory_usage: float, current_runner: str
    ) -> str:
        """Recommend optimal runner based on usage patterns."""
        current_specs = RUNNER_SPECS.get(current_runner, RUNNER_SPECS["ubuntu-latest"])

        # If usage is low, recommend smaller runner
        if cpu_usage < 30 and memory_usage < 30:
            if "arm" not in current_runner and current_specs["cores"] > 2:
                # Recommend ARM or smaller runner
                return current_runner.replace("latest", "latest-2-cores-arm")

        # If usage is high, recommend larger runner
        if cpu_usage > 80 or memory_usage > 80:
            if current_specs["cores"] < 8:
                return current_runner.replace("latest", "latest-8-cores")

        return current_runner

    def _determine_confidence_level(self, sample_size: int, utilization: float) -> str:
        """Determine confidence level for recommendations."""
        if sample_size < 5:
            return "LOW"
        elif sample_size < 20:
            return "MEDIUM"
        else:
            return "HIGH"

    def _empty_runner_analysis(self, runner_type: str) -> RunnerResourceUsage:
        """Create empty analysis for runner with no data."""
        return RunnerResourceUsage(
            runner_type=runner_type,
            total_runs=0,
            avg_duration_seconds=0,
            estimated_cpu_usage_percent=0,
            estimated_memory_usage_percent=0,
            utilization_efficiency=0,
            recommended_runner=runner_type,
            potential_savings_usd=0,
            confidence_level="LOW",
        )

    def _estimate_cpu_usage(
        self, avg_duration: float, workflow_names: List[str], runner_type: str
    ) -> float:
        """Estimate CPU usage based on workflow patterns and duration."""
        base_usage = 25.0  # Base CPU usage for any workflow

        # Adjust based on workflow types
        for workflow_name in set(workflow_names):
            name_lower = workflow_name.lower()
            if any(keyword in name_lower for keyword in ["build", "compile", "test"]):
                base_usage += 15.0
            elif any(keyword in name_lower for keyword in ["deploy", "publish"]):
                base_usage += 10.0
            elif any(keyword in name_lower for keyword in ["lint", "format"]):
                base_usage += 5.0

        # Adjust based on duration (longer runs tend to use more CPU)
        if avg_duration > 1800:  # 30 minutes
            base_usage += 20.0
        elif avg_duration > 600:  # 10 minutes
            base_usage += 10.0

        # Adjust based on runner type
        specs = RUNNER_SPECS.get(runner_type, RUNNER_SPECS["ubuntu-latest"])
        if specs["cores"] >= 8:
            base_usage = base_usage * 0.8  # Better distribution on more cores

        return min(95.0, max(15.0, base_usage))

    def _estimate_memory_usage(
        self, avg_duration: float, workflow_names: List[str], runner_type: str
    ) -> float:
        """Estimate memory usage based on workflow patterns and duration."""
        base_usage = 20.0  # Base memory usage

        # Adjust based on workflow types
        for workflow_name in set(workflow_names):
            name_lower = workflow_name.lower()
            if any(keyword in name_lower for keyword in ["build", "compile"]):
                base_usage += 25.0  # Build processes are memory intensive
            elif any(keyword in name_lower for keyword in ["test"]):
                base_usage += 15.0  # Tests can be memory intensive
            elif any(keyword in name_lower for keyword in ["docker", "container"]):
                base_usage += 30.0  # Docker builds use lots of memory

        # Adjust based on duration
        if avg_duration > 1800:  # 30 minutes
            base_usage += 15.0
        elif avg_duration > 600:  # 10 minutes
            base_usage += 8.0

        # Adjust based on runner specs
        specs = RUNNER_SPECS.get(runner_type, RUNNER_SPECS["ubuntu-latest"])
        if specs["ram_gb"] >= 16:
            base_usage = base_usage * 0.7  # More memory available, lower utilization

        return min(90.0, max(10.0, base_usage))


class MatrixOptimizationAnalyzer:
    """Analyzes matrix jobs for optimization opportunities."""

    def __init__(self):
        self.matrix_patterns = {}

    def analyze_matrix_workflows(
        self, workflows_data: List[WorkflowSummary]
    ) -> List[MatrixJobAnalysis]:
        """Analyze workflows for matrix optimization opportunities."""
        analyses = []

        for workflow in workflows_data:
            if self._is_likely_matrix_workflow(workflow):
                analysis = self._analyze_matrix_workflow(workflow)
                analyses.append(analysis)

        return analyses

    def _is_likely_matrix_workflow(self, workflow: WorkflowSummary) -> bool:
        """Determine if workflow likely uses matrix strategy."""
        # Heuristics to identify matrix workflows
        indicators = [
            "test" in workflow.workflow_name.lower(),
            "ci" in workflow.workflow_name.lower(),
            "matrix" in workflow.workflow_name.lower(),
            workflow.total_runs > 20,  # Matrix jobs tend to have many runs
            workflow.failure_rate > 0.1,  # Matrix jobs may have higher failure rates
        ]
        return sum(indicators) >= 2

    def _analyze_matrix_workflow(self, workflow: WorkflowSummary) -> MatrixJobAnalysis:
        """Analyze a specific matrix workflow."""
        # Estimate matrix dimensions based on patterns
        estimated_dimensions = self._estimate_matrix_dimensions(workflow)

        # Calculate failure patterns
        early_failure_potential = (
            workflow.failure_rate > 0.3 and workflow.avg_duration_seconds > 300
        )

        # Identify pre-matrix candidates
        pre_matrix_candidates = self._identify_pre_matrix_candidates(workflow)

        # Calculate matrix necessity score
        necessity_score = self._calculate_matrix_necessity(workflow)

        # Generate optimization recommendations
        recommendations = self._generate_matrix_recommendations(
            workflow, early_failure_potential, necessity_score
        )

        return MatrixJobAnalysis(
            workflow_name=workflow.workflow_name,
            matrix_dimensions=estimated_dimensions,
            total_matrix_jobs=workflow.total_runs,
            failed_matrix_jobs=workflow.failure_count,
            avg_job_duration=workflow.avg_duration_seconds,
            early_failure_potential=early_failure_potential,
            pre_matrix_candidates=pre_matrix_candidates,
            matrix_necessity_score=necessity_score,
            optimization_recommendations=recommendations,
        )

    def _estimate_matrix_dimensions(self, workflow: WorkflowSummary) -> List[str]:
        """Estimate matrix dimensions based on workflow patterns."""
        dimensions = []

        # Common matrix dimensions
        if "test" in workflow.workflow_name.lower():
            dimensions.extend(["python-version", "os"])
        if "node" in workflow.workflow_name.lower():
            dimensions.extend(["node-version"])
        if "ci" in workflow.workflow_name.lower():
            dimensions.extend(["os", "version"])

        return dimensions or ["os"]

    def _identify_pre_matrix_candidates(self, workflow: WorkflowSummary) -> List[str]:
        """Identify steps that could be moved before matrix execution."""
        candidates = []

        if "test" in workflow.workflow_name.lower():
            candidates.extend(
                [
                    "lint-and-format",
                    "security-scan",
                    "dependency-check",
                    "documentation-build",
                ]
            )

        if "ci" in workflow.workflow_name.lower():
            candidates.extend(
                ["code-quality-check", "vulnerability-scan", "license-check"]
            )

        return candidates

    def _calculate_matrix_necessity(self, workflow: WorkflowSummary) -> float:
        """Calculate how necessary the matrix strategy is (0-1 score)."""
        score = 0.5  # Base score

        # Increase score for workflows that likely need matrix
        if workflow.total_runs > 50:
            score += 0.2
        if "test" in workflow.workflow_name.lower():
            score += 0.2
        if workflow.failure_rate < 0.1:  # Stable workflows benefit from matrix
            score += 0.1

        return min(1.0, score)

    def _generate_matrix_recommendations(
        self, workflow: WorkflowSummary, early_failure: bool, necessity: float
    ) -> List[str]:
        """Generate optimization recommendations for matrix workflows."""
        recommendations = []

        if early_failure:
            recommendations.append(
                "Implement fail-fast strategy to stop matrix jobs early on critical failures"
            )
            recommendations.append(
                "Add pre-matrix validation steps (linting, basic tests) before matrix execution"
            )

        if necessity < 0.7:
            recommendations.append(
                "Consider reducing matrix dimensions - some tests may not need full matrix coverage"
            )

        if workflow.failure_rate > 0.2:
            recommendations.append(
                "Investigate and fix common failure patterns to reduce wasted matrix job costs"
            )

        if workflow.avg_duration_seconds > 600:
            recommendations.append(
                "Consider splitting long-running matrix jobs into parallel stages"
            )

        recommendations.append(
            "Review if all matrix combinations are necessary for your use case"
        )

        return recommendations


class SelfHostedRunnerAdvisor:
    """Provides recommendations for setting up self-hosted runners."""

    def __init__(self, repository_url: Optional[str] = None):
        """Initialize the advisor with optional repository URL."""
        self.repository_url = repository_url
        self.runner_version = get_latest_github_runner_version()

        # Server configurations based on actual usage patterns
        self.server_configs = {
            "vps_ubuntu": {
                "min_specs": {"cores": 2, "ram_gb": 4, "storage_gb": 20},
                "recommended_specs": {"cores": 4, "ram_gb": 8, "storage_gb": 50},
                "optimal_specs": {"cores": 8, "ram_gb": 16, "storage_gb": 100},
            },
            "local_fedora": {
                "min_specs": {"cores": 4, "ram_gb": 8, "storage_gb": 50},
                "recommended_specs": {"cores": 6, "ram_gb": 16, "storage_gb": 100},
                "optimal_specs": {"cores": 8, "ram_gb": 32, "storage_gb": 200},
            },
        }

    def generate_setup_recommendations(
        self, current_cost: float, usage_patterns: List[RunnerResourceUsage]
    ) -> List[SelfHostedRunnerRecommendation]:
        """Generate self-hosted runner setup recommendations."""
        recommendations = []

        # VPS Ubuntu recommendation
        vps_rec = self._generate_vps_recommendation(current_cost, usage_patterns)
        recommendations.append(vps_rec)

        # Local Fedora recommendation
        local_rec = self._generate_local_recommendation(current_cost, usage_patterns)
        recommendations.append(local_rec)

        return recommendations

    def _generate_vps_recommendation(
        self, current_cost: float, usage_patterns: List[RunnerResourceUsage]
    ) -> SelfHostedRunnerRecommendation:
        """Generate VPS Ubuntu runner recommendation."""
        total_usage = sum(pattern.total_runs for pattern in usage_patterns)

        # Estimate VPS costs (based on current market rates)
        vps_monthly_cost = 15.0  # EUR for mid-range VPS

        # Get dynamic repository URL or use placeholder
        repo_url = self.repository_url or "https://github.com/YOUR_ORG/YOUR_REPO"

        setup_commands = [
            "# VPS Ubuntu Self-Hosted Runner Setup",
            "sudo apt update && sudo apt upgrade -y",
            "sudo apt install -y curl wget git build-essential",
            "",
            "# Create runner user",
            "sudo useradd -m -s /bin/bash github-runner",
            "sudo usermod -aG sudo github-runner",
            "",
            "# Download GitHub Actions runner",
            "su - github-runner",
            "mkdir actions-runner && cd actions-runner",
            f"curl -o actions-runner-linux-x64-{self.runner_version}.tar.gz -L https://github.com/actions/runner/releases/download/v{self.runner_version}/actions-runner-linux-x64-{self.runner_version}.tar.gz",
            f"tar xzf ./actions-runner-linux-x64-{self.runner_version}.tar.gz",
            "",
            "# Configure runner (replace TOKEN with your actual token)",
            f"./config.sh --url {repo_url} --token YOUR_TOKEN",
            "",
            "# Install as service",
            "sudo ./svc.sh install",
            "sudo ./svc.sh start",
            "",
            "# Set up automatic updates",
            "echo '0 2 * * 0 /home/github-runner/actions-runner/update.sh' | sudo crontab -u github-runner -",
        ]

        security_considerations = [
            "Configure firewall to allow only necessary ports (22 for SSH)",
            "Set up automatic security updates",
            "Use SSH key authentication, disable password auth",
            "Monitor runner logs for suspicious activity",
            "Regularly update the runner software",
            "Consider using runner isolation (containers/VMs)",
            "Implement log rotation and monitoring",
        ]

        estimated_savings = max(0, current_cost - vps_monthly_cost)

        return SelfHostedRunnerRecommendation(
            server_type="vps",
            os_type="ubuntu",
            recommended_specs=self.server_configs["vps_ubuntu"]["recommended_specs"],
            cost_comparison={
                "current_github_hosted": current_cost,
                "vps_monthly": vps_monthly_cost,
                "break_even_runs": (
                    int(vps_monthly_cost / 0.008) if current_cost > 0 else 0
                ),
            },
            setup_commands=setup_commands,
            security_considerations=security_considerations,
            estimated_monthly_savings=estimated_savings,
        )

    def _generate_local_recommendation(
        self, current_cost: float, usage_patterns: List[RunnerResourceUsage]
    ) -> SelfHostedRunnerRecommendation:
        """Generate local Fedora runner recommendation."""

        # Get dynamic repository URL or use placeholder
        repo_url = self.repository_url or "https://github.com/YOUR_ORG/YOUR_REPO"

        setup_commands = [
            "# Local Fedora Self-Hosted Runner Setup",
            "sudo dnf update -y",
            "sudo dnf install -y curl wget git gcc-c++ make",
            "",
            "# Create runner user",
            "sudo useradd -m -s /bin/bash github-runner",
            "sudo usermod -aG wheel github-runner",
            "",
            "# Download GitHub Actions runner",
            "su - github-runner",
            "mkdir actions-runner && cd actions-runner",
            f"curl -o actions-runner-linux-x64-{self.runner_version}.tar.gz -L https://github.com/actions/runner/releases/download/v{self.runner_version}/actions-runner-linux-x64-{self.runner_version}.tar.gz",
            f"tar xzf ./actions-runner-linux-x64-{self.runner_version}.tar.gz",
            "",
            "# Configure runner (replace TOKEN with your actual token)",
            f"./config.sh --url {repo_url} --token YOUR_TOKEN",
            "",
            "# Install as systemd service",
            "sudo ./svc.sh install",
            "sudo systemctl enable actions.runner.*.service",
            "sudo systemctl start actions.runner.*.service",
            "",
            "# Set up wake-on-LAN (optional for power management)",
            "sudo dnf install -y ethtool",
            "sudo ethtool -s eth0 wol g",
        ]

        security_considerations = [
            "Configure SELinux for additional security",
            "Set up local firewall rules",
            "Use separate user account for runner",
            "Monitor system resources and logs",
            "Consider running jobs in containers for isolation",
            "Implement proper backup strategy",
            "Set up monitoring and alerting",
        ]

        # Local machine has only electricity costs (estimated)
        estimated_monthly_cost = 10.0  # Rough electricity estimate
        estimated_savings = max(0, current_cost - estimated_monthly_cost)

        return SelfHostedRunnerRecommendation(
            server_type="local",
            os_type="fedora",
            recommended_specs=self.server_configs["local_fedora"]["recommended_specs"],
            cost_comparison={
                "current_github_hosted": current_cost,
                "local_monthly": estimated_monthly_cost,
                "break_even_runs": (
                    int(estimated_monthly_cost / 0.008) if current_cost > 0 else 0
                ),
            },
            setup_commands=setup_commands,
            security_considerations=security_considerations,
            estimated_monthly_savings=estimated_savings,
        )


class OutputFormatter:
    """Formats analysis results for different output types using pandas native serialization."""

    @staticmethod
    def format_json(
        analysis: RepositoryAnalysis, include_raw_data: bool = False
    ) -> str:
        """Format analysis as JSON using pandas native serialization."""
        # Convert analysis to DataFrame for better serialization
        workflows_df = pd.DataFrame([w.dict() for w in analysis.workflows])

        # Create analysis dict
        data = analysis.dict()
        if not include_raw_data:
            data.pop("raw_data", None)

        # Use pandas to_json for optimized serialization
        if not workflows_df.empty:
            data["workflows"] = workflows_df.to_dict("records")

        return json.dumps(data, indent=2, default=str)

    @staticmethod
    def format_yaml(
        analysis: RepositoryAnalysis, include_raw_data: bool = False
    ) -> str:
        """Format analysis as YAML using pandas native serialization."""
        # Convert analysis to DataFrame for better serialization
        workflows_df = pd.DataFrame([w.dict() for w in analysis.workflows])

        # Create analysis dict
        data = analysis.dict()
        if not include_raw_data:
            data.pop("raw_data", None)

        # Use pandas to_dict for optimized serialization
        if not workflows_df.empty:
            data["workflows"] = workflows_df.to_dict("records")

        return yaml.dump(
            data, default_flow_style=False, sort_keys=False, allow_unicode=True
        )

    @staticmethod
    def format_csv(analysis: RepositoryAnalysis) -> str:
        """Format analysis as CSV using pandas native to_csv functionality."""
        if not analysis.workflows:
            return "No workflow data available"

        # Convert workflows to DataFrame - this is the natural pandas way
        workflows_df = pd.DataFrame([w.dict() for w in analysis.workflows])

        # Create a header DataFrame with summary information
        summary_info = {
            "workflow_name": ["REPOSITORY_SUMMARY"],
            "repository": [analysis.repository],
            "analysis_date": [analysis.analysis_date],
            "total_workflows": [analysis.total_workflows],
            "total_runs": [analysis.total_runs],
            "total_cost": [f"${analysis.total_cost:.4f}"],
            "total_waste": [f"${analysis.total_waste:.4f}"],
            "waste_percentage": [f"{analysis.waste_percentage:.1f}%"],
            "monthly_projection": [f"${analysis.monthly_projection:.2f}"],
            "annual_projection": [f"${analysis.annual_projection:.2f}"],
        }

        # Ensure all columns exist in summary with proper alignment
        for col in workflows_df.columns:
            if col not in summary_info:
                summary_info[col] = [""]

        summary_df = pd.DataFrame(summary_info)

        # Use pandas concat and to_csv - the native pandas approach
        combined_df = pd.concat([summary_df, workflows_df], ignore_index=True)
        return combined_df.to_csv(index=False) @ staticmethod

    def format_table(analysis: RepositoryAnalysis, detailed: bool = False) -> str:
        """Format analysis as human-readable table."""
        output = []

        # Header
        output.append("=" * 80)
        output.append("ðŸ—ï¸  GITHUB ACTIONS COST ANALYSIS REPORT")
        output.append("=" * 80)
        output.append("")

        # Repository overview
        output.append("ðŸ“Š Repository Overview:")
        output.append(f"   â€¢ Repository: {analysis.repository}")
        output.append(f"   â€¢ Analysis Date: {analysis.analysis_date}")
        output.append(f"   â€¢ Total Workflows: {analysis.total_workflows}")
        output.append(f"   â€¢ Total Runs Analyzed: {analysis.total_runs}")
        output.append(f"   â€¢ Total Cost: ${analysis.total_cost:.4f}")
        output.append(
            f"   â€¢ Total Waste: ${analysis.total_waste:.4f} ({analysis.waste_percentage:.1f}%)"
        )
        output.append("")

        # Projections
        output.append("ðŸ“ˆ Cost Projections:")
        output.append(f"   â€¢ Monthly: ${analysis.monthly_projection:.2f}")
        output.append(f"   â€¢ Annual: ${analysis.annual_projection:.2f}")
        output.append("")

        # Workflow drill-down table
        output.append("ðŸ” Workflow Analysis (Drill-down Table):")
        output.append("-" * 80)

        # Table header
        if detailed:
            header = f"{'Workflow':<25} {'Runs':<8} {'Success%':<9} {'Avg(s)':<8} {'Cost':<10} {'Waste':<10} {'Risk':<10}"
        else:
            header = f"{'Workflow':<30} {'Runs':<8} {'Success%':<9} {'Cost':<10} {'Risk':<10}"
        output.append(header)
        output.append("-" * len(header))

        # Sort workflows by cost impact
        sorted_workflows = sorted(
            analysis.workflows, key=lambda w: w.total_cost_usd, reverse=True
        )

        for workflow in sorted_workflows:
            if detailed:
                row = (
                    f"{workflow.workflow_name[:24]:<25} "
                    f"{workflow.total_runs:<8} "
                    f"{workflow.success_rate*100:<8.1f}% "
                    f"{workflow.avg_duration_seconds:<8.1f} "
                    f"${workflow.total_cost_usd:<9.4f} "
                    f"${workflow.waste_from_failures:<9.4f} "
                    f"{workflow.risk_level:<10}"
                )
            else:
                row = (
                    f"{workflow.workflow_name[:29]:<30} "
                    f"{workflow.total_runs:<8} "
                    f"{workflow.success_rate*100:<8.1f}% "
                    f"${workflow.total_cost_usd:<9.4f} "
                    f"{workflow.risk_level:<10}"
                )
            output.append(row)

        output.append("")

        # High priority issues
        if analysis.high_priority_issues:
            output.append("ðŸš¨ High Priority Issues:")
            output.append("-" * 50)
            for i, issue in enumerate(analysis.high_priority_issues, 1):
                output.append(f"{i}. {issue['workflow_name']}")
                output.append(f"   â€¢ Risk Level: {issue['risk_level']}")
                output.append(f"   â€¢ Failure Rate: {issue['failure_rate']*100:.1f}%")
                output.append(f"   â€¢ Monthly Waste: ${issue['monthly_waste']:.2f}")
                output.append("")

        # Recommendations
        output.append("ðŸ“‹ Optimization Recommendations:")
        output.append("-" * 50)
        for i, rec in enumerate(analysis.recommendations, 1):
            output.append(f"{i}. {rec['priority']} - {rec['action']}")
            output.append(f"   ðŸ’° Impact: {rec['impact']}")
            output.append(f"   ðŸ“ Details: {rec['details']}")
            if rec["workflows"]:
                output.append(f"   ðŸŽ¯ Workflows: {', '.join(rec['workflows'])}")
            output.append("")

        # Footer
        output.append("=" * 80)
        output.append("ðŸ“Š Analysis completed successfully")
        output.append(
            f"Report generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        )
        output.append("=" * 80)

        return "\n".join(output)

    @staticmethod
    def format_csv(analysis: RepositoryAnalysis) -> str:
        """Format workflow summaries as CSV."""
        output = io.StringIO()
        writer = csv.writer(output)

        # Header
        writer.writerow(
            [
                "Workflow Name",
                "Workflow ID",
                "Total Runs",
                "Success Rate",
                "Failure Rate",
                "Avg Duration (s)",
                "Total Cost",
                "Waste Cost",
                "Risk Level",
                "Cost Impact",
                "Monthly Projection",
                "Annual Projection",
            ]
        )

        # Data rows
        for workflow in analysis.workflows:
            writer.writerow(
                [
                    workflow.workflow_name,
                    workflow.workflow_id,
                    workflow.total_runs,
                    f"{workflow.success_rate:.3f}",
                    f"{workflow.failure_rate:.3f}",
                    f"{workflow.avg_duration_seconds:.1f}",
                    f"{workflow.total_cost_usd:.4f}",
                    f"{workflow.waste_from_failures:.4f}",
                    workflow.risk_level,
                    workflow.cost_impact,
                    f"{workflow.monthly_projection:.2f}",
                    f"{workflow.annual_projection:.2f}",
                ]
            )

        return output.getvalue()

    @staticmethod
    def format_comprehensive_analysis(analysis: RepositoryAnalysis) -> str:
        """Format comprehensive analysis with insights and recommendations similar to terminal output."""
        # Create DataFrame for analysis
        workflows_df = pd.DataFrame([w.dict() for w in analysis.workflows])

        output = []

        # Header
        output.append("=" * 70)
        output.append(f"GITHUB ACTIONS ANALYSIS: {analysis.repository}")
        output.append("=" * 70)
        output.append("")

        # Cost Summary
        output.append("ðŸ’° COST SUMMARY:")
        output.append(f"Total Cost: ${analysis.total_cost:.2f}")
        output.append(
            f"Waste Cost: ${analysis.total_waste:.2f} ({analysis.waste_percentage:.1f}%)"
        )
        output.append(f"Efficiency: {100 - analysis.waste_percentage:.1f}%")
        output.append("")

        # Risk Breakdown
        output.append("âš ï¸  RISK BREAKDOWN:")
        if not workflows_df.empty:
            risk_counts = workflows_df["risk_level"].value_counts()
            for risk, count in risk_counts.items():
                workflows = workflows_df[workflows_df["risk_level"] == risk][
                    "workflow_name"
                ].tolist()
                output.append(f"{risk}: {count} workflows - {workflows}")
        output.append("")

        # Performance Metrics
        output.append("ðŸ“Š PERFORMANCE METRICS:")
        if not workflows_df.empty:
            df_sorted = workflows_df.sort_values(
                "avg_duration_seconds", ascending=False
            )
            output.append("Longest Running Workflows:")
            for _, row in df_sorted.head(3).iterrows():
                output.append(
                    f"  â€¢ {row['workflow_name']}: {row['avg_duration_seconds']:.1f}s (Success: {row['success_rate']:.1%})"
                )
        output.append("")

        # Failure Analysis
        output.append("ðŸ”´ FAILURE ANALYSIS:")
        if not workflows_df.empty:
            df_failures = workflows_df[workflows_df["failure_rate"] > 0].sort_values(
                "failure_rate", ascending=False
            )
            for _, row in df_failures.iterrows():
                output.append(
                    f"  â€¢ {row['workflow_name']}: {row['failure_rate']:.1%} failure rate (Cost: ${row['total_cost_usd']:.2f})"
                )
        output.append("")

        # High Priority Issues
        if analysis.high_priority_issues:
            output.append("ðŸš¨ HIGH PRIORITY ISSUES:")
            for issue in analysis.high_priority_issues:
                output.append(f"  â€¢ {issue['workflow_name']} ({issue['risk_level']})")
                output.append(f"    - Failure Rate: {issue['failure_rate']:.1%}")
                output.append(f"    - Waste Cost: ${issue['waste_cost']:.2f}")
                output.append(f"    - Monthly Waste: ${issue['monthly_waste']:.2f}")
            output.append("")

        # Recommendations
        if analysis.recommendations:
            output.append("ðŸ’¡ OPTIMIZATION RECOMMENDATIONS:")
            for i, rec in enumerate(analysis.recommendations, 1):
                output.append(f"{i}. {rec['priority']} {rec['action']}")
                output.append(f"   Impact: {rec['impact']}")
                output.append(f"   Details: {rec['details']}")
                if rec.get("workflows"):
                    workflow_list = ", ".join(
                        [
                            w[:20] + "..." if len(w) > 20 else w
                            for w in rec["workflows"][:3]
                        ]
                    )
                    output.append(f"   Workflows: {workflow_list}")
            output.append("")

        # Runner Efficiency Analysis
        if hasattr(analysis, "runner_efficiency") and analysis.runner_efficiency:
            output.append("âš¡ RUNNER EFFICIENCY ANALYSIS:")
            for efficiency in analysis.runner_efficiency[:5]:  # Show top 5
                output.append(f"  â€¢ {efficiency.runner_type}:")
                output.append(
                    f"    - CPU Usage: {efficiency.estimated_cpu_usage_percent:.1f}%"
                )
                output.append(
                    f"    - Memory Usage: {efficiency.estimated_memory_usage_percent:.1f}%"
                )
                output.append(
                    f"    - Utilization Efficiency: {efficiency.utilization_efficiency:.1f}%"
                )
                output.append(
                    f"    - Recommended Runner: {efficiency.recommended_runner or 'Current is optimal'}"
                )
                output.append(
                    f"    - Potential Savings: ${efficiency.potential_savings_usd:.2f}"
                )
            output.append("")

        # Matrix Optimization Analysis
        if hasattr(analysis, "matrix_optimization") and analysis.matrix_optimization:
            output.append("ðŸ”„ MATRIX OPTIMIZATION ANALYSIS:")
            for matrix in analysis.matrix_optimization[:3]:  # Show top 3
                failure_rate = (
                    matrix.failed_matrix_jobs / max(1, matrix.total_matrix_jobs)
                ) * 100
                output.append(f"  â€¢ {matrix.workflow_name}:")
                output.append(f"    - Matrix Jobs: {matrix.total_matrix_jobs}")
                output.append(f"    - Failed Jobs: {matrix.failed_matrix_jobs}")
                output.append(f"    - Failure Rate: {failure_rate:.1f}%")
                output.append(
                    f"    - Early Failure Potential: {'Yes' if matrix.early_failure_potential else 'No'}"
                )
                output.append(
                    f"    - Necessity Score: {matrix.matrix_necessity_score:.1f}/1.0"
                )
                if matrix.optimization_recommendations:
                    output.append(
                        f"    - Optimizations: {', '.join(matrix.optimization_recommendations[:2])}"
                    )
            output.append("")

        # Self-Hosted Runner Recommendations
        if (
            hasattr(analysis, "self_hosted_recommendations")
            and analysis.self_hosted_recommendations
        ):
            output.append("ðŸ  SELF-HOSTED RUNNER RECOMMENDATIONS:")
            # Show top 2
            for recommendation in analysis.self_hosted_recommendations[:2]:
                output.append(
                    f"  â€¢ {recommendation.server_type.title()} ({recommendation.os_type}):"
                )
                output.append(
                    f"    - Monthly Savings: ${recommendation.estimated_monthly_savings:.2f}"
                )
                if recommendation.recommended_specs:
                    specs = recommendation.recommended_specs
                    output.append(
                        f"    - Recommended Specs: {specs.get('cores', 'N/A')} cores, {specs.get('ram_gb', 'N/A')}GB RAM"
                    )
                if recommendation.cost_comparison:
                    github_cost = recommendation.cost_comparison.get("github_hosted", 0)
                    self_cost = recommendation.cost_comparison.get("self_hosted", 0)
                    output.append(
                        f"    - Cost Comparison: GitHub ${github_cost:.2f} vs Self-hosted ${self_cost:.2f}"
                    )
                if recommendation.security_considerations:
                    output.append(
                        f"    - Security: {', '.join(recommendation.security_considerations[:2])}"
                    )
            output.append("")

        # Projections
        output.append("ðŸ“ˆ PROJECTIONS:")
        output.append(f"Monthly Projection: ${analysis.monthly_projection:.2f}")
        output.append(f"Annual Projection: ${analysis.annual_projection:.2f}")
        output.append("")

        # Expected Impact of Fixes
        if analysis.total_waste > 0:
            potential_savings = (
                analysis.total_waste * 12
            )  # Annual savings if waste eliminated
            target_efficiency = 95.0
            output.append("ðŸŽ¯ EXPECTED IMPACT OF FIXES:")
            output.append(f"Potential Annual Savings: ${potential_savings:.2f}")
            output.append(f"Target Efficiency: {target_efficiency}%")
            output.append(f"Current Efficiency: {100 - analysis.waste_percentage:.1f}%")

            if analysis.annual_projection > 0:
                optimized_cost = (
                    analysis.annual_projection * (100 - target_efficiency) / 100
                )
                output.append(f"Optimized Annual Cost: ${optimized_cost:.2f}")

        output.append("")
        output.append("=" * 70)

        return "\n".join(output)


def get_repository_url(repo_input: str = None) -> str:
    """
    Get repository URL from input or auto-detect from current directory.

    Args:
        repo_input: Optional repository input (full URL, owner/repo, or user/repo)

    Returns:
        Full GitHub repository URL
    """
    if repo_input:
        # If it's already a full URL, return as-is
        if repo_input.startswith(("http://", "https://")):
            return repo_input

        # If it's in owner/repo format, construct the URL
        if "/" in repo_input and len(repo_input.split("/")) == 2:
            return f"https://github.com/{repo_input}"

        # If it's a directory path, try to extract repository URL
        if os.path.isdir(repo_input):
            extracted_url = extract_repository_url_from_directory(repo_input)
            if extracted_url:
                return extracted_url
            raise ValueError(
                f"Directory {repo_input} does not contain a valid git repository"
            )

        # Invalid format
        raise ValueError(
            f"Invalid repository format: {repo_input}. Use 'owner/repo', full URL, or directory path"
        )

    # Auto-detect from current directory
    extracted_url = extract_repository_url_from_directory()
    if extracted_url:
        return extracted_url

    raise ValueError(
        "Could not auto-detect repository URL. "
        "Please provide repository URL or run from a git repository directory"
    )


def merge_configs(config_files: List[str]) -> Dict[str, Any]:
    """
    Merge multiple configuration files (supports YAML, JSON, TOML).

    Args:
        config_files: List of paths to configuration files

    Returns:
        Merged configuration dictionary
    """
    merged_config = {
        "metadata": {
            "generated_on": datetime.now().isoformat(),
            "description": "Merged GitHub Actions runner configurations",
            "source_files": config_files,
        },
        "runners": [],
    }

    for config_file in config_files:
        try:
            config = load_config_file(config_file)

            if not config or "runners" not in config:
                print(f"âš ï¸  Warning: No runners found in {config_file}")
                continue

            # Add runners from this config
            for runner in config["runners"]:
                # Add source file info to each runner
                runner["source_file"] = config_file
                merged_config["runners"].append(runner)

        except Exception as e:
            print(f"âŒ Error loading {config_file}: {e}")
            continue

    return merged_config


# Keep the old function for backward compatibility
def merge_yaml_configs(yaml_files: List[str]) -> Dict[str, Any]:
    """
    Legacy function for merging YAML configs. Use merge_configs() instead.

    Args:
        yaml_files: List of paths to YAML files

    Returns:
        Merged configuration dictionary
    """
    return merge_configs(yaml_files)


def _get_latest_runner_version():
    """Get the latest GitHub Actions runner version from GitHub API."""
    try:
        response = requests.get(
            "https://api.github.com/repos/actions/runner/releases/latest", timeout=10
        )
        if response.status_code == 200:
            data = response.json()
            version = data.get("tag_name", "").lstrip("v")
            return version if version else "2.311.0"
    except Exception:
        pass
    return "2.311.0"  # Fallback version


def _verify_runner_asset(version, arch, platform="linux"):
    """Verify that the runner asset exists for the given version/arch."""
    try:
        if platform == "linux":
            asset_name = f"actions-runner-linux-{arch}-{version}.tar.gz"
        elif platform == "windows":
            asset_name = f"actions-runner-win-{arch}-{version}.zip"
        elif platform == "darwin":
            asset_name = f"actions-runner-osx-{arch}-{version}.tar.gz"
        else:
            return False

        response = requests.get(
            f"https://api.github.com/repos/actions/runner/releases/tags/v{version}",
            timeout=10,
        )
        if response.status_code == 200:
            data = response.json()
            assets = data.get("assets", [])
            return any(asset["name"] == asset_name for asset in assets)
    except Exception:
        pass
    return False


class ScriptCommentGenerator:
    """Generate formatted comments for different shell types."""

    # Common comment templates
    HEADER_TEMPLATE = """GitHub Actions Self-Hosted Runner Setup Script for {runner_name}
{idempotent_desc}
Generated on: {generation_time}

Runner Specifications:
- Name: {runner_name}
- Cores: {cores}
- RAM: {ram_gb} GB
- Storage: {storage_gb} GB
- Architecture: {arch}
- OS Family: {os_family}
- Package Manager: {package_manager}"""

    SECURITY_NOTICE = (
        """Security-hardened installation following GitHub best practices"""
    )

    VALIDATION_NOTICE = """Machine specification validation and configuration"""

    LOGGING_SETUP_NOTICE = (
        """Logging functions for colored output and progress tracking"""
    )

    FIREWALL_CONFIG_NOTICE = """Firewall configuration and security hardening"""

    USER_SETUP_NOTICE = """Setting up dedicated runner user and permissions"""

    RUNNER_INSTALL_NOTICE = """GitHub Actions runner installation and configuration"""

    SERVICE_SETUP_NOTICE = """Service configuration and startup management"""

    CLEANUP_NOTICE = """Cleanup and final security hardening"""

    @staticmethod
    def format_comment(text: str, shell_type: str, comment_prefix: str = None) -> str:
        """Format a comment for the specified shell type."""
        if comment_prefix is None:
            comment_prefixes = {
                "bash": "# ",
                "sh": "# ",
                "powershell": "# ",
                "cmd": "REM ",
                "zsh": "# ",
            }
            comment_prefix = comment_prefixes.get(shell_type, "# ")

        lines = text.strip().split("\n")
        return "\n".join(comment_prefix + line for line in lines)

    @classmethod
    def generate_header(
        cls,
        runner_name: str,
        cores: int,
        ram_gb: float,
        storage_gb: float,
        arch: str,
        os_family: str,
        package_manager: str,
        shell_type: str,
    ) -> str:
        """Generate formatted header comment."""
        header_text = cls.HEADER_TEMPLATE.format(
            runner_name=runner_name,
            idempotent_desc=cls.SECURITY_NOTICE,
            generation_time=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            cores=cores,
            ram_gb=ram_gb,
            storage_gb=storage_gb,
            arch=arch,
            os_family=os_family,
            package_manager=package_manager,
        )
        return cls.format_comment(header_text, shell_type)

    @classmethod
    def generate_section_comment(cls, section_name: str, shell_type: str) -> str:
        """Generate a section comment."""
        section_templates = {
            "validation": cls.VALIDATION_NOTICE,
            "logging": cls.LOGGING_SETUP_NOTICE,
            "firewall": cls.FIREWALL_CONFIG_NOTICE,
            "user_setup": cls.USER_SETUP_NOTICE,
            "runner_install": cls.RUNNER_INSTALL_NOTICE,
            "service_setup": cls.SERVICE_SETUP_NOTICE,
            "cleanup": cls.CLEANUP_NOTICE,
        }

        text = section_templates.get(
            section_name, section_name.replace("_", " ").title()
        )
        return cls.format_comment(text, shell_type)


def _generate_firewall_warnings_comment(firewall_warnings: List[str]) -> str:
    """Generate bash comment block for firewall warnings."""
    if not firewall_warnings:
        return ""

    # Check if NetworkManager warning exists
    has_networkmanager_warning = any(
        "NetworkManager" in warning for warning in firewall_warnings
    )

    comment_lines = ["# FIREWALL CONFIGURATION WARNINGS"]
    comment_lines.append("# " + "=" * 40)

    for warning in firewall_warnings:
        comment_lines.append(f"# âš ï¸  {warning}")

    if has_networkmanager_warning:
        comment_lines.extend(
            [
                "#",
                "# NetworkManager Coordination:",
                "# - This script will configure NetworkManager to work with your firewall",
                "# - firewalld backend will be enabled for NetworkManager",
                "# - Connection zones will be set appropriately",
                "# - Manual firewall rules should use firewall-cmd for persistence",
            ]
        )

    comment_lines.append("# " + "=" * 40)
    comment_lines.append("")

    return "\n".join(comment_lines)


def _generate_linux_setup_script(
    runner, machine_spec: MachineSpec, repo_url=None, runner_token=None
):
    """
    Generate comprehensive, idempotent Linux setup script with security hardening.

    Args:
        runner: Runner configuration from YAML
        machine_spec: Complete machine specification with detected capabilities
        repo_url: Repository URL for runner registration
        runner_token: GitHub runner registration token

    Returns:
        str: Complete setup script content
    """
    runner_name = runner["name"]
    arch = machine_spec.arch

    # Validate machine spec data
    if not machine_spec.package_manager:
        raise ValueError(
            "Package manager not detected in machine specification. Run --generate-machine-description first."
        )

    if not machine_spec.os_family:
        raise ValueError(
            "OS family not detected in machine specification. Run --generate-machine-description first."
        )

    # Get latest runner version
    latest_version = _get_latest_runner_version()

    # Verify the asset exists, fallback if not
    if not _verify_runner_asset(latest_version, arch, "linux"):
        latest_version = "2.311.0"  # Known good fallback

    # Validate firewall configuration from machine spec
    primary_firewall = machine_spec.primary_firewall
    active_firewalls = machine_spec.active_firewalls
    firewall_warnings = machine_spec.firewall_warnings

    # Extract commonly used values from machine spec
    os_family = machine_spec.os_family
    package_manager = machine_spec.package_manager
    arch = machine_spec.arch

    if machine_spec.firewall_conflicts:
        print(
            "âš ï¸  WARNING: Firewall conflicts detected. Manual intervention may be required."
        )
        for warning in firewall_warnings:
            print(f"   - {warning}")

    # Determine firewall configuration to use in script
    if primary_firewall:
        if hasattr(primary_firewall, "value"):
            firewall_type = primary_firewall.value
        else:
            firewall_type = str(primary_firewall)  # Handle string case
    elif active_firewalls:
        first_fw = active_firewalls[0]
        if hasattr(first_fw, "value"):
            firewall_type = first_fw.value
        else:
            firewall_type = str(first_fw)  # Handle string case
    else:
        firewall_type = "ufw"  # Default fallback
        print("âš ï¸  No active firewall detected, script will install ufw as default")

    print(f"ðŸ“‹ Using machine specification data:")
    print(
        f"   - OS Family: {os_family.value if hasattr(os_family, 'value') else os_family}"
    )
    print(
        f"   - Package Manager: {package_manager.value if hasattr(package_manager, 'value') else package_manager}"
    )
    print(f"   - Architecture: {arch.value if hasattr(arch, 'value') else arch}")
    print(f"   - Primary Firewall: {primary_firewall}")
    print(
        f"   - Active Firewalls: {[fw.value if hasattr(fw, 'value') else str(fw) for fw in active_firewalls]}"
    )
    print(f"   - Firewall Warnings: {len(firewall_warnings)} warnings")

    # Set default values if not provided
    if not repo_url:
        repo_url = "https://github.com/OWNER/REPO"
    if not runner_token:
        runner_token = "YOUR_RUNNER_TOKEN"

    # Extract owner/repo from URL
    repo_match = re.search(r"github\.com/([^/]+)/([^/]+)", repo_url)
    if repo_match:
        owner, repo = repo_match.groups()
        repo = repo.replace(".git", "")
    else:
        owner, repo = "OWNER", "REPO"

    # Generate header using shared comment system
    header_comment = ScriptCommentGenerator.generate_header(
        runner_name=runner_name,
        cores=runner["cores"],
        ram_gb=runner["ram_gb"],
        storage_gb=runner["storage_gb"],
        arch=arch.value if hasattr(arch, "value") else str(arch),
        os_family=os_family.value if hasattr(os_family, "value") else str(os_family),
        package_manager=(
            package_manager.value
            if hasattr(package_manager, "value")
            else str(package_manager)
        ),
        shell_type="bash",
    )

    script_content = f"""#!/bin/bash
{header_comment}

set -euo pipefail

# Configuration
RUNNER_VERSION="{latest_version}"
RUNNER_ARCH="{arch.value if hasattr(arch, 'value') else str(arch)}"
REPO_URL="{repo_url}"
RUNNER_TOKEN="{runner_token}"
RUNNER_NAME="{runner_name}"
RUNNER_USER="github-runner"
RUNNER_GROUP="github-runners"
RUNNER_HOME="/opt/actions-runner"
SERVICE_NAME="actions.runner.{owner}-{repo}.{runner_name}"

{_generate_firewall_warnings_comment(firewall_warnings)}

# Colors for output
RED='\\033[0;31m'
GREEN='\\033[0;32m'
YELLOW='\\033[1;33m'
BLUE='\\033[0;34m'
NC='\\033[0m' # No Color

{ScriptCommentGenerator.generate_section_comment('logging', 'bash')}
log_info() {{ echo -e "${{BLUE}}[INFO]${{NC}} $1"; }}
log_success() {{ echo -e "${{GREEN}}[SUCCESS]${{NC}} $1"; }}
log_warning() {{ echo -e "${{YELLOW}}[WARNING]${{NC}} $1"; }}
log_error() {{ echo -e "${{RED}}[ERROR]${{NC}} $1"; }}

log_info "ðŸ”’ Starting idempotent GitHub Runner setup for $RUNNER_NAME..."

# Check if running as root
if [[ $EUID -ne 0 ]]; then
   log_error "âŒ This script must be run as root (use sudo)"
   exit 1
fi

{ScriptCommentGenerator.generate_section_comment('validation', 'bash')}
validate_machine_spec() {{
    log_info "Validating machine specification data..."

    # Check current OS against machine spec
    if [ -f /etc/os-release ]; then
        . /etc/os-release
        DETECTED_OS_ID="$ID"

        # Validate OS family match
        case "$DETECTED_OS_ID" in
            ubuntu|debian)
                if [ "$OS_FAMILY" != "debian" ] && [ "$OS_FAMILY" != "ubuntu" ]; then
                    log_warning "OS family mismatch: expected $OS_FAMILY, detected $DETECTED_OS_ID"
                fi
                ;;
            fedora|centos|rhel)
                if [ "$OS_FAMILY" != "rhel" ] && [ "$OS_FAMILY" != "fedora" ]; then
                    log_warning "OS family mismatch: expected $OS_FAMILY, detected $DETECTED_OS_ID"
                fi
                ;;
        esac
    else
        log_error "Cannot detect current OS for validation"
        exit 1
    fi

    # Validate package manager is available
    case "$PKG_MANAGER" in
        "apt")
            if ! command -v apt-get >/dev/null 2>&1; then
                log_error "Package manager apt not found but specified in machine spec"
                exit 1
            fi
            PKG_UPDATE="apt update"
            PKG_INSTALL="apt install -y"
            ;;
        "dnf")
            if ! command -v dnf >/dev/null 2>&1; then
                log_error "Package manager dnf not found but specified in machine spec"
                exit 1
            fi
            PKG_UPDATE="dnf check-update || true"
            PKG_INSTALL="dnf install -y"
            ;;
        "yum")
            if ! command -v yum >/dev/null 2>&1; then
                log_error "Package manager yum not found but specified in machine spec"
                exit 1
            fi
            PKG_UPDATE="yum check-update || true"
            PKG_INSTALL="yum install -y"
            ;;
        *)
            log_error "Unsupported package manager in machine spec: $PKG_MANAGER"
            exit 1
            ;;
    esac

    # Validate firewall configuration
    case "$FIREWALL_TYPE" in
        "ufw")
            if ! command -v ufw >/dev/null 2>&1; then
                log_warning "UFW not found, will install as specified in machine spec"
            fi
            ;;
        "firewalld")
            if ! command -v firewall-cmd >/dev/null 2>&1; then
                log_warning "firewalld not found, will install as specified in machine spec"
            fi
            ;;
        "iptables")
            if ! command -v iptables >/dev/null 2>&1; then
                log_warning "iptables not found, will install as specified in machine spec"
            fi
            ;;
    esac

    log_info "âœ… Machine specification validation completed"
    log_info "   OS Family: $OS_FAMILY"
    log_info "   Package Manager: $PKG_MANAGER"
    log_info "   Firewall: $FIREWALL_TYPE"
}}

# Configuration from machine specification
OS_FAMILY="{os_family.value if hasattr(os_family, 'value') else str(os_family)}"
PKG_MANAGER="{package_manager.value if hasattr(package_manager, 'value') else str(package_manager)}"
FIREWALL_TYPE="{firewall_type}"

# Function to check if package is installed
is_package_installed() {{
    case "$PKG_MANAGER" in
        "apt")
            dpkg -l "$1" 2>/dev/null | grep -q "^ii"
            ;;
        "dnf"|"yum")
            rpm -q "$1" >/dev/null 2>&1
            ;;
        *)
            return 1
            ;;
    esac
}}

# Function to install package if not already installed
install_package() {{
    local package="$1"
    if is_package_installed "$package"; then
        log_info "Package $package already installed"
    else
        log_info "Installing package: $package"
        $PKG_INSTALL "$package"
        log_success "Package $package installed"
    fi
}}

# Function to check if group exists
group_exists() {{
    getent group "$1" >/dev/null 2>&1
}}

# Function to check if user exists
user_exists() {{
    id "$1" >/dev/null 2>&1
}}

# Function to check if user is in group
user_in_group() {{
    local user="$1"
    local group="$2"
    groups "$user" | grep -q "\\b$group\\b"
}}

# Function to create or verify group
create_or_verify_group() {{
    local group="$1"
    if group_exists "$group"; then
        log_info "Group $group already exists"
    else
        log_info "Creating group: $group"
        groupadd "$group"
        log_success "Group $group created"
    fi
}}

# Function to create or verify user
create_or_verify_user() {{
    local user="$1"
    local group="$2"
    local home_dir="$3"

    if user_exists "$user"; then
        log_info "User $user already exists"

        # Verify user is in correct primary group
        current_group=$(id -gn "$user")
        if [ "$current_group" != "$group" ]; then
            log_warning "User $user primary group is $current_group, changing to $group"
            usermod -g "$group" "$user"
        fi

        # Verify home directory
        current_home=$(getent passwd "$user" | cut -d: -f6)
        if [ "$current_home" != "$home_dir" ]; then
            log_warning "User $user home is $current_home, changing to $home_dir"
            usermod -d "$home_dir" "$user"
        fi

        # Verify shell
        current_shell=$(getent passwd "$user" | cut -d: -f7)
        if [ "$current_shell" != "/bin/bash" ]; then
            log_info "Changing shell for $user to bash"
            usermod -s /bin/bash "$user"
        fi
    else
        log_info "Creating user: $user"
        useradd -m -g "$group" -s /bin/bash -d "$home_dir" "$user"
        log_success "User $user created"
    fi

    # Ensure user is in docker group (if docker exists)
    if command -v docker >/dev/null 2>&1; then
        if ! user_in_group "$user" "docker"; then
            log_info "Adding $user to docker group"
            usermod -aG docker "$user"
        fi
    fi
}}

# Function to handle NetworkManager conflicts
configure_networkmanager_firewall_coordination() {{
    # Check if NetworkManager is active
    if systemctl is-active NetworkManager >/dev/null 2>&1; then
        log_warning "NetworkManager is active - configuring firewall coordination..."

        case "$FIREWALL_TYPE" in
            "firewalld")
                # Ensure NetworkManager uses firewalld backend
                if [ -f /etc/NetworkManager/NetworkManager.conf ]; then
                    if ! grep -q "firewall-backend=firewalld" /etc/NetworkManager/NetworkManager.conf; then
                        log_info "Configuring NetworkManager to use firewalld backend"
                        echo "" >> /etc/NetworkManager/NetworkManager.conf
                        echo "[main]" >> /etc/NetworkManager/NetworkManager.conf
                        echo "firewall-backend=firewalld" >> /etc/NetworkManager/NetworkManager.conf

                        # Restart NetworkManager to apply changes
                        log_info "Restarting NetworkManager to apply firewall backend changes"
                        systemctl restart NetworkManager || log_warning "Failed to restart NetworkManager"
                    fi
                fi

                # Set connection zones appropriately
                log_info "Setting NetworkManager connection zones..."
                for conn in $(nmcli -t -f NAME connection show --active 2>/dev/null); do
                    current_zone=$(nmcli -t -f connection.zone connection show "$conn" 2>/dev/null | cut -d: -f2)
                    if [ -z "$current_zone" ] || [ "$current_zone" = "--" ]; then
                        log_info "Setting zone 'public' for connection: $conn"
                        nmcli connection modify "$conn" connection.zone public 2>/dev/null || true
                    fi
                done
                ;;

            "ufw")
                log_warning "UFW and NetworkManager coordination requires manual configuration"
                log_info "Consider switching to firewalld for better NetworkManager integration"
                ;;

            "iptables")
                log_warning "Direct iptables rules may conflict with NetworkManager"
                log_info "NetworkManager may reset iptables rules - consider using firewalld"
                ;;
        esac
    else
        log_info "NetworkManager not active - no coordination needed"
    fi
}}

# Function to configure firewall
configure_firewall() {{
    case "$FIREWALL_TYPE" in
        "ufw")
            log_info "Configuring UFW firewall..."

            # Check if UFW is active
            if ! ufw status | grep -q "Status: active"; then
                # Configure UFW without disrupting existing rules
                ufw --force enable
            fi

            # Add rules if they don't exist
            if ! ufw status numbered | grep -q "53.*ALLOW OUT"; then
                ufw allow out 53 comment "DNS"
            fi
            if ! ufw status numbered | grep -q "80.*ALLOW OUT"; then
                ufw allow out 80 comment "HTTP"
            fi
            if ! ufw status numbered | grep -q "443.*ALLOW OUT"; then
                ufw allow out 443 comment "HTTPS"
            fi
            if ! ufw status numbered | grep -q "22.*ALLOW OUT"; then
                ufw allow out 22 comment "SSH for git"
            fi
            if ! ufw status numbered | grep -q "9418.*ALLOW OUT"; then
                ufw allow out 9418 comment "Git protocol"
            fi
            ;;

        "firewalld")
            log_info "Configuring firewalld..."

            # Ensure firewalld is running
            systemctl enable firewalld || true
            systemctl start firewalld || true

            # Add services if not already present
            if ! firewall-cmd --list-services | grep -q ssh; then
                firewall-cmd --permanent --add-service=ssh
            fi
            if ! firewall-cmd --list-services | grep -q http; then
                firewall-cmd --permanent --add-service=http
            fi
            if ! firewall-cmd --list-services | grep -q https; then
                firewall-cmd --permanent --add-service=https
            fi
            if ! firewall-cmd --list-services | grep -q dns; then
                firewall-cmd --permanent --add-service=dns
            fi

            # Add git protocol port if not present
            if ! firewall-cmd --list-ports | grep -q "9418/tcp"; then
                firewall-cmd --permanent --add-port=9418/tcp
            fi

            firewall-cmd --reload
            ;;

        "iptables")
            log_info "Configuring iptables (basic rules)..."
            # Add basic rules without disrupting existing ones
            if ! iptables -C OUTPUT -p tcp --dport 443 -j ACCEPT 2>/dev/null; then
                iptables -A OUTPUT -p tcp --dport 443 -j ACCEPT
            fi
            if ! iptables -C OUTPUT -p tcp --dport 80 -j ACCEPT 2>/dev/null; then
                iptables -A OUTPUT -p tcp --dport 80 -j ACCEPT
            fi
            if ! iptables -C OUTPUT -p udp --dport 53 -j ACCEPT 2>/dev/null; then
                iptables -A OUTPUT -p udp --dport 53 -j ACCEPT
            fi
            # Save rules (distribution-dependent)
            if command -v iptables-save >/dev/null 2>&1; then
                iptables-save > /etc/iptables/rules.v4 2>/dev/null || true
            fi
            ;;
    esac

    log_success "Firewall configured"
}}

# Function to check if service exists and is active
service_exists() {{
    systemctl list-unit-files | grep -q "^$1"
}}

service_is_active() {{
    systemctl is-active --quiet "$1" 2>/dev/null
}}

# Function to get installed runner version
get_installed_runner_version() {{
    if [ -f "$RUNNER_HOME/bin/Runner.Listener" ]; then
        # Try to get version from the binary
        "$RUNNER_HOME/bin/Runner.Listener" --version 2>/dev/null | head -1 || echo "unknown"
    else
        echo "not_installed"
    fi
}}

# Function to check if runner is configured
is_runner_configured() {{
    [ -f "$RUNNER_HOME/.runner" ] && [ -f "$RUNNER_HOME/.credentials" ]
}}

# Start main execution
validate_machine_spec

log_info "ðŸ“‹ Updating package lists and installing security tools..."
$PKG_UPDATE

# Install essential security packages
for package in curl tar gzip jq; do
    install_package "$package"
done

# Install security tools based on OS
case "$PKG_MANAGER" in
    "apt")
        for package in fail2ban apparmor-utils auditd; do
            install_package "$package"
        done
        if [ "$FIREWALL_TYPE" = "ufw" ]; then
            install_package "ufw"
        fi
        ;;
    "dnf"|"yum")
        for package in fail2ban audit; do
            install_package "$package"
        done
        if [ "$FIREWALL_TYPE" = "firewalld" ]; then
            install_package "firewalld"
        fi
        ;;
esac

# Enable and start security services
for service in fail2ban auditd; do
    if systemctl list-unit-files | grep -q "^$service"; then
        systemctl enable "$service" || true
        systemctl start "$service" || true
    fi
done

log_info "ðŸ‘¤ Setting up dedicated runner user and group..."
create_or_verify_group "$RUNNER_GROUP"
create_or_verify_user "$RUNNER_USER" "$RUNNER_GROUP" "/home/$RUNNER_USER"

# Create runner directory with proper ownership
if [ ! -d "$RUNNER_HOME" ]; then
    log_info "Creating runner directory: $RUNNER_HOME"
    mkdir -p "$RUNNER_HOME"
fi

chown "$RUNNER_USER:$RUNNER_GROUP" "$RUNNER_HOME"
chmod 755 "$RUNNER_HOME"

# Install Docker if requested and not present
if command -v docker >/dev/null 2>&1; then
    log_info "Docker already installed"
else
    log_info "ðŸ³ Installing Docker..."
    case "$PKG_MANAGER" in
        "apt")
            install_package "docker.io"
            install_package "containerd"
            ;;
        "dnf"|"yum")
            install_package "podman"
            install_package "buildah"
            ;;
    esac

    systemctl enable docker 2>/dev/null || systemctl enable podman 2>/dev/null || true
    systemctl start docker 2>/dev/null || systemctl start podman 2>/dev/null || true
fi

log_info "ðŸ“¦ Checking GitHub Actions Runner installation..."

# Check current installation
current_version=$(get_installed_runner_version)
log_info "Current runner version: $current_version"
log_info "Latest available version: $RUNNER_VERSION"

# Determine if we need to install/update
need_install=false
if [ "$current_version" = "not_installed" ]; then
    log_info "Runner not installed, will install version $RUNNER_VERSION"
    need_install=true
elif [ "$current_version" != "$RUNNER_VERSION" ] && [ "$current_version" != "unknown" ]; then
    log_info "Runner version $current_version is outdated, will update to $RUNNER_VERSION"
    need_install=true
elif [ "$current_version" = "unknown" ]; then
    log_warning "Cannot determine current runner version, will reinstall"
    need_install=true
else
    log_success "Runner version $RUNNER_VERSION already installed"
fi

# Install/update runner if needed
if [ "$need_install" = "true" ]; then
    log_info "Installing/updating GitHub Actions Runner..."

    # Stop service if running
    if service_exists "$SERVICE_NAME" && service_is_active "$SERVICE_NAME"; then
        log_info "Stopping existing runner service..."
        systemctl stop "$SERVICE_NAME" || true
    fi

    # Remove old service if it exists
    if service_exists "$SERVICE_NAME"; then
        log_info "Removing old service..."
        "$RUNNER_HOME/svc.sh" uninstall 2>/dev/null || true
    fi

    # Download and extract runner
    cd "$RUNNER_HOME"

    RUNNER_ARCHIVE="actions-runner-linux-$RUNNER_ARCH-$RUNNER_VERSION.tar.gz"
    DOWNLOAD_URL="https://github.com/actions/runner/releases/download/v$RUNNER_VERSION/$RUNNER_ARCHIVE"

    log_info "Downloading runner from: $DOWNLOAD_URL"

    # Download as runner user
    if ! sudo -u "$RUNNER_USER" curl -L -o "$RUNNER_ARCHIVE" "$DOWNLOAD_URL"; then
        log_error "Failed to download runner archive"
        exit 1
    fi

    # Verify download
    if [ ! -f "$RUNNER_ARCHIVE" ]; then
        log_error "Runner archive not found after download"
        exit 1
    fi

    # Extract archive
    log_info "Extracting runner archive..."
    sudo -u "$RUNNER_USER" tar xzf "$RUNNER_ARCHIVE"

    # Clean up archive
    rm -f "$RUNNER_ARCHIVE"

    log_success "Runner binaries installed"
fi

# Configure runner if not already configured or if we just installed
if ! is_runner_configured || [ "$need_install" = "true" ]; then
    log_info "Configuring runner..."

    cd "$RUNNER_HOME"

    # Run configuration as runner user
    sudo -u "$RUNNER_USER" ./config.sh \\
        --url "$REPO_URL" \\
        --token "$RUNNER_TOKEN" \\
        --name "$RUNNER_NAME" \\
        --unattended \\
        --replace \\
        --runnergroup "default" \\
        --work "_work"

    log_success "Runner configured"
else
    log_info "Runner already configured"
fi

# Install and start service
log_info "Setting up runner service..."
if ! service_exists "$SERVICE_NAME"; then
    ./svc.sh install "$RUNNER_USER"
    log_success "Service installed"
else
    log_info "Service already exists"
fi

# Start service
if ! service_is_active "$SERVICE_NAME"; then
    systemctl start "$SERVICE_NAME"
    log_success "Service started"
else
    log_info "Service already running"
fi

# Enable service for auto-start
systemctl enable "$SERVICE_NAME" || true

log_info "ðŸ” Applying security configuration..."
configure_networkmanager_firewall_coordination
configure_firewall

# Set proper file permissions
chown -R "$RUNNER_USER:$RUNNER_GROUP" "$RUNNER_HOME"
chmod -R 750 "$RUNNER_HOME"
# Allow read access to config files for service
chmod 644 "$RUNNER_HOME"/*.json 2>/dev/null || true

# Create log directory following FHS
LOG_DIR="/var/log/github-runner"
if [ ! -d "$LOG_DIR" ]; then
    mkdir -p "$LOG_DIR"
    chown "$RUNNER_USER:$RUNNER_GROUP" "$LOG_DIR"
    chmod 750 "$LOG_DIR"
fi

# Set up audit rules if auditd is available
if command -v auditctl >/dev/null 2>&1; then
    AUDIT_RULES="/etc/audit/rules.d/github-runner.rules"
    if [ ! -f "$AUDIT_RULES" ] || ! grep -q "$RUNNER_HOME" "$AUDIT_RULES"; then
        log_info "Setting up audit monitoring..."
        echo "-w $RUNNER_HOME/ -p wa -k github-runner-access" >> "$AUDIT_RULES"
        service auditd restart 2>/dev/null || systemctl restart auditd 2>/dev/null || true
    fi
fi

# Verify installation
log_info "ðŸ” Verifying installation..."

if service_is_active "$SERVICE_NAME"; then
    log_success "âœ… Service is running"
else
    log_warning "âš ï¸  Service is not running"
fi

if is_runner_configured; then
    log_success "âœ… Runner is configured"
else
    log_error "âŒ Runner configuration failed"
fi

# Final status
log_success "âœ… GitHub Actions Runner setup complete!"
echo ""
echo "ðŸ“Š Installation Summary:"
echo "ðŸ”— Repository: $REPO_URL"
echo "ðŸƒ Runner Name: $RUNNER_NAME"
echo "ðŸ“‚ Runner Home: $RUNNER_HOME"
echo "ðŸ‘¤ Runner User: $RUNNER_USER"
echo "ðŸ”§ Service: $SERVICE_NAME"
echo "ðŸ”¥ Firewall: $FIREWALL_TYPE"
echo "ðŸ“¦ Version: $RUNNER_VERSION"
echo ""
echo "ðŸŽ¯ Next Steps:"
echo "1. Verify runner appears in GitHub repository settings"
echo "2. Test runner with a simple workflow"
echo "3. Monitor logs: sudo journalctl -u $SERVICE_NAME -f"
echo "4. Check security: sudo auditctl -l | grep github-runner"
echo ""
echo "ðŸ”’ Security Features Enabled:"
echo "- Dedicated user account ($RUNNER_USER)"
echo "- Firewall configured ($FIREWALL_TYPE)"
echo "- Audit logging enabled"
echo "- File permissions hardened"
echo "- Fail2ban protection"
echo "- Service isolation"
"""
    return script_content


def _generate_windows_setup_script(
    runner, machine_spec: MachineSpec, repo_url=None, runner_token=None
):
    """
    Generate comprehensive, idempotent Windows PowerShell setup script with security hardening.

    Args:
        runner: Runner configuration from YAML
        machine_spec: Complete machine specification with detected capabilities
        repo_url: Repository URL for runner registration
        runner_token: GitHub runner registration token

    Returns:
        str: Complete setup script content
    """
    runner_name = runner["name"]
    arch = machine_spec.arch

    # Validate machine spec for Windows
    if machine_spec.platform != "windows":
        print(
            f"âš ï¸  WARNING: Script generated for Windows but machine spec shows platform: {machine_spec.platform}"
        )

    # Get latest runner version
    latest_version = _get_latest_runner_version()

    # Verify the asset exists, fallback if not
    if not _verify_runner_asset(latest_version, arch, "windows"):
        latest_version = "2.311.0"  # Known good fallback

    print(f"ðŸ“‹ Windows script using machine specification:")
    print(f"   - Platform: {machine_spec.platform}")
    print(f"   - Architecture: {arch}")
    print(f"   - OS: {machine_spec.os_name} {machine_spec.os_version}")
    print(f"   - Firewall: {machine_spec.primary_firewall}")

    # Set default values if not provided
    if not repo_url:
        repo_url = "https://github.com/OWNER/REPO"
    if not runner_token:
        runner_token = "YOUR_RUNNER_TOKEN"

    # Extract owner/repo from URL
    repo_match = re.search(r"github\.com/([^/]+)/([^/]+)", repo_url)
    if repo_match:
        owner, repo = repo_match.groups()
        repo = repo.replace(".git", "")
    else:
        owner, repo = "OWNER", "REPO"

    # Generate header using shared comment system
    header_comment = ScriptCommentGenerator.generate_header(
        runner_name=runner_name,
        cores=runner["cores"],
        ram_gb=runner["ram_gb"],
        storage_gb=runner["storage_gb"],
        arch=arch,
        os_family=machine_spec.os_family.value if machine_spec.os_family else "windows",
        package_manager="winget/choco",
        shell_type="powershell",
    )

    script_content = f"""{header_comment}

param(
    [Parameter(Mandatory=$true)]
    [string]$RepoUrl,
    [Parameter(Mandatory=$true)]
    [string]$RunnerToken
)

# Configuration
$RUNNER_VERSION = "{latest_version}"
$RUNNER_ARCH = "{arch.value if hasattr(arch, 'value') else str(arch)}"
$REPO_URL = $RepoUrl
$RUNNER_TOKEN = $RunnerToken
$RUNNER_NAME = "{runner_name}"
$RUNNER_HOME = "C:\\actions-runner"
$SERVICE_NAME = "actions.runner.{owner}-{repo}.{runner_name}"

{ScriptCommentGenerator.generate_section_comment('logging', 'powershell')}
function Write-Log {{
    param([string]$Message, [string]$Level = "INFO")
    $timestamp = Get-Date -Format "yyyy-MM-dd HH:mm:ss"
    $color = switch ($Level) {{
        "ERROR" {{ "Red" }}
        "WARNING" {{ "Yellow" }}
        "SUCCESS" {{ "Green" }}
        default {{ "White" }}
    }}
    Write-Host "[$timestamp] [$Level] $Message" -ForegroundColor $color
}}

Write-Log "ðŸ”’ Starting security-hardened GitHub Runner setup for $RUNNER_NAME..."
# Idempotent, security-hardened installation following GitHub best practices
# Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
# Run as Administrator
#
# Runner Specifications:
# - Name: {runner_name}
# - Cores: {runner['cores']}
# - RAM: {runner['ram_gb']} GB
# - Storage: {runner['storage_gb']} GB
# - Architecture: {arch}

param(
    [Parameter(Mandatory=$false)]
    [string]$RepoUrl = "{repo_url}",
    [Parameter(Mandatory=$false)]
    [string]$RunnerToken = "{runner_token}",
    [Parameter(Mandatory=$false)]
    [string]$RunnerName = "{runner_name}"
)

# Configuration
$RunnerVersion = "{latest_version}"
$RunnerArch = "{arch}"
$RunnerUser = "github-runner"
$RunnerGroup = "GitHubRunners"
$RunnerHome = "C:\\actions-runner"
$ServiceName = "actions.runner.{owner}-{repo}.{runner_name}"

# Logging functions
function Write-Info {{ param($Message) Write-Host "[INFO] $Message" -ForegroundColor Blue }}
function Write-Success {{ param($Message) Write-Host "[SUCCESS] $Message" -ForegroundColor Green }}
function Write-Warning {{ param($Message) Write-Host "[WARNING] $Message" -ForegroundColor Yellow }}
function Write-Error {{ param($Message) Write-Host "[ERROR] $Message" -ForegroundColor Red }}

Write-Info "ðŸ”’ Starting idempotent GitHub Runner setup for $RunnerName..."

# Check if running as Administrator
if (-NOT ([Security.Principal.WindowsPrincipal] [Security.Principal.WindowsIdentity]::GetCurrent()).IsInRole([Security.Principal.WindowsBuiltInRole] "Administrator")) {{
    Write-Error "âŒ This script must be run as Administrator"
    exit 1
}}

# Function to check if group exists
function Test-LocalGroup {{
    param($GroupName)
    try {{
        Get-LocalGroup -Name $GroupName -ErrorAction Stop
        return $true
    }} catch {{
        return $false
    }}
}}

# Function to check if user exists
function Test-LocalUser {{
    param($UserName)
    try {{
        Get-LocalUser -Name $UserName -ErrorAction Stop
        return $true
    }} catch {{
        return $false
    }}
}}

# Function to check if user is in group
function Test-UserInGroup {{
    param($UserName, $GroupName)
    try {{
        $groupMembers = Get-LocalGroupMember -Group $GroupName -ErrorAction Stop
        return ($groupMembers.Name -contains "$env:COMPUTERNAME\\$UserName")
    }} catch {{
        return $false
    }}
}}

# Function to check if Windows feature is enabled
function Test-WindowsFeature {{
    param($FeatureName)
    $feature = Get-WindowsOptionalFeature -Online -FeatureName $FeatureName -ErrorAction SilentlyContinue
    return ($feature -and $feature.State -eq "Enabled")
}}

# Function to get installed runner version
function Get-InstalledRunnerVersion {{
    $versionFile = Join-Path $RunnerHome "bin\\Runner.Listener.exe"
    if (Test-Path $versionFile) {{
        try {{
            $version = & $versionFile --version 2>$null | Select-Object -First 1
            return $version
        }} catch {{
            return "unknown"
        }}
    }}
    return "not_installed"
}}

# Function to test if runner is configured
function Test-RunnerConfigured {{
    $runnerFile = Join-Path $RunnerHome ".runner"
    $credentialsFile = Join-Path $RunnerHome ".credentials"
    return ((Test-Path $runnerFile) -and (Test-Path $credentialsFile))
}}

# Function to test if service exists
function Test-ServiceExists {{
    param($ServiceName)
    return (Get-Service -Name $ServiceName -ErrorAction SilentlyContinue) -ne $null
}}

Write-Info "ðŸ“‹ Configuring Windows security settings..."

# Set execution policy if needed
$currentPolicy = Get-ExecutionPolicy -Scope CurrentUser
if ($currentPolicy -eq "Restricted") {{
    Write-Info "Setting execution policy to RemoteSigned"
    Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser -Force
}}

# Enable Windows Defender if not already enabled
$defenderStatus = Get-MpComputerStatus -ErrorAction SilentlyContinue
if (-not $defenderStatus -or -not $defenderStatus.RealTimeProtectionEnabled) {{
    Write-Info "Enabling Windows Defender real-time protection"
    Set-MpPreference -DisableRealtimeMonitoring $false -ErrorAction SilentlyContinue
}}

Write-Info "ðŸ‘¤ Setting up dedicated runner user and group..."

# Create group if it doesn't exist
if (-not (Test-LocalGroup $RunnerGroup)) {{
    Write-Info "Creating group: $RunnerGroup"
    New-LocalGroup -Name $RunnerGroup -Description "GitHub Actions Runners" -ErrorAction Stop
    Write-Success "Group $RunnerGroup created"
}} else {{
    Write-Info "Group $RunnerGroup already exists"
}}

# Create user if it doesn't exist
if (-not (Test-LocalUser $RunnerUser)) {{
    Write-Info "Creating user: $RunnerUser"
    $password = [System.Web.Security.Membership]::GeneratePassword(20, 5)
    $securePassword = ConvertTo-SecureString $password -AsPlainText -Force
    New-LocalUser -Name $RunnerUser -Password $securePassword -Description "GitHub Actions Runner Service Account" -PasswordNeverExpires -ErrorAction Stop
    Write-Success "User $RunnerUser created"
}} else {{
    Write-Info "User $RunnerUser already exists"
}}

# Add user to group if not already a member
if (-not (Test-UserInGroup $RunnerUser $RunnerGroup)) {{
    Write-Info "Adding $RunnerUser to $RunnerGroup group"
    Add-LocalGroupMember -Group $RunnerGroup -Member $RunnerUser -ErrorAction Stop
}}

# Create runner directory
if (-not (Test-Path $RunnerHome)) {{
    Write-Info "Creating runner directory: $RunnerHome"
    New-Item -Path $RunnerHome -ItemType Directory -Force | Out-Null
}}

# Set directory permissions
Write-Info "Setting directory permissions"
$acl = Get-Acl $RunnerHome
$accessRule = New-Object System.Security.AccessControl.FileSystemAccessRule($RunnerUser, "FullControl", "ContainerInherit,ObjectInherit", "None", "Allow")
$acl.SetAccessRule($accessRule)
Set-Acl -Path $RunnerHome -AclObject $acl

Write-Info "ðŸ³ Setting up container support..."

# Enable Hyper-V if needed and not already enabled
if (-not (Test-WindowsFeature "Microsoft-Hyper-V")) {{
    Write-Info "Enabling Hyper-V (requires restart)"
    Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Hyper-V -All -NoRestart -ErrorAction SilentlyContinue
}}

# Enable Containers feature if needed
if (-not (Test-WindowsFeature "Containers")) {{
    Write-Info "Enabling Containers feature"
    Enable-WindowsOptionalFeature -Online -FeatureName Containers -All -NoRestart -ErrorAction SilentlyContinue
}}

# Install Docker if not present
if (-not (Get-Command docker -ErrorAction SilentlyContinue)) {{
    Write-Info "Installing Docker..."
    try {{
        Install-Module -Name DockerMsftProvider -Repository PSGallery -Force -ErrorAction Stop
        Install-Package -Name docker -ProviderName DockerMsftProvider -Force -ErrorAction Stop
        Write-Success "Docker installed"
    }} catch {{
        Write-Warning "Failed to install Docker: $_"
    }}
}}

Write-Info "ðŸ“¦ Checking GitHub Actions Runner installation..."

# Check current installation
$currentVersion = Get-InstalledRunnerVersion
Write-Info "Current runner version: $currentVersion"
Write-Info "Latest available version: $RunnerVersion"

# Determine if we need to install/update
$needInstall = $false
if ($currentVersion -eq "not_installed") {{
    Write-Info "Runner not installed, will install version $RunnerVersion"
    $needInstall = $true
}} elseif ($currentVersion -ne $RunnerVersion -and $currentVersion -ne "unknown") {{
    Write-Info "Runner version $currentVersion is outdated, will update to $RunnerVersion"
    $needInstall = $true
}} elseif ($currentVersion -eq "unknown") {{
    Write-Warning "Cannot determine current runner version, will reinstall"
    $needInstall = $true
}} else {{
    Write-Success "Runner version $RunnerVersion already installed"
}}

# Install/update runner if needed
if ($needInstall) {{
    Write-Info "Installing/updating GitHub Actions Runner..."

    # Stop service if running
    if (Test-ServiceExists $ServiceName) {{
        $service = Get-Service -Name $ServiceName
        if ($service.Status -eq "Running") {{
            Write-Info "Stopping existing runner service..."
            Stop-Service -Name $ServiceName -Force -ErrorAction SilentlyContinue
        }}
    }}

    # Remove old service if it exists
    if (Test-ServiceExists $ServiceName) {{
        Write-Info "Removing old service..."
        & "$RunnerHome\\svc.sh" uninstall 2>$null
    }}

    # Download and extract runner
    Set-Location $RunnerHome

    $runnerArchive = "actions-runner-win-$RunnerArch-$RunnerVersion.zip"
    $downloadUrl = "https://github.com/actions/runner/releases/download/v$RunnerVersion/$runnerArchive"

    Write-Info "Downloading runner from: $downloadUrl"

    try {{
        Invoke-WebRequest -Uri $downloadUrl -OutFile $runnerArchive -UseBasicParsing -ErrorAction Stop

        # Verify download
        if (-not (Test-Path $runnerArchive)) {{
            throw "Runner archive not found after download"
        }}

        # Extract archive
        Write-Info "Extracting runner archive..."
        Expand-Archive -Path $runnerArchive -DestinationPath . -Force

        # Clean up archive
        Remove-Item $runnerArchive -Force

        Write-Success "Runner binaries installed"
    }} catch {{
        Write-Error "Failed to download or extract runner: $_"
        exit 1
    }}
}}

# Configure runner if not already configured or if we just installed
if (-not (Test-RunnerConfigured) -or $needInstall) {{
    Write-Info "Configuring runner..."

    Set-Location $RunnerHome

    # Run configuration
    & .\\config.cmd --url $RepoUrl --token $RunnerToken --name $RunnerName --unattended --replace --runasservice --windowslogonaccount $RunnerUser

    if ($LASTEXITCODE -eq 0) {{
        Write-Success "Runner configured"
    }} else {{
        Write-Error "Runner configuration failed"
        exit 1
    }}
}} else {{
    Write-Info "Runner already configured"
}}

Write-Info "ðŸ” Configuring Windows Firewall..."

# Configure Windows Firewall
$firewallProfiles = @("Domain", "Public", "Private")
foreach ($profile in $firewallProfiles) {{
    # Enable firewall for each profile
    Set-NetFirewallProfile -Profile $profile -Enabled True -ErrorAction SilentlyContinue

    # Set default actions (be careful not to lock yourself out)
    # Only set outbound to block, leave inbound as is to avoid disrupting existing config
    Set-NetFirewallProfile -Profile $profile -DefaultOutboundAction Block -ErrorAction SilentlyContinue
}}

# Add outbound rules for GitHub Actions
$rules = @(
    @{{Name="GitHub Actions - DNS"; Port=53; Protocol="UDP"}},
    @{{Name="GitHub Actions - HTTP"; Port=80; Protocol="TCP"}},
    @{{Name="GitHub Actions - HTTPS"; Port=443; Protocol="TCP"}},
    @{{Name="GitHub Actions - SSH"; Port=22; Protocol="TCP"}},
    @{{Name="GitHub Actions - Git"; Port=9418; Protocol="TCP"}}
)

foreach ($rule in $rules) {{
    $existingRule = Get-NetFirewallRule -DisplayName $rule.Name -ErrorAction SilentlyContinue
    if (-not $existingRule) {{
        Write-Info "Adding firewall rule: $($rule.Name)"
        New-NetFirewallRule -DisplayName $rule.Name -Direction Outbound -Protocol $rule.Protocol -RemotePort $rule.Port -Action Allow -ErrorAction SilentlyContinue
    }}
}}

# Set proper file permissions
Write-Info "Setting file permissions..."
$acl = Get-Acl $RunnerHome
$acl.SetOwner([System.Security.Principal.NTAccount]$RunnerUser)
Set-Acl -Path $RunnerHome -AclObject $acl

# Create log directory
$logDir = "C:\\ProgramData\\GitHubRunner\\Logs"
if (-not (Test-Path $logDir)) {{
    New-Item -Path $logDir -ItemType Directory -Force | Out-Null
    $acl = Get-Acl $logDir
    $accessRule = New-Object System.Security.AccessControl.FileSystemAccessRule($RunnerUser, "FullControl", "ContainerInherit,ObjectInherit", "None", "Allow")
    $acl.SetAccessRule($accessRule)
    Set-Acl -Path $logDir -AclObject $acl
}}

# Verify installation
Write-Info "ðŸ” Verifying installation..."

$service = Get-Service -Name $ServiceName -ErrorAction SilentlyContinue
if ($service -and $service.Status -eq "Running") {{
    Write-Success "âœ… Service is running"
}} else {{
    Write-Warning "âš ï¸  Service is not running"
}}

if (Test-RunnerConfigured) {{
    Write-Success "âœ… Runner is configured"
}} else {{
    Write-Error "âŒ Runner configuration failed"
}}

# Final status
Write-Success "âœ… GitHub Actions Runner setup complete!"
Write-Host ""
Write-Host "ðŸ“Š Installation Summary:"
Write-Host "ðŸ”— Repository: $RepoUrl"
Write-Host "ðŸƒ Runner Name: $RunnerName"
Write-Host "ðŸ“‚ Runner Home: $RunnerHome"
Write-Host "ðŸ‘¤ Runner User: $RunnerUser"
Write-Host "ðŸ”§ Service: $ServiceName"
Write-Host "ðŸ“¦ Version: $RunnerVersion"
Write-Host ""
Write-Host "ðŸŽ¯ Next Steps:"
Write-Host "1. Verify runner appears in GitHub repository settings"
Write-Host "2. Test runner with a simple workflow"
Write-Host "3. Monitor logs: Get-EventLog -LogName Application -Source 'Actions Runner Service'"
Write-Host "4. Check service: Get-Service -Name '$ServiceName'"
Write-Host ""
Write-Host "ðŸ”’ Security Features Enabled:"
Write-Host "- Dedicated user account ($RunnerUser)"
Write-Host "- Windows Firewall configured"
Write-Host "- Windows Defender enabled"
Write-Host "- File permissions hardened"
Write-Host "- Service isolation"
"""

    # Add any additional template commands if available
    template = SETUP_TEMPLATES.get("windows", {})
    for cmd in template.get("monitoring_setup", []):
        script_content += f"{cmd}\n"

    script_content += (
        'Write-Host "ðŸ” Applying security configuration..." -ForegroundColor Yellow\n'
    )

    # Add security setup commands
    for cmd in template["security_setup"]:
        script_content += f"{cmd}\n"

    script_content += "\n"
    script_content += (
        'Write-Host "âœ… GitHub Actions Runner setup complete!" -ForegroundColor Green\n'
    )
    script_content += 'Write-Host "ðŸ”— Repository: $RepoUrl" -ForegroundColor Cyan\n'
    script_content += 'Write-Host "ðŸƒ Runner Name: $RunnerName" -ForegroundColor Cyan\n'
    script_content += 'Write-Host "ðŸ“‹ Service Status:" -ForegroundColor Cyan\n'
    script_content += 'Get-Service -Name "*actions.runner*" | Format-Table -AutoSize\n'
    script_content += "\n"
    script_content += 'Write-Host ""\n'
    script_content += 'Write-Host "ðŸŽ¯ Next Steps:" -ForegroundColor Yellow\n'
    script_content += (
        'Write-Host "1. Verify runner appears in GitHub repository settings"\n'
    )
    script_content += 'Write-Host "2. Test runner with a simple workflow"\n'
    script_content += 'Write-Host "3. Monitor logs in Event Viewer"\n'
    script_content += 'Write-Host "4. Check Windows Defender logs"\n'
    script_content += 'Write-Host ""\n'
    script_content += (
        'Write-Host "ðŸ”’ Security Features Enabled:" -ForegroundColor Green\n'
    )
    script_content += 'Write-Host "- Dedicated user account (github-runner)"\n'
    script_content += 'Write-Host "- Windows Firewall configured"\n'
    script_content += 'Write-Host "- Windows Defender enabled"\n'
    script_content += 'Write-Host "- Container security (Docker/Hyper-V)"\n'
    script_content += 'Write-Host "- Service account security"\n'
    script_content += 'Write-Host "- File permissions hardened"\n'

    return script_content


def _generate_macos_setup_script(
    runner, machine_spec: MachineSpec, repo_url=None, runner_token=None
):
    """
    Generate comprehensive, idempotent macOS setup script with security hardening.

    Args:
        runner: Runner configuration from YAML
        machine_spec: Complete machine specification with detected capabilities
        repo_url: Repository URL for runner registration
        runner_token: GitHub runner registration token

    Returns:
        str: Complete setup script content
    """
    runner_name = runner["name"]
    arch = machine_spec.arch

    # Validate machine spec for macOS
    if machine_spec.platform != "darwin":
        print(
            f"âš ï¸  WARNING: Script generated for macOS but machine spec shows platform: {machine_spec.platform}"
        )

    # Get latest runner version
    latest_version = _get_latest_runner_version()

    # Verify the asset exists, fallback if not
    if not _verify_runner_asset(latest_version, arch, "darwin"):
        latest_version = "2.311.0"  # Known good fallback

    print(f"ðŸ“‹ macOS script using machine specification:")
    print(f"   - Platform: {machine_spec.platform}")
    print(f"   - Architecture: {arch}")
    print(f"   - OS: {machine_spec.os_name} {machine_spec.os_version}")
    print(f"   - Firewall: {machine_spec.primary_firewall}")

    # Set default values if not provided
    if not repo_url:
        repo_url = "https://github.com/OWNER/REPO"
    if not runner_token:
        runner_token = "YOUR_RUNNER_TOKEN"

    # Extract owner/repo from URL
    repo_match = re.search(r"github\.com/([^/]+)/([^/]+)", repo_url)
    if repo_match:
        owner, repo = repo_match.groups()
        repo = repo.replace(".git", "")
    else:
        owner, repo = "OWNER", "REPO"

    script_content = f"""#!/bin/zsh
# GitHub Actions Self-Hosted Runner Setup Script for {runner_name}
# Idempotent, security-hardened installation following GitHub best practices
# Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
#
# Runner Specifications:
# - Name: {runner_name}
# - Cores: {runner['cores']}
# - RAM: {runner['ram_gb']} GB
# - Storage: {runner['storage_gb']} GB
# - Architecture: {arch}

set -euo pipefail

# Configuration
RUNNER_VERSION="{latest_version}"
RUNNER_ARCH="{arch.value if hasattr(arch, 'value') else str(arch)}"
REPO_URL="{repo_url}"
RUNNER_TOKEN="{runner_token}"
RUNNER_NAME="{runner_name}"
RUNNER_USER="github-runner"
RUNNER_HOME="/opt/actions-runner"
SERVICE_NAME="actions.runner.{owner}-{repo}.{runner_name}"

# Colors for output
RED='\\033[0;31m'
GREEN='\\033[0;32m'
YELLOW='\\033[1;33m'
BLUE='\\033[0;34m'
NC='\\033[0m' # No Color

# Logging functions
log_info() {{ echo -e "${{BLUE}}[INFO]${{NC}} $1" }}
log_success() {{ echo -e "${{GREEN}}[SUCCESS]${{NC}} $1" }}
log_warning() {{ echo -e "${{YELLOW}}[WARNING]${{NC}} $1" }}
log_error() {{ echo -e "${{RED}}[ERROR]${{NC}} $1" }}

log_info "ðŸ”’ Starting idempotent GitHub Runner setup for $RUNNER_NAME..."

# Check if running as root
if [[ $EUID -eq 0 ]]; then
   log_error "âŒ Do not run this script as root on macOS"
   exit 1
fi

# Function to check if command exists
command_exists() {{
    command -v "$1" >/dev/null 2>&1
}}

# Function to check if user exists
user_exists() {{
    dscl . -read /Users/"$1" >/dev/null 2>&1
}}

# Function to check if group exists
group_exists() {{
    dscl . -read /Groups/"$1" >/dev/null 2>&1
}}

# Function to check if user is in group
user_in_group() {{
    local user="$1"
    local group="$2"
    dscl . -read /Groups/"$group" GroupMembership 2>/dev/null | grep -q "\\b$user\\b"
}}

# Function to get installed runner version
get_installed_runner_version() {{
    if [ -f "$RUNNER_HOME/bin/Runner.Listener" ]; then
        "$RUNNER_HOME/bin/Runner.Listener" --version 2>/dev/null | head -1 || echo "unknown"
    else
        echo "not_installed"
    fi
}}

# Function to check if runner is configured
is_runner_configured() {{
    [ -f "$RUNNER_HOME/.runner" ] && [ -f "$RUNNER_HOME/.credentials" ]
}}

# Function to check if service exists
service_exists() {{
    launchctl list | grep -q "$SERVICE_NAME" 2>/dev/null
}}

# Function to check if service is running
service_is_running() {{
    if service_exists; then
        launchctl list "$SERVICE_NAME" >/dev/null 2>&1
    else
        return 1
    fi
}}

log_info "ðŸ“‹ Setting up development environment..."

# Install Homebrew if not present
if ! command_exists brew; then
    log_info "Installing Homebrew..."
    /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

    # Add Homebrew to PATH for current session
    if [[ "$RUNNER_ARCH" == "arm64" ]]; then
        eval "$(/opt/homebrew/bin/brew shellenv)"
    else
        eval "$(/usr/local/bin/brew shellenv)"
    fi
else
    log_info "Homebrew already installed"
fi

# Update Homebrew
log_info "Updating Homebrew..."
brew update

# Install essential tools
essential_tools=("git" "curl" "wget" "jq")
for tool in "${{essential_tools[@]}}"; do
    if ! command_exists "$tool"; then
        log_info "Installing $tool..."
        brew install "$tool"
    else
        log_info "$tool already installed"
    fi
done

# Install Docker if not present
if ! command_exists docker; then
    log_info "Installing Docker Desktop..."
    brew install --cask docker
    log_info "Docker installed. You may need to start Docker Desktop manually."
else
    log_info "Docker already installed"
fi

log_info "ðŸ‘¤ Setting up dedicated runner user and group..."

# Create group if it doesn't exist
if ! group_exists "github-runners"; then
    log_info "Creating group: github-runners"
    sudo dscl . -create /Groups/github-runners
    sudo dscl . -create /Groups/github-runners PrimaryGroupID 502
    log_success "Group github-runners created"
else
    log_info "Group github-runners already exists"
fi

# Create user if it doesn't exist
if ! user_exists "$RUNNER_USER"; then
    log_info "Creating user: $RUNNER_USER"

    # Find next available UID
    local uid=502
    while dscl . -read /Users uid | grep -q " $uid"; do
        ((uid++))
    done

    sudo dscl . -create /Users/"$RUNNER_USER"
    sudo dscl . -create /Users/"$RUNNER_USER" UserShell /bin/zsh
    sudo dscl . -create /Users/"$RUNNER_USER" RealName "GitHub Actions Runner"
    sudo dscl . -create /Users/"$RUNNER_USER" UniqueID "$uid"
    sudo dscl . -create /Users/"$RUNNER_USER" PrimaryGroupID 502
    sudo dscl . -create /Users/"$RUNNER_USER" NFSHomeDirectory /Users/"$RUNNER_USER"

    # Create home directory
    sudo mkdir -p /Users/"$RUNNER_USER"
    sudo chown "$RUNNER_USER":github-runners /Users/"$RUNNER_USER"

    log_success "User $RUNNER_USER created"
else
    log_info "User $RUNNER_USER already exists"
fi

# Ensure user is in correct group
if ! user_in_group "$RUNNER_USER" "github-runners"; then
    log_info "Adding $RUNNER_USER to github-runners group"
    sudo dscl . -append /Groups/github-runners GroupMembership "$RUNNER_USER"
fi

# Create runner directory
if [ ! -d "$RUNNER_HOME" ]; then
    log_info "Creating runner directory: $RUNNER_HOME"
    sudo mkdir -p "$RUNNER_HOME"
fi

sudo chown "$RUNNER_USER":github-runners "$RUNNER_HOME"
sudo chmod 755 "$RUNNER_HOME"

log_info "ðŸ” Configuring security settings..."

# Enable macOS firewall if not already enabled
firewall_status=$(sudo /usr/libexec/ApplicationFirewall/socketfilterfw --getglobalstate)
if [[ "$firewall_status" == *"disabled"* ]]; then
    log_info "Enabling macOS firewall..."
    sudo /usr/libexec/ApplicationFirewall/socketfilterfw --setglobalstate on
    sudo /usr/libexec/ApplicationFirewall/socketfilterfw --setallowsigned on
    sudo /usr/libexec/ApplicationFirewall/socketfilterfw --setallowsignedapp on
else
    log_info "Firewall already enabled"
fi

# Enable stealth mode
sudo /usr/libexec/ApplicationFirewall/socketfilterfw --setstealthmode on

log_info "ðŸ“¦ Checking GitHub Actions Runner installation..."

# Check current installation
current_version=$(get_installed_runner_version)
log_info "Current runner version: $current_version"
log_info "Latest available version: $RUNNER_VERSION"

# Determine if we need to install/update
need_install=false
if [ "$current_version" = "not_installed" ]; then
    log_info "Runner not installed, will install version $RUNNER_VERSION"
    need_install=true
elif [ "$current_version" != "$RUNNER_VERSION" ] && [ "$current_version" != "unknown" ]; then
    log_info "Runner version $current_version is outdated, will update to $RUNNER_VERSION"
    need_install=true
elif [ "$current_version" = "unknown" ]; then
    log_warning "Cannot determine current runner version, will reinstall"
    need_install=true
else
    log_success "Runner version $RUNNER_VERSION already installed"
fi

# Install/update runner if needed
if [ "$need_install" = "true" ]; then
    log_info "Installing/updating GitHub Actions Runner..."

    # Stop service if running
    if service_is_running; then
        log_info "Stopping existing runner service..."
        sudo launchctl unload "/Library/LaunchDaemons/$SERVICE_NAME.plist" 2>/dev/null || true
    fi

    # Remove old service if it exists
    if service_exists; then
        log_info "Removing old service..."
        sudo "$RUNNER_HOME/svc.sh" uninstall 2>/dev/null || true
    fi

    # Download and extract runner
    cd "$RUNNER_HOME"

    # Determine correct archive name for macOS
    if [[ "$RUNNER_ARCH" == "arm64" ]]; then
        RUNNER_ARCHIVE="actions-runner-osx-arm64-$RUNNER_VERSION.tar.gz"
    else
        RUNNER_ARCHIVE="actions-runner-osx-x64-$RUNNER_VERSION.tar.gz"
    fi

    DOWNLOAD_URL="https://github.com/actions/runner/releases/download/v$RUNNER_VERSION/$RUNNER_ARCHIVE"

    log_info "Downloading runner from: $DOWNLOAD_URL"

    # Download as runner user
    if ! sudo -u "$RUNNER_USER" curl -L -o "$RUNNER_ARCHIVE" "$DOWNLOAD_URL"; then
        log_error "Failed to download runner archive"
        exit 1
    fi

    # Verify download
    if [ ! -f "$RUNNER_ARCHIVE" ]; then
        log_error "Runner archive not found after download"
        exit 1
    fi

    # Extract archive
    log_info "Extracting runner archive..."
    sudo -u "$RUNNER_USER" tar xzf "$RUNNER_ARCHIVE"

    # Clean up archive
    rm -f "$RUNNER_ARCHIVE"

    log_success "Runner binaries installed"
fi

# Configure runner if not already configured or if we just installed
if ! is_runner_configured || [ "$need_install" = "true" ]; then
    log_info "Configuring runner..."

    cd "$RUNNER_HOME"

    # Run configuration as runner user
    sudo -u "$RUNNER_USER" ./config.sh \\
        --url "$REPO_URL" \\
        --token "$RUNNER_TOKEN" \\
        --name "$RUNNER_NAME" \\
        --unattended \\
        --replace \\
        --runnergroup "default" \\
        --work "_work"

    log_success "Runner configured"
else
    log_info "Runner already configured"
fi

# Install and start service
log_info "Setting up runner service..."
if ! service_exists; then
    sudo ./svc.sh install "$RUNNER_USER"
    log_success "Service installed"
else
    log_info "Service already exists"
fi

# Start service
if ! service_is_running; then
    sudo ./svc.sh start
    log_success "Service started"
else
    log_info "Service already running"
fi

# Set proper file permissions
sudo chown -R "$RUNNER_USER":github-runners "$RUNNER_HOME"
sudo chmod -R 750 "$RUNNER_HOME"
# Allow read access to config files for service
sudo chmod 644 "$RUNNER_HOME"/*.json 2>/dev/null || true

# Create log directory following macOS conventions
LOG_DIR="/var/log/github-runner"
if [ ! -d "$LOG_DIR" ]; then
    sudo mkdir -p "$LOG_DIR"
    sudo chown "$RUNNER_USER":github-runners "$LOG_DIR"
    sudo chmod 750 "$LOG_DIR"
fi

# Verify installation
log_info "ðŸ” Verifying installation..."

if service_is_running; then
    log_success "âœ… Service is running"
else
    log_warning "âš ï¸  Service is not running"
fi

if is_runner_configured; then
    log_success "âœ… Runner is configured"
else
    log_error "âŒ Runner configuration failed"
fi

# Final status
log_success "âœ… GitHub Actions Runner setup complete!"
echo ""
echo "ðŸ“Š Installation Summary:"
echo "ðŸ”— Repository: $REPO_URL"
echo "ðŸƒ Runner Name: $RUNNER_NAME"
echo "ðŸ“‚ Runner Home: $RUNNER_HOME"
echo "ðŸ‘¤ Runner User: $RUNNER_USER"
echo "ðŸ”§ Service: $SERVICE_NAME"
echo "ðŸ“¦ Version: $RUNNER_VERSION"
echo ""
echo "ðŸŽ¯ Next Steps:"
echo "1. Verify runner appears in GitHub repository settings"
echo "2. Test runner with a simple workflow"
echo "3. Monitor logs: sudo tail -f $LOG_DIR/runner.log"
echo "4. Check service: sudo launchctl list | grep $SERVICE_NAME"
echo ""
echo "ðŸ”’ Security Features Enabled:"
echo "- Dedicated user account ($RUNNER_USER)"
echo "- macOS Firewall enabled"
echo "- Stealth mode enabled"
echo "- File permissions hardened"
echo "- Service isolation"
"""
    return script_content


# Template system for extensible runner configuration


def generate_runner_setup_scripts_parallel(
    merged_config: Dict[str, Any],
    repo_url: str = None,
    runner_token: str = None,
    save_to_files: bool = False,
    output_dir: str = ".",
) -> None:
    """
    Generate runner setup scripts in parallel with progress tracking.

    Args:
        merged_config: Merged configuration from YAML files
        repo_url: Repository URL for runner registration
        runner_token: GitHub runner registration token
        save_to_files: Whether to save scripts to files instead of printing
        output_dir: Output directory for saved files
    """
    runners = merged_config.get("runners", [])

    if not runners:
        print("âŒ No runners found in configuration")
        return

    print(f"ðŸ¤– Generating setup scripts for {len(runners)} runner(s)...")

    def generate_single_script(runner):
        """Generate script for a single runner."""
        try:
            runner_name = runner["name"]
            platform_type = runner.get("platform", "linux")

            # Use token from runner config if available, otherwise use the global token
            effective_token = runner.get("token", runner_token)

            # Get machine specification for architectural validation
            print(f"ðŸ“‹ Generating machine specification for {runner_name}...")
            machine_spec = MachineConfigGenerator.get_current_machine_spec()

            # Generate appropriate script content
            if platform_type == "windows":
                script_content = _generate_windows_setup_script(
                    runner, machine_spec, repo_url, effective_token
                )
                script_extension = "ps1"
            elif platform_type == "darwin":
                script_content = _generate_macos_setup_script(
                    runner, machine_spec, repo_url, effective_token
                )
                script_extension = "sh"
            else:
                script_content = _generate_linux_setup_script(
                    runner, machine_spec, repo_url, effective_token
                )
                script_extension = "sh"

            result = {
                "runner_name": runner_name,
                "platform": platform_type,
                "script_content": script_content,
                "extension": script_extension,
                "source_file": runner.get("source_file", "unknown"),
            }

            if save_to_files:
                # Save to file
                script_filename = f"setup-{runner_name}.{script_extension}"
                script_path = os.path.join(output_dir, script_filename)

                with open(script_path, "w") as f:
                    f.write(script_content)

                # Make script executable on Unix-like systems
                if script_extension == "sh":
                    os.chmod(script_path, 0o755)

                result["saved_to"] = script_path

            return result

        except Exception as e:
            return {"error": str(e), "runner_name": runner.get("name", "unknown")}

    # Process runners in parallel with progress bar
    with tqdm(total=len(runners), desc="Generating scripts", unit="script") as pbar:
        results = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
            # Submit all tasks
            future_to_runner = {
                executor.submit(generate_single_script, runner): runner
                for runner in runners
            }

            # Collect results as they complete
            for future in concurrent.futures.as_completed(future_to_runner):
                result = future.result()
                results.append(result)
                pbar.update(1)

    # Display results
    print(f"\nâœ… Generated {len(results)} setup script(s):")
    print("-" * 60)

    for result in results:
        if "error" in result:
            print(f"âŒ {result['runner_name']}: {result['error']}")
        else:
            runner_name = result["runner_name"]
            platform = result["platform"]
            source_file = result.get("source_file", "unknown")

            if save_to_files:
                print(f"ðŸ“ {runner_name} ({platform}) -> {result['saved_to']}")
                print(f"   Source: {source_file}")
            else:
                print(f"ðŸ–¥ï¸  {runner_name} ({platform}) - Source: {source_file}")
                print("=" * 60)
                print(result["script_content"])
                print("=" * 60)
                print()

    if repo_url and runner_token:
        print(f"\nðŸ”— Repository: {repo_url}")
        print(f"ðŸ”‘ Token: {'*' * (len(runner_token) - 4) + runner_token[-4:]}")
    else:
        print(
            "\nâš ï¸  Remember to provide --repo-url and --runner-token for automatic registration"
        )


def add_machine_spec_arguments(parser: argparse.ArgumentParser) -> None:
    """
    Dynamically add self-hosted runner arguments based on MachineSpec model fields.

    This function introspects the MachineSpec model to automatically generate
    CLI arguments from model metadata, ensuring consistency and maintainability.
    """
    # Get the model fields from MachineSpec
    model_fields = MachineSpec.model_fields

    # Add a name argument (not in MachineSpec but needed for runner config)
    parser.add_argument(
        "--self-hosted-runner-name",
        help="Name for self-hosted runner configuration",
    )

    # Dynamically generate arguments for MachineSpec fields
    for field_name, field_info in model_fields.items():
        arg_name = f"--self-hosted-runner-{field_name.replace('_', '-')}"

        # Start building argument configuration
        arg_config = {}

        # Extract description from Pydantic field
        if hasattr(field_info, "description") and field_info.description:
            arg_config["help"] = field_info.description

        # Extract default value from Pydantic field
        if (
            hasattr(field_info, "default")
            and field_info.default is not PydanticUndefined
            and field_info.default is not ...
            and field_info.default is not None
        ):
            if hasattr(field_info.default, "value"):  # For Enum defaults
                arg_config["default"] = field_info.default.value
            else:
                arg_config["default"] = field_info.default

        # Determine type and choices from field annotation
        field_annotation = field_info.annotation

        # Handle Union types (e.g., Optional)
        origin = get_origin(field_annotation)
        if origin is Union:
            # Get non-None types from Union
            args = get_args(field_annotation)
            non_none_types = [arg for arg in args if arg is not type(None)]
            if non_none_types:
                field_annotation = non_none_types[0]

        # Handle Enum types
        if inspect.isclass(field_annotation) and issubclass(field_annotation, Enum):
            arg_config["choices"] = [item.value for item in field_annotation]
            if "default" not in arg_config:
                # Use first enum value as default if no default specified
                arg_config["default"] = list(field_annotation)[0].value

        # Handle basic types
        elif field_annotation in (int, PositiveInt):
            arg_config["type"] = int
            if "default" not in arg_config:
                # Set reasonable defaults for numeric fields
                if field_name == "cores":
                    arg_config["default"] = 2  # Same as ubuntu-latest
        elif field_annotation in (float, PositiveFloat):
            arg_config["type"] = float
            if "default" not in arg_config:
                if field_name == "ram_gb":
                    arg_config["default"] = 7.0  # Same as ubuntu-latest
                elif field_name == "storage_gb":
                    arg_config["default"] = 14.0  # Same as ubuntu-latest
        elif field_annotation == str:
            arg_config["type"] = str

        # Add enhanced help text for specific fields
        if field_name == "cores" and "help" in arg_config:
            default_info = f"default: {arg_config.get('default', 'required')}"
            arg_config["help"] += f" ({default_info}, same as ubuntu-latest)"
        elif field_name == "ram_gb" and "help" in arg_config:
            default_info = f"default: {arg_config.get('default', 'required')}"
            arg_config["help"] += f" ({default_info}, same as ubuntu-latest)"
        elif field_name == "storage_gb" and "help" in arg_config:
            default_info = f"default: {arg_config.get('default', 'required')}"
            arg_config["help"] += f" ({default_info}, same as ubuntu-latest)"
        elif "choices" in arg_config and "help" in arg_config:
            default_info = f"default: {arg_config.get('default', 'required')}"
            arg_config["help"] += f" ({default_info})"
        elif "help" in arg_config and "default" not in arg_config:
            # For fields without defaults, don't show misleading default info
            pass  # Keep help as-is for required fields

        parser.add_argument(arg_name, **arg_config)

    # Add cost argument (not in MachineSpec but needed for analysis)
    parser.add_argument(
        "--self-hosted-runner-cost-per-minute",
        type=float,
        default=0.0,
        help="Cost per minute for self-hosted runner (default: 0.0)",
    )

    # Add multi-format config argument
    parser.add_argument(
        "--self-hosted-runner-config",
        action="append",
        help="Configuration file with self-hosted runner setup (supports YAML, JSON, TOML based on extension, can be specified multiple times, configs will be merged)",
    )


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="GitHub Actions Optimizer - Comprehensive Workflow Optimization Platform",
        epilog="""
Examples:
  # Use sample data for demonstration
  %(prog)s --sample-data --format table
  %(prog)s --sample-data --format json --output analysis.json

  # Analyze real repository data
  %(prog)s --repo owner/repository --format table
  %(prog)s --repo owner/repository --format csv --output report.csv

  # Get comprehensive analysis with insights and recommendations
  %(prog)s --repo owner/repository --comprehensive
  %(prog)s --sample-data --comprehensive

  # Get specific advanced analyses
  %(prog)s --sample-data --analyze-resources
  %(prog)s --sample-data --optimize-matrix
  %(prog)s --sample-data --self-hosted-advice
  %(prog)s --sample-data --analyze-resources --optimize-matrix --self-hosted-advice

  # List all supported runner types
  %(prog)s --list-runners
        """,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        "--format",
        choices=get_supported_file_formats() + ["table", "csv"],
        default="table",
        help="Output format (default: table)",
    )
    parser.add_argument(
        "--detailed",
        action="store_true",
        help="Include detailed metrics in table format",
    )
    parser.add_argument(
        "--comprehensive",
        action="store_true",
        help="Show comprehensive analysis with insights and recommendations",
    )
    parser.add_argument(
        "--analyze-resources",
        action="store_true",
        help="Include runner resource efficiency analysis (requires --comprehensive)",
    )
    parser.add_argument(
        "--optimize-matrix",
        action="store_true",
        help="Include matrix job optimization analysis (requires --comprehensive)",
    )
    parser.add_argument(
        "--self-hosted-advice",
        action="store_true",
        help="Include self-hosted runner setup recommendations (requires --comprehensive)",
    )
    parser.add_argument("--output", "-o", help="Output file path")
    parser.add_argument(
        "--include-raw-data",
        action="store_true",
        help="Include raw run data in JSON output",
    )
    parser.add_argument(
        "--sample-data", action="store_true", help="Use sample data for demonstration"
    )
    parser.add_argument(
        "--dynamic-pricing",
        action="store_true",
        help="Attempt to fetch latest pricing from GitHub docs (experimental)",
    )
    parser.add_argument(
        "--github-repository",
        "--repo",
        help="GitHub repository in format owner/repo (required for live data)",
    )
    parser.add_argument(
        "--list-runners",
        action="store_true",
        help="List all supported runner types and their pricing",
    )
    parser.add_argument(
        "--cache-only",
        action="store_true",
        help="Use only cached data, do not make new API requests",
    )

    # Dynamically add self-hosted runner arguments from MachineSpec model
    add_machine_spec_arguments(parser)

    # New functionality arguments
    parser.add_argument(
        "--generate-machine-config",
        action="store_true",
        help="Generate YAML configuration file from current machine specification",
    )
    parser.add_argument(
        "--generate-machine-description",
        nargs="?",
        const="",  # Default value when flag is used without argument
        help="Generate machine specification file (format determined by extension, defaults to YAML). If no filename provided, uses hostname + '.yaml'",
    )
    parser.add_argument(
        "--output-dir",
        help="Output directory for generated files (default: current directory)",
        default=".",
    )
    parser.add_argument(
        "--generate-patches",
        action="store_true",
        help="Generate workflow patches for optimal runner usage",
    )
    parser.add_argument(
        "--generate-copilot-prompts",
        action="store_true",
        help="Generate Copilot prompts for implementing improvements",
    )
    parser.add_argument(
        "--generate-self-hosted-runner-setup",
        action="store_true",
        help="Generate robust self-hosted runner setup scripts from YAML config(s)",
    )
    parser.add_argument(
        "--github-repo-url",
        "--repo-url",
        help="Repository URL for runner registration (auto-detected from current directory if not provided). Accepts full URL, owner/repo, or user/repo format",
    )
    parser.add_argument(
        "--save-scripts",
        action="store_true",
        help="Save generated setup scripts to files instead of printing to console",
    )
    parser.add_argument(
        "--verify-runners",
        action="store_true",
        help="Verify that all runners defined in configuration files are registered and online in GitHub repository",
    )

    args = parser.parse_args()

    # Setup logging with tqdm integration
    setup_logging()

    # Handle runner verification
    if args.verify_runners:
        if not args.self_hosted_runner_config:
            print(
                "âŒ --verify-runners requires --self-hosted-runner-config to specify configuration file(s)"
            )
            return

        verify_runners_status(args.self_hosted_runner_config, args.github_repo_url)
        return

    # Handle machine specification generation
    if args.generate_machine_config:
        try:
            # Handle token - automatically create one via GitHub API
            final_token = None
            print("ðŸ” Attempting to create runner registration token via GitHub API...")

            # Try to get repository URL for token generation
            repo_url = args.github_repo_url
            if not repo_url:
                # Try to auto-detect from current directory
                try:
                    result = subprocess.run(
                        ["git", "remote", "get-url", "origin"],
                        capture_output=True,
                        text=True,
                        timeout=5,
                    )
                    if result.returncode == 0:
                        repo_url = result.stdout.strip()
                        # Clean up repository URL (remove .git suffix and convert SSH to HTTPS)
                        if repo_url.startswith("git@github.com:"):
                            repo_url = repo_url.replace(
                                "git@github.com:", "https://github.com/"
                            )
                        if repo_url.endswith(".git"):
                            repo_url = repo_url[:-4]
                        # Resolve canonical URL to handle redirects/transfers
                        repo_url = resolve_canonical_repository_url(repo_url)
                except Exception:
                    pass

            # Try to create token via GitHub API if we have repo URL
            if repo_url:
                try:
                    github_api = GitHubAPI()

                    # Parse the repository URL to extract owner and repo
                    owner, repo = parse_repository_url(repo_url)
                    if owner and repo:
                        final_token = github_api.create_runner_registration_token(
                            owner, repo
                        )
                        if final_token:
                            print("âœ… Token auto-generated successfully")
                        else:
                            print(
                                "âš ï¸  Token auto-generation failed, will generate config without token"
                            )
                    else:
                        print(f"âš ï¸  Could not parse repository URL: {repo_url}")
                        print(
                            "ðŸ’¡ Expected format: https://github.com/owner/repo or owner/repo"
                        )
                except Exception as e:
                    print(f"âš ï¸  Token auto-generation failed: {e}")
                    print(
                        "ðŸ’¡ You may need to configure GitHub authentication or create a token manually"
                    )
            else:
                print("âš ï¸  No repository URL found for token generation")
                print(
                    "ðŸ’¡ Use --github-repo-url to specify repository for token generation"
                )

            # Use command line arguments if provided, otherwise detect current machine
            if any(
                [
                    args.self_hosted_runner_cores,
                    args.self_hosted_runner_ram_gb,
                    args.self_hosted_runner_storage_gb,
                    args.self_hosted_runner_arch,
                ]
            ):
                # Create MachineSpec from command line arguments
                machine_spec = MachineSpec(
                    name="custom-runner",
                    cores=args.self_hosted_runner_cores or 2,
                    ram_gb=args.self_hosted_runner_ram_gb or 7.0,
                    storage_gb=args.self_hosted_runner_storage_gb or 14.0,
                    arch=args.self_hosted_runner_arch or "x64",
                    os_name="linux",  # Default to linux for self-hosted
                    os_version="unknown",
                    os_family="ubuntu",
                    platform="linux",
                    cpu_model="Custom CPU",
                    package_manager="apt",
                    shell="bash",
                )
            else:
                # Auto-detect current machine specs
                machine_spec = MachineConfigGenerator.get_current_machine_spec()

            output_path = os.path.join(args.output_dir, "machine-config.yaml")
            MachineConfigGenerator.generate_machine_config_yaml(
                machine_spec,
                output_path,
                args.self_hosted_runner_cost_per_minute,
                token=final_token,
            )
            print(f"ðŸ“‹ Generated machine specification: {output_path}")
            print(f"   - Cores: {machine_spec.cores}")
            print(f"   - RAM: {machine_spec.ram_gb:.1f} GB")
            print(f"   - Storage: {machine_spec.storage_gb:.1f} GB")
            print(f"   - Architecture: {machine_spec.arch}")
            print(f"   - OS: {machine_spec.os_name} {machine_spec.os_version}")
            print(f"   - CPU: {machine_spec.cpu_model}")
            if final_token:
                print("   - Token: âœ… auto-generated")
            else:
                print(
                    "   - Token: âŒ not available (will be auto-generated when setting up runners)"
                )
            return
        except Exception as e:
            print(f"âŒ Error generating machine specification: {e}")
            return

    # Handle machine description generation
    if args.generate_machine_description is not None:
        try:
            # Detect current machine specification
            machine_spec = MachineConfigGenerator.get_current_machine_spec()

            # Determine output filename
            if args.generate_machine_description == "":
                # Empty string provided, use default based on hostname
                default_runner_name = machine_spec.hostname or "auto-detected-runner"
                output_filename = f"{default_runner_name}.yaml"
            else:
                # Use provided filename
                output_filename = args.generate_machine_description

            output_path = Path(args.output_dir) / output_filename

            # Generate machine specification data
            machine_data = {
                "metadata": {
                    "generated_on": datetime.now().isoformat(),
                    "description": f"Machine specification for {machine_spec.hostname}",
                    "generator": "github-actions-optimizer.py",
                },
                "machine_spec": machine_spec.model_dump(),
            }

            # Save in the detected format
            save_config_file(machine_data, output_path)

            print(f"ðŸ“‹ Generated machine description: {output_path}")
            print(f"   - Hostname: {machine_spec.hostname}")
            print(f"   - OS: {machine_spec.os_name} {machine_spec.os_version}")
            print(f"   - Architecture: {machine_spec.arch}")
            print(f"   - Cores: {machine_spec.cores}")
            print(f"   - RAM: {machine_spec.ram_gb:.1f} GB")
            print(f"   - Storage: {machine_spec.storage_gb:.1f} GB")
            print(f"   - Package Manager: {machine_spec.package_manager}")
            print(f"   - Firewall: {machine_spec.primary_firewall}")

            return
        except Exception as e:
            print(f"âŒ Error generating machine description: {e}")
            return

    # Handle runner setup script generation
    if args.generate_self_hosted_runner_setup:
        try:
            # Check if config files are provided, otherwise auto-detect machine
            if not args.self_hosted_runner_config:
                print(
                    "ðŸ” No runner configuration provided, auto-detecting current machine..."
                )

                # Auto-detect current machine specification
                machine_spec = MachineConfigGenerator.get_current_machine_spec()

                # Create a default runner configuration based on current machine
                default_runner_name = machine_spec.hostname or "auto-detected-runner"
                auto_config = {
                    "metadata": {
                        "generated_on": datetime.now().isoformat(),
                        "description": "Auto-detected machine specification for runner setup",
                    },
                    "runners": [
                        {
                            "name": default_runner_name,
                            "cores": machine_spec.cores,
                            "ram_gb": machine_spec.ram_gb,
                            "storage_gb": machine_spec.storage_gb,
                            "arch": machine_spec.arch,
                            "cost_per_minute": 0.0,
                            "description": f"Auto-detected configuration for {machine_spec.hostname}",
                        }
                    ],
                }
                merged_config = auto_config
                print(
                    f"âœ… Auto-detected machine: {machine_spec.cores} cores, {machine_spec.ram_gb}GB RAM, {machine_spec.storage_gb}GB storage"
                )
            else:
                # Merge multiple config files if provided
                print(
                    f"ðŸ“‹ Loading configuration from {len(args.self_hosted_runner_config)} file(s)..."
                )
                merged_config = merge_configs(args.self_hosted_runner_config)

                if not merged_config["runners"]:
                    print(
                        "âŒ No runners found in any of the provided configuration files"
                    )
                    return

                print(
                    f"âœ… Found {len(merged_config['runners'])} runner(s) across all config files"
                )

            # Handle token - automatically create one via GitHub API
            final_token = None
            print("ðŸ” Attempting to create runner registration token via GitHub API...")

            # Try to get repository URL for token generation
            repo_url = args.github_repo_url
            if not repo_url:
                # Try to auto-detect from current directory
                try:
                    result = subprocess.run(
                        ["git", "remote", "get-url", "origin"],
                        capture_output=True,
                        text=True,
                        timeout=5,
                    )
                    if result.returncode == 0:
                        repo_url = result.stdout.strip()
                        # Clean up repository URL (remove .git suffix and convert SSH to HTTPS)
                        if repo_url.startswith("git@github.com:"):
                            repo_url = repo_url.replace(
                                "git@github.com:", "https://github.com/"
                            )
                        if repo_url.endswith(".git"):
                            repo_url = repo_url[:-4]
                        # Resolve canonical URL to handle redirects/transfers
                        repo_url = resolve_canonical_repository_url(repo_url)
                except Exception:
                    pass

            # Try to create token via GitHub API if we have repo URL
            if repo_url:
                try:
                    # Parse repository URL to extract owner and repo
                    owner, repo_name = parse_repository_url(repo_url)
                    if owner and repo_name:
                        # Try to create token via API
                        api = GitHubAPI()
                        final_token = api.create_runner_registration_token(
                            owner, repo_name
                        )
                        if final_token:
                            print("âœ… Successfully created runner registration token!")
                        else:
                            print(
                                "âš ï¸  Could not create token via API. You may need to set up authentication."
                            )
                    else:
                        print(f"âš ï¸  Could not parse repository URL: {repo_url}")
                        print(
                            "ðŸ’¡ Expected format: https://github.com/owner/repo or owner/repo"
                        )
                except Exception as e:
                    print(f"âš ï¸  Error creating token via GitHub API: {e}")
                    print(
                        "ðŸ’¡ You may need to configure GitHub authentication or create a token manually"
                    )
            else:
                print("âš ï¸  No repository URL found for token generation")
                print(
                    "ðŸ’¡ Use --github-repo-url to specify repository for token generation"
                )

            # Ensure we have a repo_url for script generation (even if placeholder)
            if not repo_url:
                repo_url = "https://github.com/YOUR_ORG/YOUR_REPO"
                print("ðŸ’¡ Using placeholder repository URL for script generation")

            # Generate scripts in parallel
            generate_runner_setup_scripts_parallel(
                merged_config,
                repo_url,
                final_token,
                save_to_files=args.save_scripts,
                output_dir=args.output_dir,
            )
            return

        except Exception as e:
            print(f"âŒ Error generating runner setup scripts: {e}")
            return

    # Handle setup scripts generation
    if args.generate_machine_config:
        try:
            output_dir = Path(args.output_dir) / "setup_scripts"
            output_dir.mkdir(parents=True, exist_ok=True)

            for os_name, setup_info in MachineConfigGenerator.SETUP_TEMPLATES.items():
                if os_name == "windows":
                    script_content = f"""  # GitHub Actions Self-Hosted Runner Setup Script for {os_name.title()}
# Security-hardened installation following GitHub best practices
# Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
# Run as Administrator

param(
    [Parameter(Mandatory=$true)]
    [string]$RepoUrl,
    [Parameter(Mandatory=$true)]
    [string]$RunnerToken
)

Write-Host "ðŸ”’ Starting security-hardened GitHub Runner setup for {os_name.title()}..." - ForegroundColor Green

# Pre-setup security hardening
Write-Host "ðŸ“‹ Pre-setup security configuration..." - ForegroundColor Yellow"""

                    for cmd in setup_info["pre_setup"]:
                        script_content += f"\n{cmd}"

                    script_content += f"""

# User and group setup with minimal privileges
Write-Host "ðŸ‘¤ Setting up dedicated runner user..." - ForegroundColor Yellow"""

                    for cmd in setup_info["user_setup"]:
                        script_content += f"\n{cmd}"

                    script_content += f"""

# Container isolation setup
Write-Host "ðŸ“¦ Setting up container isolation..." - ForegroundColor Yellow"""

                    for cmd in setup_info["container_setup"]:
                        script_content += f"\n{cmd}"

                    script_content += f"""

# Runner installation
Write-Host "âš™ï¸ Installing GitHub Actions Runner..." - ForegroundColor Yellow
$GITHUB_RUNNER_VERSION = "2.321.0"
$ARCH = if ([Environment]: : Is64BitProcess) {{"x64" }} else {{"x86" }}"""

                    for cmd in setup_info["install_commands"]:
                        cmd = cmd.replace("{version}", "$GITHUB_RUNNER_VERSION")
                        cmd = cmd.replace("{arch}", "$ARCH")
                        cmd = cmd.replace(
                            "https://github.com/{owner}/{repo}", "$RepoUrl"
                        )
                        cmd = cmd.replace("{token}", "$RunnerToken")
                        script_content += f"\n{cmd}"

                    script_content += f"""

# Security configuration
Write-Host "ðŸ›¡ï¸ Applying security hardening..." - ForegroundColor Yellow"""

                    for cmd in setup_info["security_setup"]:
                        script_content += f"\n{cmd}"

                    script_content += f"""

# Monitoring setup
Write-Host "ðŸ“Š Setting up monitoring and auditing..." - ForegroundColor Yellow"""

                    for cmd in setup_info["monitoring_setup"]:
                        script_content += f"\n{cmd}"

                    script_content += f"""

Write-Host "âœ… GitHub Runner setup complete with security hardening!" - ForegroundColor Green
Write-Host "ðŸ” Security features enabled:" - ForegroundColor Cyan
Write-Host "  - Dedicated user account (github-runner)" - ForegroundColor White
Write-Host "  - Container isolation" - ForegroundColor White
Write-Host "  - Network firewall restrictions" - ForegroundColor White
Write-Host "  - File system permissions" - ForegroundColor White
Write-Host "  - Audit logging" - ForegroundColor White
Write-Host "âš ï¸  Remember to:" - ForegroundColor Yellow
Write-Host "  - Set password for github-runner user" - ForegroundColor White
Write-Host "  - Configure runner token with minimal permissions" - ForegroundColor White
Write-Host "  - Regularly update the runner and OS" - ForegroundColor White
Write-Host "  - Monitor audit logs for suspicious activity" - ForegroundColor White
"""

                    script_path = output_dir / f"setup_{os_name}.ps1"
                else:
                    script_content = f"""  # !/bin/bash
# GitHub Actions Self-Hosted Runner Setup Script for {os_name.title()}
# Security-hardened installation following GitHub best practices
# Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
# Run with sudo privileges

set - euo pipefail

REPO_URL = "$1"
RUNNER_TOKEN = "$2"

if [-z "$REPO_URL"] | | [-z "$RUNNER_TOKEN"]; then
    echo "Usage: $0 <repo-url> <runner-token>"
    echo "Example: $0 https://github.com/owner/repo ghs_xxxxxxxxxxxx"
    exit 1
fi

echo "ðŸ”’ Starting security-hardened GitHub Runner setup for {os_name.title()}..."

# Pre-setup security hardening
echo "ðŸ“‹ Pre-setup security configuration..."
"""

                    for cmd in setup_info["pre_setup"]:
                        script_content += f"\n{cmd}"

                    script_content += """

# User and group setup with minimal privileges
echo "ðŸ‘¤ Setting up dedicated runner user..."
"""

                    for cmd in setup_info["user_setup"]:
                        script_content += f"\n{cmd}"

                    script_content += """

# Container isolation setup
echo "ðŸ“¦ Setting up container isolation..."
"""

                    for cmd in setup_info["container_setup"]:
                        script_content += f"\n{cmd}"

                    script_content += f"""

# Runner installation
echo "âš™ï¸ Installing GitHub Actions Runner..."
GITHUB_RUNNER_VERSION = "2.321.0"
ARCH = $(["$(uname -m)" = "x86_64"] & & echo "x64" | | echo "arm64")"""

                    for cmd in setup_info["install_commands"]:
                        cmd = cmd.replace("{version}", "$GITHUB_RUNNER_VERSION")
                        cmd = cmd.replace("{arch}", "$ARCH")
                        cmd = cmd.replace(
                            "https://github.com/{owner}/{repo}", "$REPO_URL"
                        )
                        cmd = cmd.replace("{token}", "$RUNNER_TOKEN")
                        script_content += f"\n{cmd}"

                    script_content += """

# Security configuration
echo "ðŸ›¡ï¸ Applying security hardening..."
"""

                    for cmd in setup_info["security_setup"]:
                        script_content += f"\n{cmd}"

                    script_content += """

# Monitoring setup
echo "ðŸ“Š Setting up monitoring and auditing..."
"""

                    for cmd in setup_info["monitoring_setup"]:
                        script_content += f"\n{cmd}"

                    script_content += """

echo "âœ… GitHub Runner setup complete with security hardening!"
echo "ðŸ” Security features enabled:"
echo "  - Dedicated user account (github-runner)"
echo "  - Container isolation (Docker/Podman)"
echo "  - Network firewall restrictions"
echo "  - File system permissions"
echo "  - Audit logging"
echo "âš ï¸  Remember to:"
echo "  - Configure runner token with minimal permissions"
echo "  - Regularly update the runner and OS"
echo "  - Monitor audit logs for suspicious activity"
echo "  - Review container images for security vulnerabilities"
"""

                    script_path = output_dir / f"setup_{os_name}.sh"

                with open(script_path, "w") as f:
                    f.write(script_content)

                # Make script executable
                if not os_name == "windows":
                    os.chmod(script_path, 0o755)
                print(f"âœ… Generated security-hardened setup script: {script_path}")

            print(f"\nðŸ“‹ Security-hardened setup scripts generated in: {output_dir}")
            print("ðŸ” These scripts include GitHub security best practices:")
            print("  - Dedicated user accounts with minimal privileges")
            print("  - Container isolation (Docker/Podman)")
            print("  - Network firewall restrictions")
            print("  - File system permission hardening")
            print("  - Audit logging and monitoring")
            print("Usage:")
            print("  Linux/macOS: ./setup_<os>.sh <repo-url> <runner-token>")
            print(
                "  Windows: .\\setup_windows.ps1 -RepoUrl <repo-url> -RunnerToken <runner-token>"
            )
            return
        except Exception as e:
            print(f"âŒ Error generating setup scripts: {e}")
            return

    # Handle --list-runners
    if args.list_runners:
        print("ðŸƒ GitHub Actions Runner Types and Pricing")
        print("=" * 60)
        print(f"{'Runner Type':<35} {'Price/min (USD)':<15}")
        print("-" * 60)

        # Group by category
        categories = {
            "Standard Runners": [
                k
                for k in PRICING.keys()
                if not any(x in k for x in ["cores", "arm", "gpu", "self-hosted"])
            ],
            "Larger Runners (x64)": [
                k
                for k in PRICING.keys()
                if "cores" in k and "arm" not in k and "gpu" not in k
            ],
            "ARM64 Runners": [k for k in PRICING.keys() if "arm" in k],
            "GPU Runners": [k for k in PRICING.keys() if "gpu" in k],
            "Self-Hosted": [k for k in PRICING.keys() if "self-hosted" in k],
        }

        for category, runners in categories.items():
            if runners:
                print(f"\n{category}:")
                for runner in sorted(runners):
                    price = PRICING[runner]
                    print(f"  {runner:<33} ${price:<14.3f}")
        print("\n" + "=" * 60)
        return

    # Initialize optimizer
    analyzer = GitHubActionsOptimizer(
        use_dynamic_pricing=args.dynamic_pricing, require_auth=not args.sample_data
    )

    # Handle self-hosted runner configuration
    self_hosted_config = None
    if args.self_hosted_runner_config:
        try:
            self_hosted_config = SelfHostedRunnerConfig.from_config_files(
                args.self_hosted_runner_config
            )
            print(
                f"ðŸ“‹ Loaded {len(self_hosted_config.runners)} self-hosted runner(s) from {args.self_hosted_runner_config}"
            )

            # Check for zero costs and warn
            zero_cost_runners = [
                r for r in self_hosted_config.runners if r.cost_per_minute == 0.0
            ]
            if zero_cost_runners:
                runner_names = [r.name for r in zero_cost_runners]
                logger.warning(
                    f"âš ï¸  The following self-hosted runners have zero cost configured: {', '.join(runner_names)}. "
                    "Cost comparisons may not be accurate. Consider adding cost_per_minute values to your YAML config."
                )
        except Exception as e:
            print(f"âŒ Error loading self-hosted config: {e}")
            return
    elif (
        args.self_hosted_runner_name
        and args.self_hosted_runner_cores
        and args.self_hosted_runner_ram_gb
        and args.self_hosted_runner_storage_gb
    ):
        self_hosted_config = SelfHostedRunnerConfig.from_args(
            name=args.self_hosted_runner_name,
            cores=args.self_hosted_runner_cores,
            ram_gb=args.self_hosted_runner_ram_gb,
            storage_gb=args.self_hosted_runner_storage_gb,
            arch=args.self_hosted_runner_arch,
            cost_per_minute=args.self_hosted_runner_cost_per_minute,
        )
        print(f"ðŸ“‹ Using self-hosted runner: {args.self_hosted_runner_name}")

        # Warn if using default zero cost
        if args.self_hosted_runner_cost_per_minute == 0.0:
            logger.warning(
                "âš ï¸  Using default cost of $0.00/minute for self-hosted runner. "
                "Cost comparisons may not be accurate. Consider specifying "
                "--self-hosted-runner-cost-per-minute or adding cost_per_minute to your YAML config."
            )

    # Set self-hosted config if provided
    if self_hosted_config:
        analyzer.set_self_hosted_config(self_hosted_config)

    if args.sample_data:
        # Load sample data for demonstration
        print("ðŸ“Š Using sample data for demonstration...")
        sample_data = _get_sample_data()
        for workflow_name, runs in sample_data.items():
            for run in runs:
                analyzer.add_workflow_run(run, workflow_name)
    else:
        # Load real data from GitHub API
        repository_url = args.github_repository

        # Try to auto-detect repository if not provided
        if not repository_url:
            try:
                repository_url = get_repository_url()
                print(f"ðŸ” Auto-detected repository: {repository_url}")
            except ValueError as e:
                print(
                    "âŒ Error: --github-repository is required when not using sample data"
                )
                print("ðŸ’¡ Use --sample-data to run with demonstration data")
                print(f"ðŸ’¡ Auto-detection failed: {e}")
                return

        # Parse repository
        try:
            if "/" in repository_url and not repository_url.startswith(
                ("http://", "https://")
            ):
                # It's in owner/repo format
                owner, repo = repository_url.split("/", 1)
            else:
                # Extract owner/repo from full URL
                from urllib.parse import urlparse

                parsed = urlparse(repository_url)
                path_parts = parsed.path.strip("/").split("/")
                if len(path_parts) >= 2:
                    owner, repo = path_parts[0], path_parts[1]
                else:
                    raise ValueError("Invalid repository URL format")
        except ValueError:
            print(
                "âŒ Error: Repository must be in format 'owner/repo' or a valid GitHub URL"
            )
            return

        try:
            # Set cache-only mode if requested
            if args.cache_only:
                analyzer.api.set_cache_only_mode(True)

            analyzer.load_github_data(owner, repo)
        except Exception as e:
            print(f"âŒ Failed to load GitHub data: {e}")
            return

    # Generate analysis
    repository = args.github_repository or "sample/repository"
    if args.sample_data and not args.github_repository:
        repository = "sample/demonstration-repository"

    # Determine which analyses to include
    include_resource_analysis = args.analyze_resources or args.comprehensive
    include_matrix_optimization = args.optimize_matrix or args.comprehensive
    include_self_hosted_advice = args.self_hosted_advice or args.comprehensive

    # Warn if advanced flags are used without comprehensive output
    if (
        args.analyze_resources or args.optimize_matrix or args.self_hosted_advice
    ) and not args.comprehensive:
        print(
            "â„¹ï¸  Note: Advanced analysis flags require --comprehensive to display results"
        )
        print(
            "   Adding --comprehensive to show resource analysis, matrix optimization, and self-hosted recommendations"
        )
        args.comprehensive = True

    analysis = analyzer.generate_analysis(
        repository,
        include_resource_analysis=include_resource_analysis,
        include_matrix_optimization=include_matrix_optimization,
        include_self_hosted_advice=include_self_hosted_advice,
    )

    # Enhanced functionality for real repository analysis
    if not args.sample_data and args.github_repository:
        try:
            # Initialize GitHub API and runner manager
            # Get GitHub token
            github_token = None
            try:
                result = subprocess.run(
                    ["gh", "auth", "token"], capture_output=True, text=True, check=True
                )
                github_token = result.stdout.strip()
            except (subprocess.CalledProcessError, FileNotFoundError):
                github_token = os.environ.get("GITHUB_TOKEN")

            if github_token:
                g = Github(github_token)
                repo_obj = g.get_repo(args.github_repository)

                # Initialize enhanced components
                github_api = GitHubAPI()
                runner_manager = GitHubRunnerManager(github_api)

                # Get actual available runners
                actual_runners = runner_manager.get_repository_runners(repo_obj)

                # Add self-hosted runners from YAML if provided
                if self_hosted_config:
                    actual_runners.self_hosted = self_hosted_config.runners

                print(
                    f"ðŸƒ Found {len(actual_runners.github_hosted)} GitHub-hosted runners available"
                )
                print(
                    f"ðŸ“¦ Found {len(actual_runners.repository_runners)} repository self-hosted runners"
                )
                print(
                    f"ðŸ¢ Found {len(actual_runners.organization_runners)} organization self-hosted runners"
                )
                if self_hosted_config:
                    print(
                        f"âš™ï¸  Loaded {len(actual_runners.self_hosted)} self-hosted runners from configuration"
                    )

                # Analyze workflow requirements and generate recommendations
                optimizer = RunnerOptimizer(actual_runners)
                workflow_requirements = optimizer.analyze_workflow_requirements(
                    analyzer.runs_data
                )
                recommendations = optimizer.recommend_optimal_runners(
                    workflow_requirements
                )

                if recommendations:
                    print(
                        f"\nðŸŽ¯ Generated {len(recommendations)} runner optimization recommendations"
                    )

                    # Generate workflow patches if requested
                    if args.generate_patches:
                        patch_generator = WorkflowPatchGenerator()
                        patches = patch_generator.generate_runner_patches(
                            repo_obj, recommendations
                        )

                        if patches:
                            patches_dir = Path(args.output_dir) / "workflow_patches"
                            patches_dir.mkdir(parents=True, exist_ok=True)

                            for i, patch in enumerate(patches):
                                patch_file = (
                                    patches_dir
                                    / f"patch_{i+1}_{patch['file'].replace('.yml', '.patch')}"
                                )

                                savings_amount = patch["savings"]
                                changes_str = ", ".join(patch["changes"])
                                patch_content = (
                                    "--- a/.github/workflows/" + patch["file"] + "\n"
                                )
                                patch_content += (
                                    "+++ b/.github/workflows/" + patch["file"] + "\n"
                                )
                                patch_content += (
                                    "@@ Generated patch for runner optimization @@\n"
                                )
                                patch_content += f"Changes: {changes_str}\n"
                                patch_content += f"Estimated annual savings: USD {savings_amount:.2f}\n\n"
                                patch_content += patch["patched"] + "\n"

                                with open(patch_file, "w") as f:
                                    f.write(patch_content)

                                print(f"ðŸ“ Generated patch: {patch_file}")

                            print(f"ðŸ“ All patches saved to: {patches_dir}")

                    # Generate Copilot prompts if requested
                    if args.generate_copilot_prompts:
                        prompt_generator = CopilotPromptGenerator()

                        # Generate workflow optimization prompt
                        workflow_prompt = (
                            prompt_generator.generate_workflow_optimization_prompt(
                                recommendations
                            )
                        )

                        # Generate repository configuration prompt
                        analysis_results = {
                            "monthly_projection": analysis.monthly_projection,
                            "efficiency": (1 - analysis.waste_percentage / 100) * 100,
                        }
                        config_prompt = (
                            prompt_generator.generate_repository_configuration_prompt(
                                actual_runners, analysis_results
                            )
                        )

                        prompts_dir = Path(args.output_dir) / "copilot_prompts"
                        prompts_dir.mkdir(parents=True, exist_ok=True)

                        # Save workflow optimization prompt
                        with open(
                            prompts_dir / "workflow_optimization_prompt.md", "w"
                        ) as f:
                            f.write(workflow_prompt)

                        # Save repository configuration prompt
                        with open(
                            prompts_dir / "repository_configuration_prompt.md", "w"
                        ) as f:
                            f.write(config_prompt)

                        print(f"ðŸ¤– Generated Copilot prompts in: {prompts_dir}")
                        print(
                            "   â€¢ workflow_optimization_prompt.md - For optimizing workflow runner usage"
                        )
                        print(
                            "   â€¢ repository_configuration_prompt.md - For repository/organization configuration"
                        )

                # Add setup instructions for self-hosted runners
                if self_hosted_config and args.comprehensive:
                    setup_dir = Path(args.output_dir) / "self_hosted_setup"
                    setup_dir.mkdir(parents=True, exist_ok=True)

                    for runner in self_hosted_config.runners:
                        # Determine OS type from runner name/description
                        os_type = "ubuntu"  # Default
                        if (
                            "windows" in runner.name.lower()
                            or "windows" in runner.description.lower()
                        ):
                            os_type = "windows"
                        elif (
                            "fedora" in runner.name.lower()
                            or "fedora" in runner.description.lower()
                        ):
                            os_type = "fedora"
                        elif (
                            "macos" in runner.name.lower()
                            or "macos" in runner.description.lower()
                        ):
                            os_type = "macos"

                        setup_file = setup_dir / f"setup_{runner.name}.md"

                        setup_content = f"# Self-Hosted Runner Setup: {runner.name}\n\n"
                        setup_content += f"## Runner Specifications\n"
                        setup_content += f"- **Name**: {runner.name}\n"
                        setup_content += f"- **Cores**: {runner.cores}\n"
                        setup_content += f"- **RAM**: {runner.ram_gb} GB\n"
                        setup_content += f"- **Storage**: {runner.storage_gb} GB\n"
                        setup_content += f"- **Architecture**: {runner.arch}\n"
                        setup_content += (
                            f"- **Cost per minute**: USD {runner.cost_per_minute:.4f}\n"
                        )
                        setup_content += f"- **Description**: {runner.description}\n\n"
                        setup_content += "# Prerequisites\n"
                        setup_content += (
                            "1. A machine/VM with the specifications above\n"
                        )
                        setup_content += (
                            "2. Administrative access to the GitHub repository\n"
                        )
                        setup_content += "3. A GitHub personal access token with repo permissions\n\n"
                        setup_content += "# Setup Instructions\n\n"
                        setup_content += "# 1. Generate Runner Token\n"
                        setup_content += "```bash\n"
                        setup_content += "# Go to your repository settings\n"
                        setup_content += (
                            "# Navigate to Actions > Runners > New self-hosted runner\n"
                        )
                        setup_content += "# Copy the token provided\n"
                        setup_content += "```\n\n"
                        setup_content += "# 2. Download and Install Runner\n"

                        if os_type in MachineConfigGenerator.SETUP_TEMPLATES:
                            template = MachineConfigGenerator.SETUP_TEMPLATES[os_type]
                            setup_content += "\n```bash\n"
                            for cmd in template["install_commands"]:
                                cmd = cmd.replace("{version}", "2.311.0")
                                cmd = cmd.replace("{arch}", runner.arch)
                                cmd = cmd.replace(
                                    "{owner}/{repo}", args.github_repository
                                )
                                cmd = cmd.replace("{token}", "<YOUR_RUNNER_TOKEN>")
                                setup_content += f"{cmd}\n"
                            setup_content += "```\n"

                            setup_content += (
                                "\n### 3. Security Configuration\n```bash\n"
                            )
                            for cmd in template["security_setup"]:
                                setup_content += f"{cmd}\n"
                            setup_content += "```\n"

                        setup_content += "\n### 4. Verification\n"
                        setup_content += "After setup, verify the runner appears in your repository's Actions settings:\n"
                        setup_content += f"- Go to `https://github.com/{args.github_repository}/settings/actions/runners`\n"
                        setup_content += (
                            '- Your runner should appear with status "Idle"\n\n'
                        )
                        setup_content += "### 5. Usage in Workflows\n"
                        setup_content += (
                            "Update your workflow files to use this runner:\n"
                        )
                        setup_content += "```yaml\n"
                        setup_content += "jobs:\n"
                        setup_content += "  your-job:\n"
                        setup_content += f"    runs-on: {runner.name}\n"
                        setup_content += "    steps:\n"
                        setup_content += "      # Your workflow steps\n"
                        setup_content += "```\n\n"
                        setup_content += "## Cost Analysis\n"
                        setup_content += f"- Current GitHub-hosted equivalent cost: ${actual_runners.github_hosted[0].cost_per_minute:.4f}/minute\n"
                        setup_content += f"- Self-hosted cost: ${runner.cost_per_minute:.4f}/minute\n"
                        setup_content += f"- Potential savings: {((actual_runners.github_hosted[0].cost_per_minute - runner.cost_per_minute) / actual_runners.github_hosted[0].cost_per_minute * 100):.1f}%\n"

                        with open(setup_file, "w") as f:
                            f.write(setup_content)

                        print(f"ðŸ“‹ Generated setup instructions: {setup_file}")

                    print(
                        f"ðŸ“ Self-hosted runner setup instructions saved to: {setup_dir}"
                    )

        except Exception as e:
            print(f"âš ï¸  Enhanced analysis failed: {e}")
            print("Continuing with basic analysis...")

    # Format output
    if args.comprehensive:
        output = OutputFormatter.format_comprehensive_analysis(analysis)
    elif args.format == "json":
        output = OutputFormatter.format_json(analysis, args.include_raw_data)
    elif args.format == "yaml":
        output = OutputFormatter.format_yaml(analysis, args.include_raw_data)
    elif args.format == "csv":
        output = OutputFormatter.format_csv(analysis)
    else:  # table
        output = OutputFormatter.format_table(analysis, args.detailed)

    # Write output
    if args.output:
        with open(args.output, "w") as f:
            f.write(output)
        print(f"Analysis written to {args.output}")
    else:
        print(output)


def _get_sample_data() -> Dict[str, List[Dict]]:
    """Get sample data for demonstration."""
    return {
        "CI/CD Pipeline": [
            {
                "id": 17398246063,
                "workflow_id": 185487740,
                "status": "completed",
                "conclusion": "failure",
                "created_at": "2025-09-02T08:46:29Z",
                "updated_at": "2025-09-02T08:46:33Z",
                "event": "workflow_run",
                "runner_type": "ubuntu-latest",
            },
            {
                "id": 17398244000,
                "workflow_id": 185487740,
                "status": "completed",
                "conclusion": "failure",
                "created_at": "2025-09-02T08:45:29Z",
                "updated_at": "2025-09-02T08:45:38Z",
                "event": "workflow_run",
                "runner_type": "ubuntu-latest",
            },
            {
                "id": 17398241000,
                "workflow_id": 185487740,
                "status": "completed",
                "conclusion": "failure",
                "created_at": "2025-09-02T08:44:29Z",
                "updated_at": "2025-09-02T08:44:35Z",
                "event": "workflow_run",
                "runner_type": "ubuntu-latest",
            },
        ],
        "Auto Ready for Review": [
            {
                "id": 17397745594,
                "workflow_id": 185758991,
                "status": "completed",
                "conclusion": "success",
                "created_at": "2025-09-02T08:25:07Z",
                "updated_at": "2025-09-02T08:25:43Z",
                "event": "pull_request",
                "runner_type": "ubuntu-latest",
            },
            {
                "id": 17397743123,
                "workflow_id": 185758991,
                "status": "completed",
                "conclusion": "success",
                "created_at": "2025-09-02T08:25:02Z",
                "updated_at": "2025-09-02T08:25:29Z",
                "event": "pull_request",
                "runner_type": "ubuntu-latest",
            },
            {
                "id": 17398249469,
                "workflow_id": 185758991,
                "status": "completed",
                "conclusion": "failure",
                "created_at": "2025-09-02T08:46:36Z",
                "updated_at": "2025-09-02T08:46:44Z",
                "event": "workflow_run",
                "runner_type": "ubuntu-latest",
            },
        ],
        "Performance Tests": [
            {
                "id": 17398250000,
                "workflow_id": 185758992,
                "status": "completed",
                "conclusion": "success",
                "created_at": "2025-09-02T08:46:36Z",
                "updated_at": "2025-09-02T08:56:44Z",
                "event": "workflow_run",
                "runner_type": "ubuntu-latest-4-cores",
                "duration_seconds": 600,
            },
            {
                "id": 17398250001,
                "workflow_id": 185758992,
                "status": "completed",
                "conclusion": "success",
                "created_at": "2025-09-02T08:46:36Z",
                "updated_at": "2025-09-02T08:51:44Z",
                "event": "workflow_run",
                "runner_type": "ubuntu-latest-8-cores",
                "duration_seconds": 300,
            },
        ],
        "Windows Build": [
            {
                "id": 17398250002,
                "workflow_id": 185758993,
                "status": "completed",
                "conclusion": "success",
                "created_at": "2025-09-02T08:46:36Z",
                "updated_at": "2025-09-02T08:51:44Z",
                "event": "workflow_run",
                "runner_type": "windows-latest",
                "duration_seconds": 300,
            },
        ],
        "ARM64 Tests": [
            {
                "id": 17398250003,
                "workflow_id": 185758994,
                "status": "completed",
                "conclusion": "success",
                "created_at": "2025-09-02T08:46:36Z",
                "updated_at": "2025-09-02T08:49:44Z",
                "event": "workflow_run",
                "runner_type": "ubuntu-latest-4-cores-arm",
                "duration_seconds": 180,
            },
        ],
    }


# GitHub CLI Extension Main Function
# ==================================

def cmd_analyze(args):
    """Handle analyze subcommand."""
    repository, token = setup_github_api(args.repo)
    
    # Set legacy arguments for compatibility with existing analysis code
    class LegacyArgs:
        def __init__(self):
            self.github_repo_url = f"https://github.com/{repository}"
            self.sample_data = args.sample_data if hasattr(args, 'sample_data') else False
            self.format = args.format
            self.detailed = args.detailed if hasattr(args, 'detailed') else False
            self.comprehensive = args.comprehensive if hasattr(args, 'comprehensive') else False
            self.analyze_resources = args.analyze_resources if hasattr(args, 'analyze_resources') else False
            self.optimize_matrix = args.optimize_matrix if hasattr(args, 'optimize_matrix') else False
            self.self_hosted_advice = args.self_hosted_advice if hasattr(args, 'self_hosted_advice') else False
            self.verbose = args.verbose
            self.quiet = args.quiet
            
    legacy_args = LegacyArgs()
    
    # Call existing analysis logic with legacy args
    # This will need to be refactored to extract the analysis logic from main()
    print(f"ðŸ” Analyzing repository: {repository}")
    print("ðŸ“Š Analysis functionality will be implemented here")
    
    # For now, show what would be analyzed
    result = {
        "repository": repository,
        "analysis_type": "workflow_optimization",
        "status": "not_implemented",
        "message": "Analysis logic needs to be extracted from main() function"
    }
    
    output_result(result, args.format, args.output)

def cmd_generate_runner_setup(args):
    """Handle generate runner-setup subcommand."""
    repository, token = setup_github_api(args.repo)
    
    print(f"ðŸƒ Generating runner setup for: {repository}")
    print("âš™ï¸  Runner setup generation functionality will be implemented here")
    
    result = {
        "repository": repository,
        "generation_type": "runner_setup", 
        "status": "not_implemented",
        "message": "Runner setup generation logic needs to be extracted from main() function"
    }
    
    output_result(result, args.format, args.output)

def cmd_generate_machine_config(args):
    """Handle generate machine-config subcommand."""
    repository, token = setup_github_api(args.repo)
    
    print(f"ðŸ–¥ï¸  Generating machine config for: {repository}")
    print("âš™ï¸  Machine config generation functionality will be implemented here")
    
    result = {
        "repository": repository,
        "generation_type": "machine_config",
        "status": "not_implemented", 
        "message": "Machine config generation logic needs to be extracted from main() function"
    }
    
    output_result(result, args.format, args.output)

def cmd_generate_workflow_patch(args):
    """Handle generate workflow-patch subcommand."""
    repository, token = setup_github_api(args.repo)
    
    print(f"ðŸ”§ Generating workflow patch for: {repository}")
    print("âš™ï¸  Workflow patch generation functionality will be implemented here")
    
    result = {
        "repository": repository,
        "generation_type": "workflow_patch",
        "status": "not_implemented",
        "message": "Workflow patch generation logic needs to be extracted from main() function"
    }
    
    output_result(result, args.format, args.output)

def cmd_generate_copilot_prompt(args):
    """Handle generate copilot-prompt subcommand."""
    repository, token = setup_github_api(args.repo)
    
    print(f"ðŸ¤– Generating Copilot prompt for: {repository}")
    print("âš™ï¸  Copilot prompt generation functionality will be implemented here")
    
    result = {
        "repository": repository,
        "generation_type": "copilot_prompt",
        "status": "not_implemented",
        "message": "Copilot prompt generation logic needs to be extracted from main() function"
    }
    
    output_result(result, args.format, args.output)

def cmd_validate(args):
    """Handle validate subcommand."""
    repository, token = setup_github_api(args.repo)
    
    print(f"âœ… Validating workflows for: {repository}")
    print("âš™ï¸  Validation functionality will be implemented here")
    
    result = {
        "repository": repository,
        "validation_type": "workflows",
        "status": "not_implemented",
        "message": "Validation logic needs to be extracted from main() function"
    }
    
    output_result(result, args.format, args.output)

def cmd_benchmark(args):
    """Handle benchmark subcommand.""" 
    repository, token = setup_github_api(args.repo)
    
    print(f"ðŸ“Š Benchmarking workflows for: {repository}")
    print("âš™ï¸  Benchmarking functionality will be implemented here")
    
    result = {
        "repository": repository,
        "benchmark_type": "workflow_performance",
        "status": "not_implemented",
        "message": "Benchmarking logic needs to be extracted from main() function"
    }
    
    output_result(result, args.format, args.output)

def main_cli():
    """GitHub CLI extension main entry point."""
    parser = argparse.ArgumentParser(
        prog="gh actions-optimizer",
        description="GitHub Actions Optimizer - GitHub CLI Extension",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    subparsers = parser.add_subparsers(dest='command', help='Available commands')
    subparsers.required = True
    
    # Analyze subcommand
    analyze_parser = subparsers.add_parser('analyze', help='Analyze workflow performance and costs')
    add_common_arguments(analyze_parser)
    analyze_parser.add_argument('--sample-data', action='store_true', help='Use sample data for demonstration')
    analyze_parser.add_argument('--detailed', action='store_true', help='Include detailed metrics')
    analyze_parser.add_argument('--comprehensive', action='store_true', help='Show comprehensive analysis')
    analyze_parser.add_argument('--analyze-resources', action='store_true', help='Analyze resource usage')
    analyze_parser.add_argument('--optimize-matrix', action='store_true', help='Optimize build matrix')
    analyze_parser.add_argument('--self-hosted-advice', action='store_true', help='Get self-hosted runner advice')
    analyze_parser.set_defaults(func=cmd_analyze)
    
    # Generate subcommand with sub-subcommands
    generate_parser = subparsers.add_parser('generate', help='Generate configurations and patches')
    generate_subparsers = generate_parser.add_subparsers(dest='generate_type', help='Generation types')
    generate_subparsers.required = True
    
    # Generate runner-setup subcommand
    runner_setup_parser = generate_subparsers.add_parser('runner-setup', help='Generate self-hosted runner setup')
    add_common_arguments(runner_setup_parser)
    runner_setup_parser.add_argument('--output-dir', default='.', help='Output directory for generated files')
    runner_setup_parser.set_defaults(func=cmd_generate_runner_setup)
    
    # Generate machine-config subcommand  
    machine_config_parser = generate_subparsers.add_parser('machine-config', help='Generate machine configuration')
    add_common_arguments(machine_config_parser)
    machine_config_parser.add_argument('--output-dir', default='.', help='Output directory for generated files')
    machine_config_parser.add_argument('--cores', type=int, help='Number of CPU cores')
    machine_config_parser.add_argument('--ram-gb', type=float, help='RAM in GB')
    machine_config_parser.add_argument('--storage-gb', type=float, help='Storage in GB')
    machine_config_parser.add_argument('--arch', choices=['x64', 'arm64'], help='Architecture')
    # Add machine spec arguments dynamically
    add_machine_spec_arguments(machine_config_parser)
    machine_config_parser.set_defaults(func=cmd_generate_machine_config)
    
    # Generate workflow-patch subcommand
    workflow_patch_parser = generate_subparsers.add_parser('workflow-patch', help='Generate workflow optimization patches')
    add_common_arguments(workflow_patch_parser)
    workflow_patch_parser.add_argument('--output-dir', default='.', help='Output directory for generated files')
    workflow_patch_parser.set_defaults(func=cmd_generate_workflow_patch)
    
    # Generate copilot-prompt subcommand
    copilot_prompt_parser = generate_subparsers.add_parser('copilot-prompt', help='Generate GitHub Copilot optimization prompts')
    add_common_arguments(copilot_prompt_parser)
    copilot_prompt_parser.add_argument('--output-dir', default='.', help='Output directory for generated files')
    copilot_prompt_parser.set_defaults(func=cmd_generate_copilot_prompt)
    
    # Validate subcommand
    validate_parser = subparsers.add_parser('validate', help='Validate workflow configurations')
    add_common_arguments(validate_parser)
    validate_parser.add_argument('--config-file', action='append', help='Configuration file to validate')
    validate_parser.set_defaults(func=cmd_validate)
    
    # Benchmark subcommand
    benchmark_parser = subparsers.add_parser('benchmark', help='Benchmark workflow performance')
    add_common_arguments(benchmark_parser)
    benchmark_parser.add_argument('--duration', type=int, default=30, help='Benchmark duration in days')
    benchmark_parser.set_defaults(func=cmd_benchmark)
    
    # Parse arguments and dispatch to appropriate handler
    args = parser.parse_args()
    
    try:
        args.func(args)
    except Exception as e:
        if args.verbose:
            import traceback
            traceback.print_exc()
        else:
            print(f"âŒ Error: {e}")
        sys.exit(1)



if __name__ == "__main__":
    main_cli()
